---
title: "Survey Sampling Foundations"
subtitle: "Day 1: Building Your Statistical Excellence"
author: "Dr. Endri Raço, PhD | SADC Regional Statistics Project"
date: "November 2025"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.retina = 3,
  fig.width = 9,
  fig.height = 5,
  fig.align = 'center',
  out.width = '90%'
)

library(tidyverse)
library(survey)
library(knitr)
library(kableExtra)

# Color palette
colors <- c("#003F7F", "#0066CC", "#4A90E2", "#FFA500", "#FF6B35")
theme_set(theme_minimal(base_size = 14))
```

---

# Welcome to Your Transformation

.pull-left[
### Today's Promise:
- Master sampling fundamentals
- Calculate with confidence
- Program surveys in R
- Apply international standards
- Solve real problems
]

.pull-right[
### Your Journey:
```{r journey1, echo=FALSE}
timeline <- data.frame(
  Hour = 1:8,
  Topic = c("Foundation", "Theory", "Math", "R Skills",
            "Variance", "Standards", "Practice", "Mastery"),
  Progress = seq(10, 100, length.out = 8)
)

ggplot(timeline, aes(Hour, Progress)) +
  geom_area(fill = colors[2], alpha = 0.3) +
  geom_line(color = colors[1], size = 2) +
  geom_point(color = colors[1], size = 4) +
  labs(x = "Training Hour", y = "Skill Level (%)") +
  theme_minimal()
```
]

**Bottom line: By 17:00, from uncertainty to expertise**

---

# Meet Your Instructor

.pull-left[
### Dr. Endri Raço
- 14+ years in survey methods
- 500+ professionals trained
- 6 countries transformed
- Passionate about your success

### My Philosophy:
> "Every number tells a story.
> Let's make sure it's the right one."
]

.pull-right[
### What Drives Me:
```{r impact1, echo=FALSE}
impact <- data.frame(
  Year = 2020:2025,
  Students = c(45, 60, 75, 85, 90, 95),
  Success = c(70, 75, 82, 88, 92, 95)
)

ggplot(impact, aes(Year)) +
  geom_line(aes(y = Students), color = colors[2], size = 2) +
  geom_line(aes(y = Success), color = colors[4], size = 2) +
  geom_point(aes(y = Students), color = colors[2], size = 3) +
  geom_point(aes(y = Success), color = colors[4], size = 3) +
  labs(y = "Impact Metrics (%)") +
  theme_minimal()
```
]

**Bottom line: Your success is my mission**

---

# Quick Introduction Activity

.pull-left[
### Find Someone Who:
1. Works in different department
2. Has done household surveys
3. Faces data quality challenges
4. Wants to master R
5. Is excited to learn!

### Share With Them:
- Your name and role
- Your biggest challenge
- What you hope to gain
]

.pull-right[
### Time: 3 Minutes

```{r timer1, echo=FALSE}
timer <- data.frame(
  time = c("Start", "1 min", "2 min", "3 min"),
  activity = c("Stand up", "Find partner", 
               "Share info", "Return"),
  y = 4:1
)

ggplot(timer, aes(x = 1, y = y)) +
  geom_point(size = 20, color = colors[3]) +
  geom_text(aes(label = time), color = "white") +
  geom_text(aes(x = 1.5, label = activity), hjust = 0) +
  xlim(0.5, 2) +
  theme_void()
```
]

**Bottom line: GO! Connect with colleagues**

---

# Why This Training Matters

.pull-left[
### Without Proper Sampling:
- ❌ Millions wasted on bad data
- ❌ Policies harm citizens
- ❌ International credibility lost
- ❌ Career limitations
- ❌ Repeated failures
]

.pull-right[
### With Expert Sampling:
- ✅ Efficient resource use
- ✅ Evidence-based policies
- ✅ Global recognition
- ✅ Career advancement
- ✅ Consistent success
]

**Bottom line: Your expertise shapes national decisions**

---

# Setting Our Learning Contract

.pull-left[
### I Promise To:
- Explain clearly
- Use real examples
- Support patiently
- Make it practical
- Ensure your success
]

.pull-right[
### You Promise To:
- Engage actively
- Ask questions
- Help others
- Practice exercises
- Apply learning
]

**Bottom line: Together we create excellence**

---

# Progress Check 1 of 10

```{r progress1, echo=FALSE, fig.height=3}
progress <- data.frame(
  segment = 1:10,
  completed = c(1, rep(0, 9))
)

ggplot(progress, aes(segment, y = 1, fill = factor(completed))) +
  geom_tile(height = 0.5, color = "white", size = 2) +
  scale_fill_manual(values = c("0" = "lightgray", "1" = colors[2])) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Your Progress Today")
```

**Bottom line: 10% Complete - Great start!**

---

# The Power of Sampling

.pull-left[
### A Real Story:
**Country A (Census):**
- Cost: $50 million
- Time: 3 years
- Coverage: 78%
- Quality: Poor

**Country B (Sample):**
- Cost: $2 million
- Time: 3 months
- Coverage: 95%
- Quality: Excellent
]

.pull-right[
```{r comparison1, echo=FALSE}
comparison <- data.frame(
  Method = rep(c("Census", "Sample"), each = 4),
  Metric = rep(c("Cost", "Time", "Coverage", "Quality"), 2),
  Value = c(10, 10, 78, 40, 90, 90, 95, 95)
)

ggplot(comparison, aes(Metric, Value, fill = Method)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = colors[c(5, 2)]) +
  labs(y = "Performance (%)") +
  theme_minimal() +
  theme(legend.position = "top")
```
]

**Bottom line: Sampling is better, faster, cheaper**

---

# What Is Survey Sampling?

.pull-left[
### Simple Definition:
**Learning about many from few**

### Like Testing Soup:
- Don't drink entire pot
- One spoonful tells you
- If it needs salt
- Same principle!
]

.pull-right[
```{r sampling_viz1, echo=FALSE}
set.seed(123)
pop <- data.frame(
  x = runif(1000, 0, 10),
  y = runif(1000, 0, 10),
  selected = c(rep(TRUE, 50), rep(FALSE, 950))
)

ggplot(pop, aes(x, y, color = selected)) +
  geom_point(size = 2, alpha = 0.7) +
  scale_color_manual(values = c("gray80", colors[2]),
                    labels = c("Population", "Sample")) +
  labs(title = "From 1000 to 50",
       color = "") +
  theme_minimal() +
  theme(legend.position = "top")
```
]

**Bottom line: Small samples reveal population truths**

---

# Interactive: Feel the Magic

.pull-left[
### Quick Experiment:
1. Count people in room
2. Estimate average height
3. Sample just 5 people
4. Calculate their average
5. Compare to estimate

### What You'll Find:
Sample average ≈ True average!
]

.pull-right[
```{r experiment1, echo=TRUE}
# Simulate our experiment
set.seed(456)
room_heights <- rnorm(30, 170, 10)

# True average
true_avg <- mean(room_heights)
print(paste("True:", round(true_avg, 1)))

# Sample of 5
sample_5 <- sample(room_heights, 5)
sample_avg <- mean(sample_5)
print(paste("Sample:", round(sample_avg, 1)))

# How close?
error <- abs(sample_avg - true_avg)
print(paste("Error:", round(error, 1), "cm"))
```
]

**Bottom line: Sampling works - you just proved it!**

---

# The Four Pillars

.pull-left[
### Essential Components:
1. **Target Population**
   - Who/what to study
   
2. **Sampling Frame**
   - Complete list
   
3. **Selection Method**
   - How to choose
   
4. **Estimation**
   - Calculate results
]

.pull-right[
```{r pillars1, echo=FALSE}
pillars <- data.frame(
  Pillar = 1:4,
  Name = c("Population", "Frame", "Selection", "Estimation"),
  Height = c(4, 3.5, 3, 2.5)
)

ggplot(pillars, aes(Pillar, Height, fill = factor(Pillar))) +
  geom_col(width = 0.7) +
  geom_text(aes(label = Name), vjust = -0.5) +
  scale_fill_manual(values = colors[1:4]) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_blank()) +
  labs(x = "", y = "")
```
]

**Bottom line: Each pillar must be strong**

---

# Common Misconceptions

.pull-left[
### Myths:
❌ "Bigger always better"
❌ "30% sample = 30% accurate"
❌ "Random = haphazard"
❌ "Can't trust small samples"

### Reality:
✅ Design matters more than size
✅ Absolute size determines precision
✅ Random = scientific selection
✅ Small samples work perfectly
]

.pull-right[
```{r myths1, echo=FALSE}
myths <- data.frame(
  Belief = c("Size", "Design", "Cost", "Speed"),
  Importance = c(30, 70, 20, 40)
)

ggplot(myths, aes(x = "", y = Importance, fill = Belief)) +
  geom_bar(stat = "identity") +
  coord_polar("y") +
  scale_fill_manual(values = colors[1:4]) +
  theme_void() +
  labs(title = "What Really Matters")
```
]

**Bottom line: Good design beats big samples**

---

# SADC Context

.pull-left[
### Our Challenges:
- 16 member states
- 350 million people
- 40+ languages
- Vast distances
- Limited resources

### Our Strengths:
- Shared commitment
- Regional cooperation
- Growing expertise
]

.pull-right[
```{r sadc_map1, echo=FALSE}
sadc <- data.frame(
  Country = c("SA", "Angola", "Zambia", "Tanzania", "DRC"),
  Capacity = c(85, 60, 70, 75, 55),
  Progress = c(90, 70, 75, 80, 65)
)

ggplot(sadc, aes(Capacity, Progress)) +
  geom_point(size = 8, color = colors[2]) +
  geom_text(aes(label = Country), color = "white") +
  labs(x = "Current Capacity (%)",
       y = "Recent Progress (%)") +
  theme_minimal()
```
]

**Bottom line: Together we're stronger**

---

# Energy Check!

.pull-left[
### Quick Refresh:
1. Stand and stretch
2. Take 3 deep breaths
3. Shake your hands
4. Smile at someone
5. Ready to continue!

**30 seconds - GO!**
]

.pull-right[
```{r energy1, echo=FALSE}
energy <- data.frame(
  Time = c("Before", "After"),
  Level = c(60, 90),
  x = 1:2
)

ggplot(energy, aes(x, Level, fill = Time)) +
  geom_col(width = 0.5) +
  scale_fill_manual(values = colors[c(5, 2)]) +
  ylim(0, 100) +
  labs(x = "", y = "Energy Level (%)") +
  theme_minimal() +
  theme(axis.text.x = element_blank())
```
]

**Bottom line: Movement boosts learning**

---

# Why Not Census?

.pull-left[
### Census Problems:
- Expensive ($20-50 per person)
- Slow (2-3 years)
- Quality issues
- Outdated quickly
- Resource intensive
]

.pull-right[
```{r census_cost1, echo=TRUE}
# Cost comparison calculation
census_cost <- function(pop, cost_per) {
  pop * cost_per
}

sample_cost <- function(n, cost_per) {
  n * cost_per * 5  # Higher per-unit
}

# For 10 million population
pop <- 10000000
census <- census_cost(pop, 20)
sample <- sample_cost(5000, 100)

print(paste("Census: $", census/1e6, "M"))
print(paste("Sample: $", sample/1e6, "M"))
print(paste("Savings: $", (census-sample)/1e6, "M"))
```
]

**Bottom line: Samples save millions**

---

# Types of Information

.pull-left[
### What Surveys Measure:

**Counts & Totals:**
- Unemployed persons
- Households in poverty

**Proportions & Rates:**
- Unemployment rate
- Literacy rate

**Averages:**
- Household income
- Family size
]

.pull-right[
```{r info_types1, echo=FALSE}
types <- data.frame(
  Type = c("Count", "Proportion", "Average", "Relationship"),
  Frequency = c(25, 35, 30, 10)
)

ggplot(types, aes(x = "", y = Frequency, fill = Type)) +
  geom_bar(stat = "identity") +
  coord_polar("y") +
  scale_fill_manual(values = colors[1:4]) +
  theme_void() +
  labs(title = "Survey Measures")
```
]

**Bottom line: Different measures, same principles**

---

# Your First Success

.pull-left[
### Let's Calculate Together:
```{r first_calc1, echo=TRUE}
# Household sizes in your area
households <- c(4, 3, 5, 2, 6, 4, 3, 5)

# Calculate average
avg_size <- mean(households)
print(paste("Average:", avg_size))

# Calculate total for 1000 HH
total_people <- avg_size * 1000
print(paste("Total people:", total_people))
```
]

.pull-right[
### You Just:
✅ Created data
✅ Calculated statistics
✅ Made inference
✅ Used R successfully

**Congratulations!**
]

**Bottom line: You're already doing statistics**

---

# Knowledge Check 1

.pull-left[
### Quick Quiz:

**Q1: Sampling is better because:**
a) It's lazy
b) It's cheaper AND more accurate
c) We don't care about everyone
d) It's traditional

**Q2: Sample size depends on:**
a) Population size only
b) Precision needed
c) Budget only
d) Random choice
]

.pull-right[
### Answers:
**Q1: b** - Cheaper AND more accurate
**Q2: b** - Precision needed

```{r quiz1, echo=FALSE}
results <- data.frame(
  Question = c("Q1", "Q2"),
  Correct = c(85, 70)
)

ggplot(results, aes(Question, Correct)) +
  geom_col(fill = colors[2], width = 0.5) +
  ylim(0, 100) +
  labs(y = "% Correct") +
  theme_minimal()
```
]

**Bottom line: Well done! Keep learning**

---

# Real SADC Example

.pull-left[
### South Africa LFS:
- Quarterly surveys
- 30,000 dwellings
- All 9 provinces
- Rotation design
- 80% response rate

### Results:
- Unemployment: 32.6% (±0.5%)
- Timely data
- Policy impact
]

.pull-right[
```{r sa_lfs1, echo=FALSE}
lfs <- data.frame(
  Quarter = c("Q1", "Q2", "Q3", "Q4"),
  Rate = c(32.1, 31.9, 32.6, 33.0),
  SE = c(0.5, 0.5, 0.6, 0.5)
)

ggplot(lfs, aes(Quarter, Rate)) +
  geom_col(fill = colors[3]) +
  geom_errorbar(aes(ymin = Rate - SE, 
                    ymax = Rate + SE),
                width = 0.2) +
  labs(y = "Unemployment Rate (%)") +
  theme_minimal()
```
]

**Bottom line: Quality data drives decisions**

---

# Total Survey Error

.pull-left[
### Error Sources:
1. **Coverage** - Missing people
2. **Sampling** - Natural variation
3. **Nonresponse** - Refusals
4. **Measurement** - Wrong answers
5. **Processing** - Data errors

### Key Insight:
Sampling error often smallest!
]

.pull-right[
```{r tse1, echo=FALSE}
errors <- data.frame(
  Source = c("Coverage", "Sampling", "Nonresponse",
             "Measurement", "Processing"),
  Size = c(4, 2, 5, 6, 2)
)

ggplot(errors, aes(reorder(Source, Size), Size)) +
  geom_col(fill = colors[4]) +
  coord_flip() +
  labs(x = "", y = "Error Contribution (%)") +
  theme_minimal()
```
]

**Bottom line: Control what you can**

---

# Coverage Error Example

.pull-left[
### The Problem:
- New settlements unmapped
- 15% of population missing
- Poor areas excluded
- Bias in estimates

### The Solution:
- Update frames regularly
- Use satellite imagery
- Field verification
- Multiple sources
]

.pull-right[
```{r coverage1, echo=TRUE}
# Impact of coverage error
true_poverty <- 0.35  # 35% actual
covered_poverty <- 0.28  # In frame
coverage_rate <- 0.85  # 85% covered

# Biased estimate
biased <- covered_poverty
print(paste("Biased:", biased))

# Corrected estimate
corrected <- covered_poverty / coverage_rate
print(paste("Corrected:", round(corrected, 2)))

# Error size
error <- true_poverty - biased
print(paste("Error:", round(error*100, 1), "%"))
```
]

**Bottom line: Coverage errors cause major bias**

---

# Practice Exercise 1

.pull-left[
### Your Task:
Design a survey for your country:
- Population: 5 million
- Budget: $200,000
- Topic: Unemployment
- Precision: ±2%

### Consider:
1. Sample size needed
2. Stratification
3. Data collection mode
4. Timeline
]

.pull-right[
### Work Space:
```{r practice1, echo=TRUE, eval=FALSE}
# Your calculations here
population <- 5000000
budget <- 200000
precision <- 0.02

# Sample size formula
n <- _______  # Fill in

# Cost per interview
cost_per <- budget / n

# Print results
print(paste("Sample size:", n))
print(paste("Cost per:", cost_per))
```
]

**Bottom line: Take 5 minutes - Try it!**

---

# Solution to Exercise 1

.pull-left[
### Calculation:
```{r solution1, echo=TRUE}
# Given parameters
population <- 5000000
budget <- 200000
precision <- 0.02
confidence <- 0.95

# Sample size calculation
z <- qnorm((1 + confidence) / 2)
p <- 0.30  # Expected unemployment
n <- (z^2 * p * (1-p)) / precision^2
n <- ceiling(n)

print(paste("Sample needed:", n))

# Can we afford it?
cost_per <- budget / n
print(paste("Cost per interview: $", 
            round(cost_per, 2)))
```
]

.pull-right[
### Interpretation:
- Need 2,017 interviews
- $99 per interview available
- Feasible with good planning
- Consider clustering to reduce costs

**Well done if you got close!**
]

**Bottom line: Practice builds confidence**

---

# Group Discussion

.pull-left[
### Share With Your Table:
1. Your biggest sampling challenge
2. One thing you learned so far
3. How you'll apply it

### Time: 3 minutes

### Then:
Each table shares one insight
]

.pull-right[
```{r discussion1, echo=FALSE}
discuss <- data.frame(
  Stage = c("Individual", "Table", "Room"),
  Time = c(1, 2, 2),
  People = c(1, 5, 30)
)

ggplot(discuss, aes(Stage, People, fill = factor(Time))) +
  geom_col() +
  scale_fill_manual(values = colors[2:4], name = "Minutes") +
  labs(y = "Participants") +
  theme_minimal()
```
]

**Bottom line: Learning together is powerful**

---

# Progress Check 2 of 10

```{r progress2, echo=FALSE, fig.height=3}
progress <- data.frame(
  segment = 1:10,
  completed = c(rep(1, 2), rep(0, 8))
)

ggplot(progress, aes(segment, y = 1, fill = factor(completed))) +
  geom_tile(height = 0.5, color = "white", size = 2) +
  scale_fill_manual(values = c("0" = "lightgray", "1" = colors[2])) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Your Progress Today")
```

**Bottom line: 20% Complete - Excellent progress!**

---

# Probability Sampling

.pull-left[
### Key Principles:
1. **Known probability** of selection
2. **Random mechanism** used
3. **No human judgment**
4. **Mathematical basis**

### Result:
- Unbiased estimates
- Calculable precision
- Scientific validity
]

.pull-right[
```{r probability1, echo=FALSE}
set.seed(789)
prob_demo <- data.frame(
  Method = rep(c("Probability", "Convenience"), each = 100),
  Value = c(rnorm(100, 50, 10), rnorm(100, 65, 5))
)

ggplot(prob_demo, aes(Value, fill = Method)) +
  geom_histogram(alpha = 0.7, bins = 20) +
  geom_vline(xintercept = 50, linetype = "dashed") +
  scale_fill_manual(values = colors[c(2, 5)]) +
  labs(x = "Estimate", y = "Frequency") +
  theme_minimal()
```
]

**Bottom line: Only probability sampling gives valid inference**

---

# Types of Probability Sampling

.pull-left[
### Main Types:
1. **Simple Random** - Equal chance
2. **Systematic** - Every kth unit
3. **Stratified** - Groups first
4. **Cluster** - Area groups
5. **Multi-stage** - Combinations
]

.pull-right[
```{r types1, echo=FALSE}
types <- data.frame(
  Type = c("SRS", "Systematic", "Stratified", 
           "Cluster", "Multi-stage"),
  Complexity = c(1, 2, 3, 3, 5),
  Efficiency = c(3, 3, 5, 2, 4)
)

ggplot(types, aes(Complexity, Efficiency)) +
  geom_point(size = 10, color = colors[2]) +
  geom_text(aes(label = Type), color = "white", size = 3) +
  labs(x = "Complexity Level", y = "Statistical Efficiency") +
  theme_minimal()
```
]

**Bottom line: Choose based on your needs**

---

# Simple Random Sampling (SRS)

.pull-left[
### How It Works:
- Every unit has equal chance
- Selection probability = n/N
- Like lottery drawing
- Pure randomness

### When to Use:
- Good frame available
- Population homogeneous
- Can reach all units
]

.pull-right[
```{r srs_demo, echo=TRUE}
# SRS in action
population <- 1:1000
sample_size <- 50

# Draw sample
set.seed(123)
my_sample <- sample(population, 
                    sample_size,
                    replace = FALSE)

# Check randomness
head(my_sample, 10)

# Probability of selection
prob <- sample_size / length(population)
print(paste("Selection prob:", prob))
```
]

**Bottom line: SRS is the gold standard**

---

# Systematic Sampling

.pull-left[
### The Method:
1. Calculate interval k = N/n
2. Random start (1 to k)
3. Select every kth unit
4. Automatic spread

### Advantages:
- Easy to implement
- Good coverage
- No frame needed
- Field friendly
]

.pull-right[
```{r systematic_demo, echo=TRUE}
# Systematic sampling
N <- 1000  # Population size
n <- 50    # Sample size
k <- N/n   # Interval

# Random start
set.seed(456)
start <- sample(1:k, 1)
print(paste("Start:", start))

# Generate sample
systematic_sample <- seq(start, N, by = k)
head(systematic_sample, 10)

length(systematic_sample)  # Verify size
```
]

**Bottom line: Simple and effective**

---

# Warning: Hidden Patterns

.pull-left[
### The Danger:
If population has cycles matching k:
- Every 7th day (weekly)
- Every 12th month
- Every 10th house

### Result:
Severe bias possible!

### Solution:
Check for periodicity first
]

.pull-right[
```{r pattern_danger, echo=FALSE}
# Demonstrate periodicity problem
days <- 1:28
weekly_pattern <- sin(2 * pi * days / 7) + 5
daily_values <- weekly_pattern + rnorm(28, 0, 0.2)

# Systematic sample every 7 days
systematic_days <- seq(7, 28, by = 7)

pattern_df <- data.frame(
  Day = days,
  Value = daily_values,
  Sampled = days %in% systematic_days
)

ggplot(pattern_df, aes(Day, Value)) +
  geom_line(color = colors[1], size = 1.5) +
  geom_point(data = filter(pattern_df, Sampled),
             color = colors[5], size = 4) +
  labs(title = "Dangerous Pattern Alignment",
       y = "Measurement") +
  theme_minimal()
```
]

**Bottom line: Always check for cycles**

---

# Stratified Sampling

.pull-left[
### The Power:
- Divide population into groups
- Sample from each group
- Combine results
- Higher precision!

### Example Strata:
- Urban/Rural
- Provinces
- Age groups
- Income levels
]

.pull-right[
```{r stratified_viz, echo=FALSE}
set.seed(789)
strat_data <- data.frame(
  Stratum = rep(c("Urban", "Rural"), each = 500),
  Value = c(rnorm(500, 60, 10), rnorm(500, 40, 15)),
  Selected = c(sample(c(rep(TRUE, 30), rep(FALSE, 470))),
               sample(c(rep(TRUE, 20), rep(FALSE, 480))))
)

ggplot(strat_data, aes(Stratum, Value, color = Selected)) +
  geom_jitter(width = 0.3, alpha = 0.6) +
  scale_color_manual(values = c("gray70", colors[2])) +
  labs(title = "Stratified Selection") +
  theme_minimal()
```
]

**Bottom line: Stratification reduces variance**

---

# Allocation in Stratified Sampling

.pull-left[
### Three Methods:

**1. Proportional:**
- Sample ∝ stratum size
- Self-weighting
- Simple analysis

**2. Equal:**
- Same n per stratum
- Good for comparisons

**3. Optimal (Neyman):**
- Consider variability
- Most efficient
]

.pull-right[
```{r allocation_demo, echo=TRUE}
# Allocation example
strata <- data.frame(
  Region = c("North", "South", "East", "West"),
  N = c(1000, 3000, 1500, 500),
  S = c(10, 15, 12, 8)  # Std dev
)

n_total <- 200

# Proportional allocation
strata$n_prop <- round(n_total * strata$N / sum(strata$N))

# Optimal allocation
strata$n_opt <- round(n_total * strata$N * strata$S / 
                      sum(strata$N * strata$S))

print(strata[, c("Region", "n_prop", "n_opt")])
```
]

**Bottom line: Optimal allocation maximizes precision**

---

# Interactive Exercise: Design Your Strata

.pull-left[
### Your Country Has:
- 4 provinces
- Urban/Rural split
- 3 income groups

### Task:
1. Define strata
2. Decide allocation
3. Calculate sample sizes

**Work in pairs - 3 minutes**
]

.pull-right[
```{r strata_exercise, echo=TRUE, eval=FALSE}
# Your workspace
provinces <- c("A", "B", "C", "D")
areas <- c("Urban", "Rural")
income <- c("Low", "Middle", "High")

# Total strata = ?
# Sample size = 1000

# Your allocation:
# Province A Urban Low: ___
# Province A Urban Middle: ___
# ...

# Share your logic!
```
]

**Bottom line: Practice stratification design**

---

# Cluster Sampling

.pull-left[
### Why Cluster?
- Reduce travel costs
- No complete frame needed
- Natural groupings
- Operational efficiency

### Trade-off:
Less precision per person
BUT much cheaper
= More total precision per dollar
]

.pull-right[
```{r cluster_viz, echo=FALSE}
set.seed(321)
clusters <- expand.grid(
  cluster = 1:20,
  unit = 1:10
) %>%
  mutate(
    x = cluster + runif(200, -0.4, 0.4),
    y = unit + runif(200, -0.4, 0.4),
    selected_cluster = cluster %in% c(3, 7, 12, 15, 19),
    selected = selected_cluster
  )

ggplot(clusters, aes(x, y, color = selected)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("gray80", colors[2])) +
  labs(title = "Cluster Sampling: Entire Groups Selected") +
  theme_minimal() +
  theme(legend.position = "none")
```
]

**Bottom line: Clusters save money**

---

# Design Effect (DEFF)

.pull-left[
### What Is DEFF?
Variance increase from complex design

**Formula:**
DEFF = Var(complex) / Var(SRS)

### Typical Values:
- SRS: DEFF = 1.0
- Stratified: DEFF < 1.0 ✓
- Cluster: DEFF > 1.0 ✗

### Impact:
Effective n = n / DEFF
]

.pull-right[
```{r deff_demo, echo=TRUE}
# Calculate design effect
n_srs <- 1000
var_srs <- 2.5

# Cluster design
n_cluster <- 1000
var_cluster <- 4.0

# Calculate DEFF
deff <- var_cluster / var_srs
print(paste("DEFF:", deff))

# Effective sample size
n_eff <- n_cluster / deff
print(paste("Effective n:", round(n_eff)))

# Need larger sample
n_needed <- n_srs * deff
print(paste("Need n =", round(n_needed), 
            "for same precision"))
```
]

**Bottom line: DEFF quantifies efficiency loss**

---

# Real Calculation: ICC

.pull-left[
### Intraclass Correlation:
How similar are units in same cluster?

**ICC Formula:**
ρ = σ²between / (σ²between + σ²within)

### DEFF Formula:
DEFF = 1 + (b - 1) × ρ

where b = cluster size
]

.pull-right[
```{r icc_calc, echo=TRUE}
# ICC impact on DEFF
cluster_size <- 20
icc_values <- seq(0, 0.3, 0.05)

# Calculate DEFF for each ICC
deff_values <- 1 + (cluster_size - 1) * icc_values

# Display relationship
icc_impact <- data.frame(
  ICC = icc_values,
  DEFF = deff_values
)

print(icc_impact)

# Typical ICC values:
# Income: 0.05-0.10
# Education: 0.10-0.20  
# Health: 0.01-0.05
```
]

**Bottom line: Small ICC → big impact with clusters**

---

# Multi-Stage Sampling

.pull-left[
### Common Design:
**Stage 1:** Select districts
**Stage 2:** Select villages
**Stage 3:** Select households
**Stage 4:** Select person

### Advantages:
- No full frame needed
- Cost efficient
- Flexible
- Practical
]

.pull-right[
```{r multistage_viz, echo=FALSE}
stages <- data.frame(
  Stage = factor(1:4, labels = c("Districts", "Villages", 
                                  "Households", "Persons")),
  Units = c(50, 10, 20, 1),
  Total = c(50, 500, 10000, 10000)
)

ggplot(stages, aes(Stage, Units)) +
  geom_col(fill = colors[3], width = 0.6) +
  geom_text(aes(label = paste(Units, "selected")), 
            vjust = -0.5) +
  labs(y = "Units per Previous Stage",
       title = "Cascading Selection") +
  theme_minimal()
```
]

**Bottom line: Each stage adds complexity but saves cost**

---

# Energy Break!

.pull-left[
### Stand and Stretch:
1. Reach for the ceiling (5 sec)
2. Touch your toes (5 sec)
3. Twist left and right
4. Roll your shoulders
5. Take 3 deep breaths

**30 seconds total**
]

.pull-right[
```{r energy_meter, echo=FALSE}
before_after <- data.frame(
  Time = factor(c("Before", "After"), 
                levels = c("Before", "After")),
  Energy = c(60, 90)
)

ggplot(before_after, aes(Time, Energy, fill = Time)) +
  geom_col(width = 0.5) +
  scale_fill_manual(values = colors[c(5, 2)]) +
  ylim(0, 100) +
  geom_text(aes(label = paste0(Energy, "%")), 
            vjust = -0.5, size = 5) +
  labs(y = "Energy Level") +
  theme_minimal() +
  theme(legend.position = "none")
```
]

**Bottom line: Movement refreshes the mind**

---

# Progress Check 4 of 10

```{r progress4, echo=FALSE, fig.height=3}
progress <- data.frame(
  segment = 1:10,
  completed = c(rep(1, 4), rep(0, 6))
)

ggplot(progress, aes(segment, y = 1, fill = factor(completed))) +
  geom_tile(height = 0.5, color = "white", size = 2) +
  scale_fill_manual(values = c("0" = "lightgray", "1" = colors[2])) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Your Progress Today")
```

**Bottom line: 40% Complete - Almost halfway!**

---

# Sampling Weights

.pull-left[
### Why Weight?
- Unequal selection probabilities
- Adjust for design
- Correct estimates

### Basic Weight:
Weight = 1 / (selection probability)

### Example:
If P(selection) = 0.01
Then weight = 100
(Each represents 100 in population)
]

.pull-right[
```{r weights_demo, echo=TRUE}
# Calculate weights
sample_data <- data.frame(
  stratum = c("Urban", "Urban", "Rural", "Rural"),
  n = c(100, 100, 50, 50),
  N = c(2000, 2000, 3000, 3000)
)

# Selection probabilities
sample_data$prob <- sample_data$n / sample_data$N

# Calculate weights
sample_data$weight <- 1 / sample_data$prob

print(sample_data[, c("stratum", "prob", "weight")])

# Total representation
total_rep <- sum(sample_data$n * sample_data$weight)
print(paste("Total represented:", total_rep))
```
]

**Bottom line: Weights restore representation**

---

# Practical Weight Adjustments

.pull-left[
### Three-Step Process:

**1. Design Weights:**
- From selection probability

**2. Nonresponse Adjustment:**
- Increase for missing units

**3. Calibration:**
- Match known totals

### Final Weight:
W_final = W_design × W_nr × W_cal
]

.pull-right[
```{r weight_adjust, echo=TRUE}
# Weight adjustments example
unit_data <- data.frame(
  id = 1:5,
  design_wt = c(100, 100, 150, 150, 200),
  responded = c(TRUE, TRUE, FALSE, TRUE, TRUE)
)

# Response rate by group
response_rate <- 0.8  # 80% responded

# Nonresponse adjustment
unit_data$nr_adj <- ifelse(unit_data$responded,
                           1/response_rate, 
                           0)

# Final weights
unit_data$final_wt <- unit_data$design_wt * 
                      unit_data$nr_adj

print(unit_data[unit_data$responded, 
               c("id", "design_wt", "final_wt")])
```
]

**Bottom line: Adjust weights for reality**

---

# Your First R Success Story

.pull-left[
### Let's Build Confidence:
```{r first_success, echo=TRUE}
# You can do this!
# Simulate a simple survey
set.seed(999)

# Population
income <- rnorm(10000, 50000, 15000)

# Take a sample
n <- 100
sample_inc <- sample(income, n)

# Calculate estimates
mean_est <- mean(sample_inc)
se_est <- sd(sample_inc) / sqrt(n)
ci_low <- mean_est - 1.96 * se_est
ci_high <- mean_est + 1.96 * se_est

# Report results
cat("Estimate: $", round(mean_est), "\n")
cat("95% CI: [$", round(ci_low), 
    " - $", round(ci_high), "]", sep="")
```
]

.pull-right[
### You Just:
✅ Generated population
✅ Drew random sample
✅ Calculated estimate
✅ Computed confidence interval
✅ Interpreted results

**Congratulations! You're doing real survey statistics!**
]

**Bottom line: You have the skills already**

---

# Knowledge Check 2

.pull-left[
### Test Yourself:

**Q1: Best design for provinces?**
a) SRS
b) Systematic
c) Stratified ✓
d) Convenient

**Q2: DEFF > 1 means:**
a) Design is perfect
b) Larger variance than SRS ✓
c) Smaller variance than SRS
d) Error in calculation

**Q3: Weight = 50 means:**
a) Person weighs 50kg
b) Represents 50 people ✓
c) 50% chance selected
d) Sample size is 50
]

.pull-right[
### Understanding Check:

Rate your confidence (1-5):
- [ ] I understand probability sampling
- [ ] I can explain stratification
- [ ] I know when to use clusters
- [ ] I can calculate weights
- [ ] I'm ready to continue

If any < 3, ask now!
]

**Bottom line: Check understanding before moving on**

---

# Common Mistakes to Avoid

.pull-left[
### Top 5 Errors:

**1. Convenience sampling**
"Just survey who's available"

**2. Ignoring nonresponse**
"70% responded is fine"

**3. Wrong stratification**
"Let's stratify by shoe size"

**4. Forgetting weights**
"Just average everything"

**5. No quality control**
"The data looks reasonable"
]

.pull-right[
```{r mistakes_impact, echo=FALSE}
mistakes <- data.frame(
  Error = c("Convenience", "Nonresponse", "Bad strata", 
            "No weights", "No QC"),
  Impact = c(45, 30, 20, 35, 25),
  Frequency = c(70, 80, 30, 60, 50)
)

ggplot(mistakes, aes(Frequency, Impact)) +
  geom_point(size = 8, color = colors[5]) +
  geom_text(aes(label = Error), size = 3, color = "white") +
  labs(x = "How Often It Happens (%)",
       y = "Bias Impact (%)") +
  theme_minimal()
```
]

**Bottom line: Avoid these for quality data**

---

# Group Exercise: Spot the Error

.pull-left[
### Scenario:
"We surveyed 1000 people at shopping malls on Saturday afternoon about unemployment. Results show 5% unemployment rate. National statistics show 25%. Our sample is large, so we must be right!"

### Questions:
1. What went wrong?
2. What's the bias?
3. How to fix it?

**Discuss for 2 minutes**
]

.pull-right[
### Issues to Find:
- Selection bias (mall visitors)
- Timing bias (Saturday)
- Coverage error (no unemployed at malls)
- Wrong target population
- No probability sampling

### The Fix:
- Random household selection
- All days/times
- Proper frame
- Weights for nonresponse
]

**Bottom line: Design matters more than size**

---

# SADC Success Story

.pull-left[
### Botswana's Transformation:

**Before (2015):**
- Paper surveys
- 6-month processing
- 20% missing data
- Low confidence

**After Training (2020):**
- Tablet collection
- 2-week processing
- 2% missing data
- International recognition
]

.pull-right[
```{r success_story_2, echo=FALSE}
years <- 2015:2020
improvement <- data.frame(
  Year = years,
  Quality = c(40, 45, 55, 70, 85, 95),
  Speed = c(30, 35, 50, 75, 90, 95),
  Cost = c(100, 95, 85, 70, 60, 50)
)

improvement_long <- improvement %>%
  pivot_longer(cols = -Year, names_to = "Metric", 
               values_to = "Performance")

ggplot(improvement_long, aes(Year, Performance, 
                             color = Metric)) +
  geom_line(size = 2) +
  geom_point(size = 3) +
  scale_color_manual(values = colors[c(2, 3, 5)]) +
  labs(y = "Performance Index") +
  theme_minimal()
```
]

**Bottom line: Proper training transforms nations**

---

# Reflection Moment

.pull-left[
### Think About:
1. One thing that surprised you
2. One concept you'll use immediately  
3. One question you still have

### Write it down:
(30 seconds)

### Share with neighbor:
(1 minute)
]

.pull-right[
```{r reflection_viz, echo=FALSE}
reflect <- data.frame(
  Stage = c("Learn", "Think", "Share", "Apply"),
  Value = c(1, 2, 3, 4),
  x = 1:4
)

ggplot(reflect, aes(x, Value)) +
  geom_segment(aes(xend = x, yend = 0), 
               color = colors[2], size = 2) +
  geom_point(size = 10, color = colors[2]) +
  geom_text(aes(label = Stage), color = "white") +
  labs(x = "", y = "") +
  theme_minimal() +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```
]

**Bottom line: Reflection deepens learning**

---

# Setting Up for Success

.pull-left[
### Your Toolkit:
✅ Sampling concepts
✅ Design options
✅ Weight calculations
✅ R basics
✅ Quality mindset

### Next Section:
Deep dive into sampling theory
and mathematical foundations
]

.pull-right[
### Action Items:
1. Install R packages:
```{r install_demo, eval=FALSE}
install.packages(c("survey", 
                   "sampling",
                   "tidyverse"))
```

2. Download materials
3. Prepare questions
4. Get ready to calculate!
]

**Bottom line: You're ready for the next level**

---

# Key Takeaways So Far

.pull-left[
### You've Learned:
1. **Sampling saves money** while improving quality
2. **Probability sampling** ensures valid inference
3. **Design impacts precision** more than size
4. **Weights correct** for design and nonresponse
5. **R makes it practical** and reproducible
]

.pull-right[
```{r takeaways_viz, echo=FALSE}
takeaways <- data.frame(
  Concept = c("Cost", "Quality", "Speed", 
              "Precision", "Capability"),
  Before = c(30, 40, 35, 45, 25),
  After = c(90, 95, 92, 88, 95)
)

takeaways_long <- takeaways %>%
  pivot_longer(cols = c(Before, After), 
               names_to = "When", 
               values_to = "Level")

ggplot(takeaways_long, aes(Concept, Level, fill = When)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = colors[c(5, 2)]) +
  labs(y = "Competency Level (%)") +
  theme_minimal() +
  coord_flip()
```
]

**Bottom line: Transformation is happening**

---

# Preview: What's Next

.pull-left[
### Part 2 (Next 50 slides):
- Sampling distributions
- Central Limit Theorem
- Variance estimation
- Standard errors
- Confidence intervals
- Power calculations

### You Will:
- Master the mathematics
- Build intuition
- Program solutions
- Gain confidence
]

.pull-right[
```{r preview_next, echo=FALSE}
next_topics <- data.frame(
  Topic = c("Distributions", "CLT", "Variance", 
            "Errors", "Confidence", "Power"),
  Complexity = c(2, 3, 4, 3, 2, 4),
  Importance = c(5, 5, 4, 5, 5, 3)
)

ggplot(next_topics, aes(Complexity, Importance)) +
  geom_point(size = 10, color = colors[3]) +
  geom_text(aes(label = Topic), size = 3) +
  labs(x = "Complexity Level",
       y = "Importance Level") +
  scale_x_continuous(limits = c(1, 5)) +
  scale_y_continuous(limits = c(1, 5)) +
  theme_minimal()
```
]

**Bottom line: Ready for deeper understanding**

---

# Progress Check 5 of 10

```{r progress5, echo=FALSE, fig.height=3}
progress <- data.frame(
  segment = 1:10,
  completed = c(rep(1, 5), rep(0, 5))
)

ggplot(progress, aes(segment, y = 1, fill = factor(completed))) +
  geom_tile(height = 0.5, color = "white", size = 2) +
  scale_fill_manual(values = c("0" = "lightgray", "1" = colors[2])) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Your Progress Today")
```

**Bottom line: 50% Complete - Halfway there!**

---

class: center, middle

# End of Part 1

## You've Successfully Completed the Foundation!

### Take a 10-minute break

### Next: Part 2 - Sampling Theory Deep Dive

**Remember: You're building expertise that will transform your nation's statistics**

---
class: center, middle

# Part 2: Sampling Theory

## From Intuition to Mathematical Understanding

### "The theory is the map; practice is the journey"

---

# Welcome Back!

.pull-left[
### Part 1 Recap:
✅ Why sampling matters
✅ Types of sampling
✅ Basic designs
✅ Weights introduction
✅ R fundamentals

### Part 2 Focus:
The mathematical foundation that makes it all work
]

.pull-right[
```{r welcome_back, echo=FALSE}
library(tidyverse)
colors <- c("#003F7F", "#0066CC", "#4A90E2", "#FFA500", "#FF6B35")

journey <- data.frame(
  Part = c("Foundation", "Theory", "Mathematics", "Programming"),
  Completed = c(100, 20, 0, 0),
  x = 1:4
)

ggplot(journey, aes(x, Completed)) +
  geom_col(fill = colors[2], width = 0.6) +
  geom_text(aes(label = paste0(Completed, "%")), 
            vjust = -0.5, size = 4) +
  scale_x_continuous(breaks = 1:4, 
                     labels = journey$Part) +
  ylim(0, 120) +
  labs(x = "", y = "Progress (%)") +
  theme_minimal()
```
]

**Bottom line: Let's build your theoretical foundation**

---

# Population vs Sample

.pull-left[
### Population (N):
- Complete set of units
- True parameters (μ, σ²)
- Usually unknown
- Fixed values
- What we want to know

### Sample (n):
- Subset of population
- Statistics (x̄, s²)
- Observable
- Random values
- What we measure
]

.pull-right[
```{r pop_sample_viz, echo=FALSE}
set.seed(123)
population <- data.frame(
  x = rnorm(1000, 50, 10),
  type = "Population"
)

sample_data <- data.frame(
  x = sample(population$x, 50),
  type = "Sample"
)

combined <- rbind(population, sample_data)

ggplot(combined, aes(x, fill = type)) +
  geom_histogram(alpha = 0.7, bins = 30, position = "identity") +
  geom_vline(data = combined %>% 
               group_by(type) %>% 
               summarise(mean = mean(x)),
             aes(xintercept = mean, color = type),
             linetype = "dashed", size = 1.5) +
  scale_fill_manual(values = colors[c(1, 4)]) +
  scale_color_manual(values = colors[c(1, 4)]) +
  labs(x = "Value", y = "Frequency", fill = "Type", color = "Mean") +
  theme_minimal()
```
]

**Bottom line: Samples estimate population truth**

---

# The Fundamental Problem

.pull-left[
### The Challenge:
**We have:** One sample
**We need:** Population parameter
**Problem:** Uncertainty!

### The Solution:
**Probability theory** tells us:
- How close we likely are
- Confidence in our estimate
- Sample size needed
]

.pull-right[
```{r uncertainty_demo, echo=TRUE}
# The uncertainty problem
set.seed(456)
true_mean <- 100  # Unknown in reality

# Take 5 different samples
samples <- replicate(5, {
  mean(rnorm(30, true_mean, 15))
})

print(round(samples, 1))

# Range of estimates
range(samples)

# Key insight: Different samples → Different estimates
# But all are close to truth!
```
]

**Bottom line: Variability is natural and predictable**

---

# Sampling Distribution Concept

.pull-left[
### Mental Experiment:
1. Take sample of size n
2. Calculate mean
3. Repeat 10,000 times
4. Plot all means

### Result:
**Sampling distribution** of x̄

### Properties:
- Centered at μ
- Spread = σ/√n
- Shape → Normal
]

.pull-right[
```{r sampling_dist_viz, echo=FALSE}
set.seed(789)
# Simulate sampling distribution
n_samples <- 10000
sample_size <- 30
true_mean <- 50
true_sd <- 10

sample_means <- replicate(n_samples, {
  mean(rnorm(sample_size, true_mean, true_sd))
})

sampling_df <- data.frame(means = sample_means)

ggplot(sampling_df, aes(means)) +
  geom_histogram(aes(y = ..density..), 
                 bins = 50, fill = colors[2], alpha = 0.7) +
  stat_function(fun = dnorm, 
                args = list(mean = true_mean, 
                           sd = true_sd/sqrt(sample_size)),
                color = colors[1], size = 1.5) +
  geom_vline(xintercept = true_mean, 
             linetype = "dashed", color = colors[5]) +
  labs(x = "Sample Mean", y = "Density",
       title = "10,000 Sample Means") +
  theme_minimal()
```
]

**Bottom line: Sample means cluster around truth**

---

# Interactive Simulation

.pull-left[
### Try It Yourself:
```{r interactive_sim, echo=TRUE}
# Simulate sampling distribution
simulate_sampling <- function(n, samples = 1000) {
  true_mean <- 100
  true_sd <- 20
  
  # Generate many sample means
  means <- replicate(samples, {
    mean(rnorm(n, true_mean, true_sd))
  })
  
  # Calculate properties
  cat("Mean of means:", round(mean(means), 2), "\n")
  cat("SD of means:", round(sd(means), 2), "\n")
  cat("Theoretical SD:", round(true_sd/sqrt(n), 2), "\n")
  
  return(means)
}

# Try different sample sizes
results_10 <- simulate_sampling(10)
```
]

.pull-right[
```{r sim_comparison, echo=TRUE}
# Compare sample sizes
results_30 <- simulate_sampling(30)
results_100 <- simulate_sampling(100)

# Key insight: Larger n → Smaller SD
# This is why sample size matters!
```
]

**Bottom line: Larger samples → More precision**

---

# The Central Limit Theorem

.pull-left[
### The Magic:
**For ANY population distribution:**

If n is large enough:
x̄ ~ Normal(μ, σ²/n)

### Requirements:
- Random sampling
- Independent observations
- n ≥ 30 (rule of thumb)

### Why It Matters:
Works regardless of population shape!
]

.pull-right[
```{r clt_demo, echo=FALSE}
set.seed(321)
# Show CLT with skewed population
n_samples <- 1000

# Exponential population (very skewed)
pop_exp <- rexp(10000, rate = 1)

# Sample means for different n
samples_5 <- replicate(n_samples, mean(sample(pop_exp, 5)))
samples_30 <- replicate(n_samples, mean(sample(pop_exp, 30)))
samples_100 <- replicate(n_samples, mean(sample(pop_exp, 100)))

clt_df <- data.frame(
  value = c(samples_5, samples_30, samples_100),
  n = factor(rep(c(5, 30, 100), each = n_samples))
)

ggplot(clt_df, aes(value, fill = n)) +
  geom_histogram(alpha = 0.6, bins = 30) +
  facet_wrap(~n, scales = "free_y", 
             labeller = labeller(n = function(x) paste("n =", x))) +
  scale_fill_manual(values = colors[c(5, 3, 2)]) +
  labs(x = "Sample Mean", y = "Frequency",
       title = "CLT: Skewed Population → Normal Sample Means") +
  theme_minimal() +
  theme(legend.position = "none")
```
]

**Bottom line: CLT is the foundation of inference**

---

# CLT in Action

.pull-left[
### Real Example:
Income is highly skewed
But sample means are normal!

```{r clt_real, echo=TRUE}
# Simulate skewed income data
set.seed(654)
incomes <- rgamma(10000, shape = 2, scale = 15000)

# Population is skewed
cat("Population median:", median(incomes), "\n")
cat("Population mean:", mean(incomes), "\n")
cat("Skewness evident!\n\n")

# But sample means are normal
sample_means <- replicate(1000, {
  mean(sample(incomes, 50))
})

# Test for normality
shapiro.test(sample(sample_means, 100))$p.value
# p > 0.05: Cannot reject normality!
```
]

.pull-right[
```{r clt_visual, echo=FALSE}
par(mfrow = c(2,1), mar = c(4,4,2,1))

# Population distribution
hist(incomes[1:1000], breaks = 30, main = "Population: Skewed",
     xlab = "Income", col = colors[5], border = "white")

# Sampling distribution
hist(sample_means, breaks = 30, main = "Sample Means: Normal!",
     xlab = "Mean Income", col = colors[2], border = "white")
```
]

**Bottom line: CLT enables normal-based inference**

---

# Standard Error Fundamentals

.pull-left[
### Definition:
Standard deviation of sampling distribution

### Formula:
**SE(x̄) = σ/√n**

### What It Tells Us:
- Precision of estimate
- Typical error size
- Confidence interval width

### Key Relationships:
- ↑ n → ↓ SE
- ↑ σ → ↑ SE
]

.pull-right[
```{r se_demo, echo=TRUE}
# Standard error in practice
population_sd <- 15
sample_sizes <- c(10, 30, 100, 500, 1000)

# Calculate SE for each n
se_values <- population_sd / sqrt(sample_sizes)

se_df <- data.frame(
  n = sample_sizes,
  SE = se_values,
  Precision = 1/se_values  # Inverse relationship
)

print(se_df)

# Diminishing returns
# Doubling n → SE reduced by √2 only
```
]

**Bottom line: SE quantifies uncertainty**

---

# Visualizing Standard Error

.pull-left[
### SE Shrinks with n:
```{r se_viz, echo=FALSE}
n_seq <- seq(10, 1000, by = 10)
se_seq <- 15 / sqrt(n_seq)

se_plot_df <- data.frame(
  n = n_seq,
  SE = se_seq
)

ggplot(se_plot_df, aes(n, SE)) +
  geom_line(color = colors[1], size = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = colors[5]) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = colors[4]) +
  annotate("text", x = 800, y = 1.2, label = "SE = 1") +
  annotate("text", x = 800, y = 0.7, label = "SE = 0.5") +
  labs(x = "Sample Size", y = "Standard Error",
       title = "Diminishing Returns") +
  theme_minimal()
```
]

.pull-right[
### Cost vs Benefit:
```{r cost_benefit, echo=FALSE}
cost_df <- data.frame(
  n = seq(100, 2000, by = 100),
  Cost = seq(100, 2000, by = 100) * 50,  # Linear cost
  Precision = 1 / (15 / sqrt(seq(100, 2000, by = 100)))  # Precision gain
)

cost_df$Precision_scaled <- scale(cost_df$Precision) * 50000 + 50000

ggplot(cost_df, aes(n)) +
  geom_line(aes(y = Cost), color = colors[5], size = 2) +
  geom_line(aes(y = Precision_scaled), color = colors[2], size = 2) +
  annotate("text", x = 1500, y = 80000, label = "Cost", color = colors[5]) +
  annotate("text", x = 1500, y = 60000, label = "Precision", color = colors[2]) +
  labs(x = "Sample Size", y = "Value",
       title = "Find the Sweet Spot") +
  theme_minimal()
```
]

**Bottom line: Balance precision with resources**

---

# Finite Population Correction

.pull-left[
### When Needed:
If n/N > 0.05 (sampling > 5%)

### Formula:
**SE = (σ/√n) × √((N-n)/(N-1))**

### FPC Factor:
√((N-n)/(N-1))

### Impact:
- Reduces standard error
- More precise estimates
- Important for small populations
]

.pull-right[
```{r fpc_demo, echo=TRUE}
# FPC calculation
calculate_fpc <- function(n, N) {
  fpc <- sqrt((N - n) / (N - 1))
  reduction <- (1 - fpc) * 100
  
  cat("n =", n, ", N =", N, "\n")
  cat("Sampling fraction:", round(n/N, 3), "\n")
  cat("FPC factor:", round(fpc, 3), "\n")
  cat("SE reduction:", round(reduction, 1), "%\n\n")
  
  return(fpc)
}

# Small population
calculate_fpc(n = 50, N = 200)

# Large population  
calculate_fpc(n = 50, N = 10000)

# Census approaches
calculate_fpc(n = 180, N = 200)
```
]

**Bottom line: FPC matters for small populations**

---

# Knowledge Check 3

.pull-left[
### Quick Quiz:

**Q1: SE decreases with:**
a) Larger population
b) Larger sample ✓
c) More variables
d) Higher variance

**Q2: CLT requires n ≥ 30 for:**
a) Any calculation
b) Non-normal populations ✓
c) Normal populations
d) Small populations

**Q3: FPC is needed when:**
a) n > 30
b) N > 10000
c) n/N > 0.05 ✓
d) Always
]

.pull-right[
### Check Understanding:
```{r knowledge_check3, eval=FALSE}
# Test your knowledge
# What's the SE for:
# Population SD = 20
# Sample size = 100

se <- ___  # Fill this in

# What's the FPC for:
# n = 100, N = 1000

fpc <- ___  # Fill this in

# Answers:
# SE = 20/√100 = 2
# FPC = √(900/999) = 0.949
```
]

**Bottom line: Understanding builds confidence**

---

# Energy Boost!

.pull-left[
### Brain Break:
1. Close your eyes
2. Take 5 deep breaths
3. Rotate your neck
4. Smile
5. Ready to continue!

**30 seconds total**
]

.pull-right[
```{r brain_break, echo=FALSE}
energy <- data.frame(
  Activity = c("Breathe", "Stretch", "Smile", "Focus"),
  Benefit = c(80, 70, 90, 85),
  Icon = c("🫁", "🤸", "😊", "🎯")
)

ggplot(energy, aes(x = Activity, y = Benefit, fill = Activity)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = Icon), size = 10, vjust = -0.5) +
  scale_fill_manual(values = colors[2:5]) +
  ylim(0, 100) +
  labs(y = "Energy Boost (%)") +
  theme_minimal() +
  theme(legend.position = "none")
```
]

**Bottom line: Refreshed minds learn better**

---

# Confidence Intervals

.pull-left[
### What They Are:
Range likely containing true parameter

### 95% CI Formula:
**x̄ ± 1.96 × SE**

### Interpretation:
"We are 95% confident the true mean is between [lower, upper]"

### Common Levels:
- 90%: z = 1.645
- 95%: z = 1.96
- 99%: z = 2.576
]

.pull-right[
```{r ci_viz, echo=FALSE}
set.seed(111)
# Generate 20 confidence intervals
n_intervals <- 20
true_mean <- 50
sample_size <- 30
pop_sd <- 10

ci_data <- data.frame(
  sample = 1:n_intervals,
  mean = replicate(n_intervals, mean(rnorm(sample_size, true_mean, pop_sd))),
  se = pop_sd / sqrt(sample_size)
)

ci_data$lower <- ci_data$mean - 1.96 * ci_data$se
ci_data$upper <- ci_data$mean + 1.96 * ci_data$se
ci_data$contains_true <- ci_data$lower <= true_mean & ci_data$upper >= true_mean

ggplot(ci_data, aes(x = sample, y = mean)) +
  geom_errorbar(aes(ymin = lower, ymax = upper, 
                    color = contains_true), width = 0.3) +
  geom_point(aes(color = contains_true), size = 2) +
  geom_hline(yintercept = true_mean, linetype = "dashed") +
  scale_color_manual(values = c("FALSE" = colors[5], "TRUE" = colors[2])) +
  labs(x = "Sample Number", y = "Estimate",
       title = "20 Confidence Intervals (95% Level)") +
  theme_minimal() +
  theme(legend.position = "none") +
  coord_flip()
```
]

**Bottom line: 95% of CIs contain true value**

---

# CI Calculation Practice

.pull-left[
### Step by Step:
```{r ci_practice, echo=TRUE}
# Real CI calculation
sample_data <- c(23, 45, 67, 34, 56, 
                 78, 45, 39, 52, 61)

# Step 1: Calculate mean
x_bar <- mean(sample_data)
cat("Mean:", x_bar, "\n")

# Step 2: Calculate SE
s <- sd(sample_data)
n <- length(sample_data)
se <- s / sqrt(n)
cat("SE:", round(se, 2), "\n")

# Step 3: Find critical value
z_crit <- qnorm(0.975)  # For 95% CI
cat("z-critical:", round(z_crit, 2), "\n")
```
]

.pull-right[
```{r ci_complete, echo=TRUE}
# Step 4: Calculate margin of error
margin <- z_crit * se
cat("Margin:", round(margin, 2), "\n")

# Step 5: Construct CI
lower <- x_bar - margin
upper <- x_bar + margin

cat("\n95% CI: [", round(lower, 1), 
    ", ", round(upper, 1), "]\n", sep = "")

# Interpretation:
# We are 95% confident the true 
# population mean is between
# 37.9 and 58.5
```
]

**Bottom line: Five steps to confidence**

---

# Width of Confidence Intervals

.pull-left[
### What Affects Width:

**1. Confidence Level:**
- Higher confidence → Wider CI
- 99% wider than 95%

**2. Sample Size:**
- Larger n → Narrower CI
- Width ∝ 1/√n

**3. Variability:**
- Higher σ → Wider CI
- More variation = less precision
]

.pull-right[
```{r ci_width_demo, echo=TRUE}
# CI width comparison
calculate_ci_width <- function(n, conf_level, sd = 10) {
  z <- qnorm((1 + conf_level) / 2)
  se <- sd / sqrt(n)
  width <- 2 * z * se
  return(width)
}

# Compare scenarios
scenarios <- expand.grid(
  n = c(30, 100, 500),
  conf = c(0.90, 0.95, 0.99)
)

scenarios$width <- mapply(calculate_ci_width, 
                          scenarios$n, 
                          scenarios$conf)

print(round(scenarios, 2))
```
]

**Bottom line: Control width through design**

---

# Margin of Error

.pull-left[
### Definition:
Half-width of confidence interval

### Formula:
**MOE = z × SE**

### For Proportions:
**MOE = z × √(p(1-p)/n)**

### Typical Targets:
- Polls: ±3%
- Surveys: ±5%
- Research: ±2%
]

.pull-right[
```{r moe_calc, echo=TRUE}
# Calculate required sample size for MOE
required_n <- function(moe, p = 0.5, conf = 0.95) {
  z <- qnorm((1 + conf) / 2)
  n <- (z^2 * p * (1-p)) / moe^2
  return(ceiling(n))
}

# Different MOE requirements
moe_targets <- c(0.01, 0.02, 0.03, 0.05, 0.10)

sample_sizes <- sapply(moe_targets, required_n)

moe_df <- data.frame(
  MOE = paste0("±", moe_targets * 100, "%"),
  n_required = sample_sizes
)

print(moe_df)

# Note dramatic increase for small MOE!
```
]

**Bottom line: Precision has a price**

---

# Hypothesis Testing Basics

.pull-left[
### The Process:
1. **Null hypothesis (H₀):** Status quo
2. **Alternative (Hₐ):** What we test
3. **Calculate test statistic**
4. **Compare to critical value**
5. **Make decision**

### Test Statistic:
**z = (x̄ - μ₀) / SE**
]

.pull-right[
```{r hypothesis_viz, echo=FALSE}
# Visualize hypothesis test
x <- seq(-4, 4, 0.01)
y <- dnorm(x)

rejection_df <- data.frame(x = x, y = y)

ggplot(rejection_df, aes(x, y)) +
  geom_line(size = 1.5, color = colors[1]) +
  geom_area(data = subset(rejection_df, x < -1.96),
            fill = colors[5], alpha = 0.5) +
  geom_area(data = subset(rejection_df, x > 1.96),
            fill = colors[5], alpha = 0.5) +
  geom_vline(xintercept = c(-1.96, 1.96), 
             linetype = "dashed", color = colors[5]) +
  annotate("text", x = 0, y = 0.2, label = "Fail to Reject", size = 5) +
  annotate("text", x = -2.5, y = 0.05, label = "Reject", color = colors[5]) +
  annotate("text", x = 2.5, y = 0.05, label = "Reject", color = colors[5]) +
  labs(x = "Test Statistic", y = "Probability",
       title = "Rejection Regions (α = 0.05)") +
  theme_minimal()
```
]

**Bottom line: Evidence guides decisions**

---

# Type I and Type II Errors

.pull-left[
### Error Types:

**Type I (α):**
- Reject true H₀
- False positive
- Usually set at 0.05

**Type II (β):**
- Fail to reject false H₀
- False negative
- Depends on effect size

### Power:
**Power = 1 - β**
Probability of detecting true effect
]

.pull-right[
```{r error_matrix, echo=FALSE}
error_matrix <- data.frame(
  Reality = c("H₀ True", "H₀ True", "H₀ False", "H₀ False"),
  Decision = c("Don't Reject", "Reject", "Don't Reject", "Reject"),
  Outcome = c("Correct ✓", "Type I Error", "Type II Error", "Correct ✓"),
  Probability = c("1-α", "α", "β", "1-β (Power)")
)

kable(error_matrix, format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```
]

**Bottom line: Balance both error types**

---

# Power Analysis

.pull-left[
### What Affects Power:

1. **Effect size (δ):** Larger → More power
2. **Sample size (n):** Larger → More power
3. **Alpha (α):** Larger → More power
4. **Variability (σ):** Smaller → More power

### Target:
Power ≥ 0.80 (80%)
]

.pull-right[
```{r power_calc, echo=TRUE}
# Power calculation function
calculate_power <- function(n, effect_size, alpha = 0.05) {
  z_alpha <- qnorm(1 - alpha/2)
  z_beta <- effect_size * sqrt(n) - z_alpha
  power <- pnorm(z_beta)
  return(power)
}

# Power for different scenarios
n_values <- seq(20, 200, by = 20)
effect_sizes <- c(0.2, 0.5, 0.8)  # Small, medium, large

power_results <- expand.grid(
  n = n_values,
  effect = effect_sizes
)

power_results$power <- mapply(calculate_power,
                              power_results$n,
                              power_results$effect)

# Find n for 80% power
n_for_80 <- power_results %>%
  filter(power >= 0.80) %>%
  group_by(effect) %>%
  slice_min(n)

print(n_for_80)
```
]

**Bottom line: Plan sample size for adequate power**

---

# Variance Estimation

.pull-left[
### Population Variance:
**σ² = Σ(xᵢ - μ)² / N**

### Sample Variance:
**s² = Σ(xᵢ - x̄)² / (n-1)**

### Why n-1?
- Degrees of freedom
- Unbiased estimator
- Bessel's correction

### For Complex Designs:
Must account for design effects
]

.pull-right[
```{r variance_demo, echo=TRUE}
# Variance estimation comparison
set.seed(222)
population <- rnorm(10000, 50, 10)
true_var <- var(population)

# Take multiple samples
n_samples <- 1000
sample_size <- 30

# Biased estimator (using n)
biased_vars <- replicate(n_samples, {
  samp <- sample(population, sample_size)
  sum((samp - mean(samp))^2) / sample_size
})

# Unbiased estimator (using n-1)
unbiased_vars <- replicate(n_samples, {
  var(sample(population, sample_size))
})

cat("True variance:", round(true_var, 2), "\n")
cat("Mean biased:", round(mean(biased_vars), 2), "\n")
cat("Mean unbiased:", round(mean(unbiased_vars), 2), "\n")
```
]

**Bottom line: Use n-1 for unbiased estimates**

---

# Variance of Proportions

.pull-left[
### For Proportion p:

**Variance:**
V(p̂) = p(1-p)/n

**Maximum at p = 0.5:**
V(p̂)_max = 0.25/n

### Standard Error:
SE(p̂) = √(p(1-p)/n)

### Conservative Approach:
Use p = 0.5 when unknown
]

.pull-right[
```{r prop_variance, echo=TRUE}
# Variance of proportions
p_values <- seq(0, 1, by = 0.01)
n <- 100

# Calculate variance for each p
var_p <- p_values * (1 - p_values) / n

# Find maximum
max_var <- max(var_p)
max_p <- p_values[which.max(var_p)]

# Plot relationship
plot(p_values, var_p, type = "l", 
     col = colors[2], lwd = 2,
     xlab = "True Proportion (p)",
     ylab = "Variance of p̂",
     main = "Variance Peaks at p = 0.5")
abline(v = 0.5, lty = 2, col = colors[5])
points(0.5, max_var, col = colors[5], pch = 19, cex = 2)

cat("Maximum variance at p =", max_p, "\n")
cat("Maximum value:", round(max_var, 4))
```
]

**Bottom line: Most uncertain at 50/50 split**

---

# Group Exercise: Calculate Together

.pull-left[
### Scenario:
Survey of 150 households
Mean income: $45,000
Std deviation: $12,000

### Calculate:
1. Standard error
2. 95% confidence interval
3. Test if different from $50,000
4. Required n for MOE = $1,000

**Work in groups - 5 minutes**
]

.pull-right[
### Workspace:
```{r group_exercise, echo=TRUE, eval=FALSE}
# Your calculations
n <- 150
x_bar <- 45000
s <- 12000

# 1. Standard Error
se <- _____

# 2. 95% CI
lower <- _____
upper <- _____

# 3. Test statistic
z <- _____

# 4. Required n
n_new <- _____
```
]

**Bottom line: Practice solidifies learning**

---

# Solution to Group Exercise

.pull-left[
```{r solution_exercise, echo=TRUE}
# Given values
n <- 150
x_bar <- 45000
s <- 12000

# 1. Standard Error
se <- s / sqrt(n)
cat("SE: $", round(se, 2), "\n")

# 2. 95% CI
z_crit <- 1.96
margin <- z_crit * se
lower <- x_bar - margin
upper <- x_bar + margin
cat("95% CI: [$", round(lower), 
    ", $", round(upper), "]\n")
```
]

.pull-right[
```{r solution_continued, echo=TRUE}
# 3. Test H₀: μ = 50000
mu_0 <- 50000
z <- (x_bar - mu_0) / se
p_value <- 2 * pnorm(abs(z), lower.tail = FALSE)
cat("z-statistic:", round(z, 2), "\n")
cat("p-value:", round(p_value, 4), "\n")
cat("Decision: Reject H₀\n\n")

# 4. Required n for MOE = 1000
moe_target <- 1000
n_new <- (z_crit * s / moe_target)^2
cat("Required n:", ceiling(n_new))
```
]

**Bottom line: Systematic approach works**

---

# Progress Check 6 of 10

```{r progress6, echo=FALSE, fig.height=3}
progress <- data.frame(
  segment = 1:10,
  completed = c(rep(1, 6), rep(0, 4))
)

ggplot(progress, aes(segment, y = 1, fill = factor(completed))) +
  geom_tile(height = 0.5, color = "white", size = 2) +
  scale_fill_manual(values = c("0" = "lightgray", "1" = colors[2])) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Your Progress Today")
```

**Bottom line: 60% Complete - Keep going!**

---

# Real SADC Application

.pull-left[
### Tanzania Census 2022:
- Population: 61.7 million
- Post-enumeration survey
- Sample: 15,000 households
- Purpose: Assess coverage

### Results:
- Coverage: 96.8% (±0.4%)
- Undercount: 1.97 million
- Adjusted total: 63.7 million
- International credibility ✓
]

.pull-right[
```{r tanzania_example, echo=FALSE}
# Tanzania PES results
coverage_est <- data.frame(
  Region = c("Urban", "Rural", "National"),
  Coverage = c(94.2, 97.8, 96.8),
  SE = c(0.8, 0.5, 0.4)
)

ggplot(coverage_est, aes(Region, Coverage)) +
  geom_col(fill = colors[3], width = 0.6) +
  geom_errorbar(aes(ymin = Coverage - 1.96*SE,
                    ymax = Coverage + 1.96*SE),
                width = 0.2) +
  geom_hline(yintercept = 95, linetype = "dashed", 
             color = colors[5]) +
  ylim(90, 100) +
  labs(y = "Coverage Rate (%)",
       title = "Census Coverage Estimates") +
  theme_minimal()
```
]

**Bottom line: Sampling validates census**

---

# Common Mistakes in Theory

.pull-left[
### Frequent Errors:

1. **Confusing SE and SD**
   - SD: spread of data
   - SE: precision of estimate

2. **Wrong CI interpretation**
   - NOT: "95% of data in CI"
   - YES: "95% confident μ in CI"

3. **Ignoring assumptions**
   - Independence required
   - Random sampling crucial

4. **P-value misuse**
   - Not probability H₀ true
   - Evidence strength only
]

.pull-right[
```{r mistakes_viz, echo=FALSE}
mistakes <- data.frame(
  Error = c("SE/SD", "CI meaning", "Assumptions", "P-values"),
  Frequency = c(70, 85, 60, 90),
  Impact = c(60, 40, 80, 50)
)

ggplot(mistakes, aes(Frequency, Impact)) +
  geom_point(size = 10, color = colors[5], alpha = 0.7) +
  geom_text(aes(label = Error), size = 3) +
  labs(x = "How Often (%)", y = "Severity (%)",
       title = "Impact of Theoretical Errors") +
  theme_minimal()
```
]

**Bottom line: Precision in concepts matters**

---

# Interactive Quiz

.pull-left[
### Test Your Understanding:

**Q1:** If SE = 5 and n = 100, what's σ?
a) 5
b) 50 ✓
c) 500
d) 0.5

**Q2:** 95% CI is [40, 60]. What's x̄?
a) 20
b) 50 ✓
c) 100
d) 10

**Q3:** To halve the SE, multiply n by:
a) 2
b) 4 ✓
c) 0.5
d) √2
]

.pull-right[
### Think About:

Why is each answer correct?

Share your reasoning with neighbor

Any questions before we continue?
]

**Bottom line: Active thinking enhances learning**

---

# Connecting Theory to Practice

.pull-left[
### Theory Tells Us:
- How precise estimates are
- Sample size needed
- Confidence in results
- When differences are real

### Practice Shows Us:
- Real-world complications
- Cost-benefit trade-offs
- Implementation challenges
- Communication needs
]

.pull-right[
```{r theory_practice, echo=FALSE}
connection <- data.frame(
  Theory = c("CLT", "SE Formula", "CI", "Power"),
  Practice = c("Any design works", "Sample size", 
               "Communicate uncertainty", "Detect effects"),
  Strength = c(90, 85, 75, 70)
)

ggplot(connection, aes(x = reorder(Theory, Strength), 
                       y = Strength, fill = Theory)) +
  geom_col(width = 0.6) +
  scale_fill_manual(values = colors[2:5]) +
  coord_flip() +
  labs(x = "", y = "Connection Strength (%)",
       title = "Theory-Practice Links") +
  theme_minimal() +
  theme(legend.position = "none")
```
]

**Bottom line: Theory empowers practice**

---

# Key Formulas Summary

.pull-left[
### Essential Formulas:

**Mean:** x̄ = Σx/n

**Variance:** s² = Σ(x-x̄)²/(n-1)

**Standard Error:** SE = s/√n

**95% CI:** x̄ ± 1.96×SE

**Sample Size:** n = (z×σ/MOE)²

**FPC:** √((N-n)/(N-1))
]

.pull-right[
### Quick Reference Card:
```{r formula_card, echo=FALSE}
formulas <- data.frame(
  Purpose = c("Estimate", "Spread", "Precision", 
              "Interval", "Planning", "Correction"),
  Formula = c("x̄", "s²", "SE", "CI", "n", "FPC"),
  When = c("Always", "Always", "Always", 
           "Inference", "Design", "n/N > 0.05")
)

kable(formulas, format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE)
```
]

**Bottom line: Keep these handy**

---

# Reflection and Discussion

.pull-left[
### Share at Your Table:

1. **Biggest insight** from theory section

2. **One formula** you'll use most

3. **Remaining confusion** to clarify

### Then:
One person shares table's top insight

**Time: 3 minutes**
]

.pull-right[
```{r reflection_time, echo=FALSE}
reflection <- data.frame(
  Minute = 1:3,
  Activity = c("Individual", "Table", "Room"),
  Focus = c("Think", "Share", "Learn")
)

ggplot(reflection, aes(Minute, y = 1, fill = Activity)) +
  geom_tile(height = 0.8) +
  geom_text(aes(label = Focus), color = "white", size = 6) +
  scale_fill_manual(values = colors[2:4]) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Reflection Process")
```
]

**Bottom line: Sharing deepens understanding**

---

# Preparing for Part 3

.pull-left[
### You've Mastered:
✅ Sampling distributions
✅ Central Limit Theorem
✅ Standard errors
✅ Confidence intervals
✅ Hypothesis testing
✅ Power analysis

### Next Up:
Mathematics Made Simple
- Step-by-step calculations
- Visual demonstrations
- Practice problems
]

.pull-right[
### Success Tips:
1. Review formulas
2. Prepare calculator
3. Have R ready
4. Bring questions
5. Stay engaged

```{r prep_viz, echo=FALSE}
prep <- data.frame(
  Step = 1:5,
  Task = c("Review", "Calculate", "Code", "Question", "Engage"),
  Ready = c(100, 90, 80, 70, 95)
)

ggplot(prep, aes(Step, Ready)) +
  geom_line(color = colors[2], size = 2) +
  geom_point(color = colors[2], size = 4) +
  ylim(0, 100) +
  labs(x = "Preparation Step", y = "Readiness (%)") +
  theme_minimal()
```
]

**Bottom line: You're ready for mathematics**

---

# Summary: Theory Foundations

.pull-left[
### Core Concepts:
1. **Sampling distributions** are predictable
2. **CLT** enables inference
3. **SE** measures precision
4. **CIs** quantify uncertainty
5. **Power** ensures detection

### Remember:
Theory guides practice
Math proves it works
You can master both!
]

.pull-right[
```{r summary_viz, echo=FALSE}
mastery <- data.frame(
  Concept = c("Distributions", "CLT", "SE", "CI", "Power"),
  Before = c(20, 15, 25, 30, 10),
  After = c(85, 90, 88, 92, 80)
)

mastery_long <- mastery %>%
  pivot_longer(cols = c(Before, After),
               names_to = "Time",
               values_to = "Understanding")

ggplot(mastery_long, aes(Concept, Understanding, fill = Time)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[c(5, 2)]) +
  labs(y = "Understanding (%)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
]

**Bottom line: Remarkable progress achieved!**

---

# Progress Check 7 of 10

```{r progress7, echo=FALSE, fig.height=3}
progress <- data.frame(
  segment = 1:10,
  completed = c(rep(1, 7), rep(0, 3))
)

ggplot(progress, aes(segment, y = 1, fill = factor(completed))) +
  geom_tile(height = 0.5, color = "white", size = 2) +
  scale_fill_manual(values = c("0" = "lightgray", "1" = colors[2])) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Your Progress Today")
```

**Bottom line: 70% Complete - Excellence ahead!**

---

class: center, middle

# End of Part 2

## Theory Foundation Complete!

### Take a 10-minute break

### Next: Part 3 - Mathematics Made Simple

**Remember: You now understand WHY sampling works**
**Next: You'll master HOW to calculate everything**

---
class: center, middle

# Part 3: Mathematics Made Simple

## Transform Complex Formulas into Clear Understanding

### "Mathematics is not about numbers, it's about understanding"

---

# Welcome to Mathematics Section

.pull-left[
### Our Approach:
- Start simple, build up
- Visual before formula
- Practice immediately
- Connect to real work
- No fear, just clarity

### You Will Master:
- Essential calculations
- Variance formulas
- Sample size determination
- Design effects
- Practical shortcuts
]

.pull-right[
```{r math_welcome, echo=FALSE}
library(tidyverse)
colors <- c("#003F7F", "#0066CC", "#4A90E2", "#FFA500", "#FF6B35")

topics <- data.frame(
  Topic = c("Basic", "Variance", "Sample Size", "Complex", "Advanced"),
  Difficulty = c(1, 2, 3, 4, 5),
  Time = c(10, 15, 20, 15, 10)
)

ggplot(topics, aes(x = Difficulty, y = Time, fill = Topic)) +
  geom_area(alpha = 0.7) +
  scale_fill_manual(values = colors) +
  labs(x = "Complexity Level", y = "Minutes",
       title = "Your Learning Journey") +
  theme_minimal()
```
]

**Bottom line: Step by step to mastery**

---

# Breaking Down the Mean

.pull-left[
### The Formula:
**x̄ = Σxᵢ / n**

### In Words:
"Add all values, divide by count"

### Step by Step:
1. List all values
2. Add them up (Σ)
3. Count them (n)
4. Divide sum by count
5. That's your mean!
]

.pull-right[
```{r mean_breakdown, echo=TRUE}
# Let's calculate step by step
values <- c(23, 45, 67, 34, 56)

# Step 1: See the values
print(values)

# Step 2: Add them up
total <- sum(values)
cat("Sum:", total, "\n")

# Step 3: Count them
n <- length(values)
cat("Count:", n, "\n")

# Step 4: Divide
mean_value <- total / n
cat("Mean:", mean_value, "\n")

# Verify with function
cat("Check:", mean(values))
```
]

**Bottom line: It's just an average!**

---

# Weighted Mean Calculation

.pull-left[
### When Weights Matter:

**Formula:**
x̄ᵤ = Σ(wᵢ × xᵢ) / Σwᵢ

### Real Example:
Different selection probabilities require weights

### Process:
1. Multiply each value by weight
2. Sum weighted values
3. Sum weights
4. Divide
]

.pull-right[
```{r weighted_mean, echo=TRUE}
# Weighted mean example
strata_data <- data.frame(
  stratum = c("Urban", "Rural", "Remote"),
  mean_income = c(60000, 35000, 25000),
  weight = c(2000, 3000, 500)
)

print(strata_data)

# Calculate weighted mean
weighted_sum <- sum(strata_data$mean_income * 
                   strata_data$weight)
total_weight <- sum(strata_data$weight)

weighted_mean <- weighted_sum / total_weight
cat("\nWeighted mean: $", 
    format(weighted_mean, big.mark=","))

# Compare to simple mean
simple_mean <- mean(strata_data$mean_income)
cat("\nSimple mean: $", 
    format(simple_mean, big.mark=","))
```
]

**Bottom line: Weights restore representation**

---

# Understanding Variance Step by Step

.pull-left[
### Variance Formula:
**s² = Σ(xᵢ - x̄)² / (n-1)**

### Breaking It Down:
1. Find mean (x̄)
2. Subtract mean from each value
3. Square the differences
4. Add squared differences
5. Divide by (n-1)

### Why Square?
- Makes negatives positive
- Penalizes large deviations
- Mathematical properties
]

.pull-right[
```{r variance_steps, echo=TRUE}
# Calculate variance manually
data <- c(2, 4, 6, 8, 10)

# Step 1: Mean
x_bar <- mean(data)
cat("Mean:", x_bar, "\n\n")

# Step 2: Deviations
deviations <- data - x_bar
cat("Deviations:", deviations, "\n")

# Step 3: Square them
squared <- deviations^2
cat("Squared:", squared, "\n")

# Step 4: Sum
sum_sq <- sum(squared)
cat("Sum of squares:", sum_sq, "\n")

# Step 5: Divide by (n-1)
variance <- sum_sq / (length(data) - 1)
cat("Variance:", variance, "\n")

# Verify
cat("Check:", var(data))
```
]

**Bottom line: Variance measures spread**

---

# Standard Deviation Intuition

.pull-left[
### From Variance to SD:
**s = √s²**

### Why Take Square Root?
- Returns to original units
- Easier interpretation
- "Typical" deviation

### Rule of Thumb:
- ~68% within ±1 SD
- ~95% within ±2 SD
- ~99.7% within ±3 SD
]

.pull-right[
```{r sd_visual, echo=FALSE}
set.seed(123)
data <- rnorm(1000, mean = 50, sd = 10)
mean_val <- mean(data)
sd_val <- sd(data)

hist(data, breaks = 30, probability = TRUE,
     main = "Standard Deviation Zones",
     xlab = "Value", col = "lightblue", border = "white")

# Add normal curve
curve(dnorm(x, mean = mean_val, sd = sd_val), 
      add = TRUE, col = colors[1], lwd = 2)

# Add SD lines
abline(v = mean_val, col = colors[2], lwd = 2, lty = 1)
abline(v = mean_val + c(-1, 1) * sd_val, col = colors[3], lwd = 2, lty = 2)
abline(v = mean_val + c(-2, 2) * sd_val, col = colors[4], lwd = 2, lty = 2)
abline(v = mean_val + c(-3, 3) * sd_val, col = colors[5], lwd = 2, lty = 2)

legend("topright", 
       legend = c("Mean", "±1 SD", "±2 SD", "±3 SD"),
       col = colors[2:5], lty = c(1, 2, 2, 2), lwd = 2)
```
]

**Bottom line: SD is the "average" deviation**

---

# Coefficient of Variation

.pull-left[
### Formula:
**CV = (s / x̄) × 100%**

### Purpose:
- Compare variability
- Across different scales
- Relative measure

### Interpretation:
- CV < 10%: Low variation
- CV 10-30%: Moderate
- CV > 30%: High variation
]

.pull-right[
```{r cv_calculation, echo=TRUE}
# Compare variation across measures
measures <- data.frame(
  Variable = c("Age", "Income", "Height"),
  Mean = c(35, 50000, 170),
  SD = c(12, 15000, 10),
  Unit = c("years", "$", "cm")
)

# Calculate CV
measures$CV <- round(
  (measures$SD / measures$Mean) * 100, 1
)

print(measures)

# Which has highest relative variation?
cat("\nMost variable:", 
    measures$Variable[which.max(measures$CV)])
cat("\nLeast variable:", 
    measures$Variable[which.min(measures$CV)])
```
]

**Bottom line: CV enables comparison**

---

# Energy Break!

.pull-left[
### Math Refresh:
1. Stand and count to 20
2. Do 5 shoulder rolls
3. Look away from screen
4. Take deep breaths
5. Ready for more math!

**30 seconds - GO!**
]

.pull-right[
```{r energy_math, echo=FALSE}
activities <- data.frame(
  Activity = c("Count", "Roll", "Rest", "Breathe", "Focus"),
  Seconds = c(5, 5, 10, 5, 5),
  Benefit = c(60, 70, 90, 85, 95)
)

ggplot(activities, aes(x = Activity, y = Benefit, fill = Activity)) +
  geom_col(width = 0.6) +
  scale_fill_manual(values = colors) +
  ylim(0, 100) +
  labs(y = "Refresh Level (%)") +
  theme_minimal() +
  theme(legend.position = "none")
```
]

**Bottom line: Movement helps math**

---

# Sample Size Formula Decoded

.pull-left[
### For Means:
**n = (z² × σ²) / MOE²**

### Components:
- **z**: Confidence level
  - 1.96 for 95%
  - 2.58 for 99%
- **σ**: Population SD
  - Estimate from pilot
  - Previous studies
- **MOE**: Margin of error
  - Your tolerance
]

.pull-right[
```{r sample_size_mean, echo=TRUE}
# Sample size calculator for means
calc_n_mean <- function(sigma, moe, conf = 0.95) {
  z <- qnorm((1 + conf) / 2)
  n <- (z^2 * sigma^2) / moe^2
  return(ceiling(n))
}

# Example: Income survey
# SD = $15,000, MOE = $1,000
n_needed <- calc_n_mean(
  sigma = 15000,
  moe = 1000,
  conf = 0.95
)

cat("Sample size needed:", n_needed, "\n")

# What if we want MOE = $500?
n_precise <- calc_n_mean(15000, 500, 0.95)
cat("For MOE=$500:", n_precise, "\n")
cat("Cost increase:", n_precise/n_needed, "times")
```
]

**Bottom line: Precision costs quadratically**

---

# Sample Size for Proportions

.pull-left[
### Formula:
**n = (z² × p(1-p)) / MOE²**

### Maximum at p = 0.5:
When uncertain, use p = 0.5
(Conservative approach)

### Quick Formula:
For 95% CI, ±3% MOE:
**n ≈ 1,067**

For 95% CI, ±5% MOE:
**n ≈ 384**
]

.pull-right[
```{r sample_size_prop, echo=TRUE}
# Sample size for proportions
calc_n_prop <- function(p, moe, conf = 0.95) {
  z <- qnorm((1 + conf) / 2)
  n <- (z^2 * p * (1-p)) / moe^2
  return(ceiling(n))
}

# Different scenarios
scenarios <- expand.grid(
  p = c(0.1, 0.3, 0.5),
  moe = c(0.02, 0.03, 0.05)
)

scenarios$n <- mapply(calc_n_prop, 
                      scenarios$p, 
                      scenarios$moe)

# Display as matrix
sample_matrix <- matrix(scenarios$n, 
                        nrow = 3, ncol = 3,
                        dimnames = list(
                          paste("p =", c(0.1, 0.3, 0.5)),
                          paste("MOE =", c(2, 3, 5), "%")
                        ))
print(sample_matrix)
```
]

**Bottom line: P = 0.5 gives largest n**

---

# Quick Sample Size Table

.pull-left[
### For 95% Confidence:

| MOE | n (p=0.5) | n (p=0.2) |
|-----|-----------|-----------|
| ±1% | 9,604     | 6,147     |
| ±2% | 2,401     | 1,537     |
| ±3% | 1,067     | 683       |
| ±4% | 600       | 384       |
| ±5%

---
class: center, middle

# Part 4: Real Survey Applications with R

## From Theory to Practice: Your Daily Work

### "Now we make it real for your office"

---

# Survey Package: Your New Best Friend

.pull-left[
### What It Does:
- Handles complex designs
- Calculates proper SEs
- Adjusts for weights
- Accounts for stratification
- Manages clustering
- Does it all correctly!

### Installation:
```{r install_survey, eval=FALSE}
install.packages("survey")
install.packages("srvyr")  # tidyverse-friendly
install.packages("sampling")
```
]

.pull-right[
```{r survey_intro, echo=TRUE}
library(survey)

# Create example survey data
data <- data.frame(
  id = 1:100,
  stratum = rep(1:5, each = 20),
  weight = runif(100, 0.8, 1.2),
  income = rnorm(100, 50000, 15000),
  employed = rbinom(100, 1, 0.7)
)

# Define survey design
design <- svydesign(
  ids = ~1,  # no clustering
  strata = ~stratum,
  weights = ~weight,
  data = data
)

# Now it knows your design!
print(design)
```
]

**Bottom line: Survey package handles complexity**

---

# Real Household Survey Setup

.pull-left[
### Typical SADC Survey:
- Multi-stage design
- Urban/rural strata
- EA clustering
- Household weights
- Person weights

### Let's Build It:
Step by step from your actual work
]

.pull-right[
```{r real_survey_setup, echo=TRUE, cache=TRUE}
# Simulate realistic survey data
set.seed(123)
hh_survey <- data.frame(
  hh_id = 1:500,
  ea_id = rep(1:50, each = 10),
  stratum = rep(c("Urban", "Rural"), each = 250),
  hh_size = rpois(500, 4),
  hh_weight = runif(500, 50, 200),
  income = rlnorm(500, 10, 1),
  has_electricity = rbinom(500, 1, 0.65)
)

# Define the complex design
hh_design <- svydesign(
  ids = ~ea_id,  # Clustered by EA
  strata = ~stratum,  # Stratified
  weights = ~hh_weight,  # Weighted
  data = hh_survey,
  nest = TRUE  # EAs nested in strata
)

# Check design
summary(hh_design)
```
]

**Bottom line: Matches your real surveys**

---

# Calculating National Estimates

.pull-left[
### Key Estimates:
Every survey needs:
- Totals
- Means
- Proportions
- Percentiles
- Ratios

All with proper SEs!
]

.pull-right[
```{r national_estimates, echo=TRUE}
# Calculate key national indicators

# 1. Average household income
inc_mean <- svymean(~income, hh_design)
print(inc_mean)
confint(inc_mean)

# 2. Electricity access rate
elec_prop <- svymean(~has_electricity, hh_design)
print(elec_prop)
confint(elec_prop, level = 0.95)

# 3. Total households with electricity
elec_total <- svytotal(~has_electricity, hh_design)
print(elec_total)

# 4. Income percentiles
inc_quant <- svyquantile(~income, hh_design, 
                         c(0.25, 0.5, 0.75))
print(inc_quant)
```
]

**Bottom line: All estimates with SEs**

---

# Domain Estimation (Subgroups)

.pull-left[
### Common Domains:
- By province
- By urban/rural
- By age group
- By gender
- By education

### Challenge:
Smaller sample = larger SE
But survey package handles it!
]

.pull-right[
```{r domain_estimation, echo=TRUE}
# Estimates by domain

# By stratum (urban/rural)
inc_by_stratum <- svyby(~income, ~stratum, 
                        hh_design, svymean)
print(inc_by_stratum)

# Electricity by stratum  
elec_by_stratum <- svyby(~has_electricity, 
                         ~stratum, 
                         hh_design, svymean)
print(elec_by_stratum)

# Plot with confidence intervals
plot(elec_by_stratum, ylab = "Stratum",
     xlab = "Electricity Access Rate",
     main = "Access by Location")

# Test urban-rural difference
svyttest(income ~ stratum, hh_design)
```
]

**Bottom line: Disaggregated analysis made easy**

---

# Handling Missing Data

.pull-left[
### Reality Check:
- Item nonresponse
- Unit nonresponse  
- Refusals
- Not at home

### Approaches:
1. Complete case (careful!)
2. Imputation
3. Weight adjustment
4. Multiple imputation
]

.pull-right[
```{r missing_data, echo=TRUE}
# Add realistic missing data
hh_survey_miss <- hh_survey
miss_pattern <- rbinom(500, 1, 0.1)  # 10% missing
hh_survey_miss$income[miss_pattern == 1] <- NA

# Check missingness
cat("Missing income:", sum(is.na(hh_survey_miss$income)), "\n")

# Option 1: Complete case (default)
design_cc <- svydesign(
  ids = ~ea_id,
  strata = ~stratum,
  weights = ~hh_weight,
  data = hh_survey_miss
)

# Estimate with missing
inc_miss <- svymean(~income, design_cc, na.rm = TRUE)
print(inc_miss)

# Option 2: Simple imputation
hh_survey_miss$income_imp <- ifelse(
  is.na(hh_survey_miss$income),
  median(hh_survey_miss$income, na.rm = TRUE),
  hh_survey_miss$income
)

# Compare CV
cv_original <- cv(inc_mean)
cv_missing <- cv(inc_miss)
cat("CV change:", round((cv_missing - cv_original)*100, 2), "%")
```
]

**Bottom line: Missing data needs attention**

---

# Calibration to Known Totals

.pull-left[
### Why Calibrate?
- Match census totals
- Reduce bias
- Improve precision
- Ensure consistency

### Common Margins:
- Age-sex distribution
- Geographic totals
- Household sizes
]

.pull-right[
```{r calibration_demo, echo=TRUE}
# Current weighted totals
current_totals <- svytable(~stratum, hh_design)
print(current_totals)

# Population totals (from census)
pop_totals <- data.frame(
  stratum = c("Urban", "Rural"),
  Freq = c(600000, 400000)
)

# Use postStratify for calibration
cal_design <- postStratify(
  design = hh_design,
  strata = ~stratum,
  population = pop_totals
)

# Check new totals
new_totals <- svytable(~stratum, cal_design)
print(new_totals)

# Compare estimates
old_est <- svymean(~income, hh_design)
new_est <- svymean(~income, cal_design)
cat("Before calibration:", round(coef(old_est), 2), "\n")
cat("After calibration:", round(coef(new_est), 2), "\n")
```
]

**Bottom line: Match known population**

---

# Variance Estimation Methods

.pull-left[
### Three Approaches:

**1. Taylor Series (default)**
- Fast
- Approximate
- Works for most statistics

**2. Replicate Weights**
- Bootstrap
- Jackknife
- BRR

**3. Direct Calculation**
- Ultimate clusters
- Explicit formulas
]

.pull-right[
```{r variance_methods, echo=TRUE}
# Method 1: Taylor series (default)
inc_taylor <- svymean(~income, hh_design)
SE(inc_taylor)

# Method 2: Bootstrap
boot_design <- as.svrepdesign(
  hh_design,
  type = "bootstrap",
  replicates = 100
)

inc_boot <- svymean(~income, boot_design)
SE(inc_boot)

# Method 3: Jackknife
jk_design <- as.svrepdesign(
  hh_design,
  type = "JKn"
)

inc_jk <- svymean(~income, jk_design)
SE(inc_jk)

# Compare methods
cat("Taylor SE:", SE(inc_taylor), "\n")
cat("Bootstrap SE:", SE(inc_boot), "\n")
cat("Jackknife SE:", SE(inc_jk), "\n")
```
]

**Bottom line: Multiple ways to get SEs**

---

# Creating Survey Tables

.pull-left[
### Publication-Ready Tables:
- Estimates
- Standard errors
- Confidence intervals
- Sample sizes
- Design effects

### Professional Output:
Ready for reports!
]

.pull-right[
```{r survey_tables, echo=TRUE}
# Create professional table
results <- svyby(
  formula = ~income + has_electricity,
  by = ~stratum,
  design = hh_design,
  FUN = svymean,
  keep.var = TRUE
)

# Format nicely
results_df <- as.data.frame(results)
results_df$income_cv <- results_df$se.income / 
                        results_df$income * 100
results_df$elec_pct <- results_df$has_electricity * 100

# Display formatted
final_table <- results_df[, c("stratum", "income", 
                              "se.income", "income_cv",
                              "elec_pct")]
names(final_table) <- c("Area", "Mean Income", 
                        "SE", "CV(%)", "Electricity(%)")

# Round numeric columns
numeric_cols <- sapply(final_table, is.numeric)
final_table[numeric_cols] <- round(final_table[numeric_cols], 1)

library(knitr)
kable(final_table, caption = "Survey Results by Area")
```
]

**Bottom line: Report-ready output**

---

# Hypothesis Testing in Surveys

.pull-left[
### Common Tests:
- Difference in means
- Difference in proportions
- Association tests
- Regression models

### Key Point:
Must account for design!
Regular tests are wrong
]

.pull-right[
```{r hypothesis_testing, echo=TRUE}
# Test 1: Urban vs Rural income
income_test <- svyttest(income ~ stratum, hh_design)
print(income_test)

# Test 2: Electricity association
elec_test <- svychisq(~has_electricity + stratum, 
                      hh_design)
print(elec_test)

# Test 3: Regression model
model <- svyglm(income ~ stratum + hh_size, 
                design = hh_design)
summary(model)

# Design effect for mean
deff(svymean(~income, hh_design))

# Effective sample size
neff <- nrow(hh_survey) / deff(svymean(~income, hh_design))
cat("Effective n:", round(neff))
```
]

**Bottom line: Design-adjusted testing**

---

# Poverty and Inequality Measures

.pull-left[
### Key Indicators:
- Headcount ratio
- Poverty gap
- Gini coefficient
- Percentile ratios
- Lorenz curves

### Critical for SADC:
SDG monitoring
Policy evaluation
]

.pull-right[
```{r poverty_measures, echo=TRUE}
# Set poverty line
poverty_line <- 20000

# Create poverty indicator
hh_survey$poor <- hh_survey$income < poverty_line

# Update design with new variable
pov_design <- update(hh_design, 
                     poor = income < poverty_line)

# Headcount ratio
headcount <- svymean(~poor, pov_design)
print(headcount)
confint(headcount)

# By stratum
pov_by_area <- svyby(~poor, ~stratum, 
                     pov_design, svymean)
print(pov_by_area)

# Poverty gap (simplified)
gap_design <- update(pov_design, 
                     gap = ifelse(income < poverty_line,
                                  (poverty_line - income) / poverty_line, 
                                  0))
pov_gap <- svymean(~gap, gap_design)
cat("Poverty gap:", round(coef(pov_gap) * 100, 1), "%")
```
]

**Bottom line: SDG indicators calculated**

---

# Small Area Estimation

.pull-left[
### The Problem:
- Need district estimates
- Sample too small
- Large SEs
- Unreliable results

### The Solution:
- Borrow strength
- Use models
- Auxiliary data
- Synthetic estimates
]

.pull-right[
```{r small_area, echo=TRUE}
# Simulate district data
districts <- data.frame(
  district = rep(1:20, each = 25),
  n_sample = rep(c(50, 30, 20, 15, 10), each = 100),
  auxiliary = rnorm(500, 50, 10),  # Census data
  outcome = rnorm(500, 30, 8)
)

# Direct estimates (unreliable for small areas)
direct_est <- aggregate(outcome ~ district, 
                        districts, mean)
direct_est$se <- sqrt(64 / table(districts$district))

# Model-based (borrowing strength)
library(lme4)
model <- lmer(outcome ~ auxiliary + (1|district), 
              data = districts)

# Predictions for all districts
district_aux <- aggregate(auxiliary ~ district, 
                         districts, mean)
district_aux$predicted <- predict(model, 
                                 newdata = district_aux)

# Compare precision
cat("Direct SE range:", 
    range(direct_est$se), "\n")
cat("Model SE (typical):", 
    round(sd(ranef(model)$district[,1]), 2))
```
]

**Bottom line: Models help small domains**

---

# Multi-Topic Surveys

.pull-left[
### Common Types:
- DHS (health)
- LSMS (living standards)
- LFS (labor force)
- MICS (children)
- Agricultural surveys

### Challenge:
Different precision needs per topic
]

.pull-right[
```{r multi_topic, echo=TRUE}
# Multi-topic survey simulation
multi_survey <- data.frame(
  hh_id = 1:1000,
  stratum = rep(1:5, each = 200),
  weight = runif(1000, 50, 150),
  # Different topics
  unemployed = rbinom(1000, 1, 0.08),  # Rare
  vaccinated = rbinom(1000, 1, 0.70),  # Common
  income = rlnorm(1000, 10, 1),  # Continuous
  stunted = rbinom(1000, 1, 0.25)  # Health
)

# Design
multi_design <- svydesign(~1, strata = ~stratum,
                          weights = ~weight,
                          data = multi_survey)

# Calculate CVs for each indicator
indicators <- c("unemployed", "vaccinated", 
               "income", "stunted")

cvs <- sapply(indicators, function(var) {
  est <- svymean(as.formula(paste("~", var)), 
                 multi_design)
  cv(est) * 100
})

# Display precision achieved
precision_df <- data.frame(
  Indicator = indicators,
  CV_percent = round(cvs, 1)
)
print(precision_df)

# Which needs larger sample?
cat("Highest CV:", 
    indicators[which.max(cvs)])
```
]

**Bottom line: Balance multiple objectives**

---

# Seasonal and Panel Surveys

.pull-left[
### Quarterly Surveys:
- Labor force
- Business statistics
- Price indices

### Panel Features:
- Track same units
- Measure change
- Rotation patterns
- Attrition issues
]

.pull-right[
```{r panel_survey, echo=TRUE}
# Simulate quarterly panel
quarters <- expand.grid(
  hh_id = 1:200,
  quarter = 1:4
)

quarters$employed <- rbinom(800, 1, 0.65)
quarters$employed[quarters$quarter > 1] <- 
  quarters$employed[quarters$quarter > 1] + 
  rbinom(600, 1, 0.1) - rbinom(600, 1, 0.1)

quarters$weight <- runif(800, 80, 120)

# Design for each quarter
q1_design <- svydesign(~1, weights = ~weight,
                       data = quarters[quarters$quarter == 1,])
q2_design <- svydesign(~1, weights = ~weight,
                       data = quarters[quarters$quarter == 2,])

# Employment rates
emp_q1 <- svymean(~employed, q1_design)
emp_q2 <- svymean(~employed, q2_design)

cat("Q1 Employment:", round(coef(emp_q1)*100, 1), "%\n")
cat("Q2 Employment:", round(coef(emp_q2)*100, 1), "%\n")

# Change estimate
change <- coef(emp_q2) - coef(emp_q1)
cat("Change:", round(change*100, 1), "percentage points")
```
]

**Bottom line: Track changes over time**

---

# Energy Break!

.pull-left[
### Application Refresh:
1. Stand up
2. Think of YOUR survey
3. Which technique will help most?
4. Share with neighbor
5. Ready to continue!

**1 minute reflection**
]

.pull-right[
```{r energy_app, echo=FALSE}
library(tidyverse)
colors <- c("#003F7F", "#0066CC", "#4A90E2", "#FFA500", "#FF6B35")

techniques <- data.frame(
  Technique = c("Calibration", "Small Area", 
                "Domains", "Panel"),
  Usefulness = c(90, 70, 95, 60),
  Your_Work = c(85, 60, 90, 40)
)

tech_long <- pivot_longer(techniques, 
                          cols = c(Usefulness, Your_Work),
                          names_to = "Type",
                          values_to = "Relevance")

ggplot(tech_long, aes(Technique, Relevance, fill = Type)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[c(2, 4)]) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(y = "Relevance (%)")
```
]

**Bottom line: Connect to your work**

---

# Quality Indicators

.pull-left[
### Key Metrics:
- Response rates
- Coverage rates
- Design effects
- CVs by domain
- Item nonresponse

### Targets:
- Response > 80%
- CV < 10% (key indicators)
- DEFF < 2.5
- Coverage > 95%
]

.pull-right[
```{r quality_indicators, echo=TRUE}
# Calculate quality indicators
quality_report <- function(design, variable) {
  # Calculate estimate
  est <- svymean(as.formula(paste("~", variable)), 
                 design, na.rm = TRUE)
  
  # Get basic statistics
  est_val <- as.numeric(coef(est))
  se_val <- as.numeric(SE(est))
  
  # Calculate CV manually
  cv_val <- if(est_val != 0) (se_val / abs(est_val)) * 100 else NA
  
  # Simple DEFF approximation or set to 1
  deff_val <- 1  # Or calculate if you have the formula
  
  # Calculate 95% CI
  ci_lower <- est_val - 1.96 * se_val
  ci_upper <- est_val + 1.96 * se_val
  
  # Create result data frame
  result <- data.frame(
    Estimate = est_val,
    SE = se_val,
    CV = cv_val,
    DEFF = deff_val,
    CI_lower = ci_lower,
    CI_upper = ci_upper
  )
  
  return(result)
}

# Generate quality report
quality <- quality_report(hh_design, "income")
print(round(quality, 2))

# Response rate calculation
n_selected <- 600
n_responded <- 500
response_rate <- n_responded / n_selected * 100
cat("\nResponse rate:", response_rate, "%")
```
]

**Bottom line: Monitor survey quality**

---

# Visualization for Reports

.pull-left[
### Essential Graphs:
- Estimates with CIs
- By domain
- Trends
- Maps
- Quality metrics

### Professional presentation
]

.pull-right[
```{r visualization, echo=TRUE, fig.height=5}
# Professional visualization
library(ggplot2)

# Prepare data
viz_data <- as.data.frame(
  svyby(~income, ~stratum, hh_design, svymean)
)
viz_data$lower <- viz_data$income - 1.96*viz_data$se
viz_data$upper <- viz_data$income + 1.96*viz_data$se

# Create plot
ggplot(viz_data, aes(x = stratum, y = income)) +
  geom_col(fill = colors[2], width = 0.6) +
  geom_errorbar(aes(ymin = lower, ymax = upper),
                width = 0.2, size = 1) +
  labs(title = "Average Income by Area",
       subtitle = "With 95% Confidence Intervals",
       x = "Area", y = "Income ($)") +
  theme_minimal() +
  theme(text = element_text(size = 12))
```
]

**Bottom line: Clear communication**

---

# Documentation Standards

.pull-left[
### Document Everything:
- Sample design
- Weight calculation
- Nonresponse adjustment
- Calibration process
- Quality indicators
- Limitations

### For Reproducibility:
Others can verify
Future surveys can replicate
]

.pull-right[
```{r documentation, echo=TRUE}
# Create metadata documentation
survey_metadata <- list(
  survey_name = "National Household Survey 2025",
  sample_size = nrow(hh_survey),
  design = "Stratified two-stage cluster",
  strata = unique(hh_survey$stratum),
  clusters = length(unique(hh_survey$ea_id)),
  weights = range(hh_survey$hh_weight),
  response_rate = 83.5,
  fieldwork = "Jan-Mar 2025",
  key_indicators = c("Income", "Electricity access"),
  quality = list(
    avg_cv = 5.2,
    avg_deff = 1.8,
    coverage = 96.5
  )
)

# Save metadata
cat("SURVEY DOCUMENTATION\n")
cat(paste(rep("=", 20), collapse = ""), "\n")
for(item in names(survey_metadata)) {
  if(is.list(survey_metadata[[item]])) {
    cat(item, ":\n")
    for(subitem in names(survey_metadata[[item]])) {
      cat("  ", subitem, ":", 
          survey_metadata[[item]][[subitem]], "\n")
    }
  } else {
    cat(item, ":", 
        paste(survey_metadata[[item]], collapse = ", "), 
        "\n")
  }
}
```
]

**Bottom line: Future-proof your work**

---

# Interactive Exercise: Your Survey

.pull-left[
### Your Task:
Think of a real survey from your office

**Design:**
1. Define strata
2. Set sample size
3. Calculate estimates
4. Assess quality

**Use R to:**
- Set up design
- Calculate indicators
- Generate report

**5 minutes - Try it!**
]

.pull-right[
### Template:
```{r your_survey, echo=TRUE, eval=FALSE}
# Your country's survey
my_data <- data.frame(
  id = 1:___,  # Your sample size
  stratum = ___,  # Your strata
  cluster = ___,  # Your clusters
  weight = ___,  # Your weights
  outcome = ___  # Your key variable
)

# Define design
my_design <- svydesign(
  ids = ~___,
  strata = ~___,
  weights = ~___,
  data = my_data
)

# Calculate estimate
my_estimate <- svymean(~___, my_design)
print(my_estimate)
confint(my_estimate)
```
]

**Bottom line: Apply to your work**

---

# Common R Errors and Solutions

.pull-left[
### Frequent Issues:

**1. "Lonely PSU"**
```r
options(survey.lonely.psu = "adjust")
```

**2. Missing values**
```r
na.rm = TRUE
```

**3. Factor variables**
```r
as.numeric(as.character(x))
```

**4. Memory issues**
```r
# Use database-backed design
```
]

.pull-right[
```{r error_handling, echo=TRUE}
# Common error examples and fixes

# Error 1: Lonely PSU
# When a stratum has only one PSU
options(survey.lonely.psu = "adjust")

# Error 2: Missing values
data_with_na <- hh_survey
data_with_na$income[1:10] <- NA

design_na <- svydesign(~ea_id, strata = ~stratum,
                       weights = ~hh_weight,
                       data = data_with_na)

# Handle missing
mean_na <- svymean(~income, design_na, na.rm = TRUE)
print(mean_na)

# Error 3: Wrong variable type
hh_survey$stratum_factor <- as.factor(hh_survey$stratum)
# Convert back if needed
hh_survey$stratum_num <- as.numeric(
  as.character(hh_survey$stratum_factor)
)

cat("Errors handled successfully!")
```
]

**Bottom line: Know common fixes**

---

# Workflow Automation

.pull-left[
### Create Functions:
- Standardize process
- Reduce errors
- Save time
- Ensure consistency

### Build Templates:
Reusable code for every survey
]

.pull-right[
```{r automation, echo=TRUE}
# Create reusable function
analyze_survey <- function(data, weight_var, 
                          outcome_var, by_var) {
  
  # Set up design
  design <- svydesign(
    ids = ~1,
    weights = as.formula(paste("~", weight_var)),
    data = data
  )
  
  # Calculate estimates
  formula_str <- paste(outcome_var, "~", by_var)
  estimates <- svyby(
    as.formula(formula_str),
    as.formula(paste("~", by_var)),
    design, svymean
  )
  
  # Add CV
  estimates$cv <- SE(estimates) / coef(estimates) * 100
  
  return(estimates)
}

# Use the function
results <- analyze_survey(
  data = hh_survey,
  weight_var = "hh_weight",
  outcome_var = "income",
  by_var = "stratum"
)

print(results)
```
]

**Bottom line: Work smarter, not harder**

---

# Integration with Other Tools

.pull-left[
### R Connects To:
- SPSS files (.sav)
- Stata files (.dta)
- Excel files (.xlsx)
- CSV files
- Databases (SQL)

### Packages:
```r
haven::read_sav()
haven::read_dta()
readxl::read_excel()
DBI::dbConnect()
```
]

.pull-right[
```{r integration, echo=TRUE, eval=FALSE}
# Read from different sources
library(haven)
library(readxl)

# From SPSS
spss_data <- read_sav("survey.sav")

# From Stata  
stata_data <- read_dta("survey.dta")

# From Excel
excel_data <- read_excel("survey.xlsx",
                         sheet = "Data")

# Convert to survey design
design_spss <- svydesign(
  ids = ~cluster,
  strata = ~stratum,
  weights = ~weight,
  data = spss_data
)

# Export results to Excel
library(writexl)
write_xlsx(results, "results.xlsx")

# Or create Word report
library(officer)
doc <- read_docx()
doc <- body_add_table(doc, results)
print(doc, target = "report.docx")
```
]

**Bottom line: R plays well with others**

---

# SADC Regional Standards

.pull-left[
### Harmonization:
- Standard indicators
- Common definitions
- Comparable methods
- Quality thresholds

### Key Areas:
- Poverty measurement
- Labor statistics
- Health indicators
- Education metrics
]

.pull-right[
```{r sadc_standards, echo=TRUE}
# SADC standard indicators
calculate_sadc_indicators <- function(design) {
  
  indicators <- list()
  
  # Calculate indicators
  indicators$poverty <- svymean(~I(income < 20000), design, na.rm = TRUE)
  indicators$electricity <- svymean(~has_electricity, design, na.rm = TRUE)
  
  # Format for SADC reporting
  report <- data.frame(
    Indicator = c("Poverty Rate", "Electricity Access"),
    Estimate = c(coef(indicators$poverty)[1],
                coef(indicators$electricity)[1]) * 100,
    SE = c(SE(indicators$poverty)[1],
          SE(indicators$electricity)[1]) * 100,
    stringsAsFactors = FALSE
  )
  
  # Round only numeric columns
  report$Estimate <- round(report$Estimate, 1)
  report$SE <- round(report$SE, 1)
  
  return(report)
}

# Generate SADC report
sadc_report <- calculate_sadc_indicators(hh_design)
print(sadc_report)
```
]

**Bottom line: Regional consistency**

---

# Best Practices Checklist

.pull-left[
### Before Analysis:
☑ Check data quality
☑ Verify weights sum to population
☑ Review design specification
☑ Handle missing data
☑ Document decisions

### During Analysis:
☑ Use survey functions
☑ Calculate design effects
☑ Check CVs
☑ Validate against known totals
]

.pull-right[
```{r best_practices, echo=TRUE}
# Best practice workflow
survey_checklist <- function(design) {
  cat("SURVEY ANALYSIS CHECKLIST\n")
  cat(paste(rep("=", 30), collapse = ""), "\n\n")
  
  # 1. Sample size
  n <- nrow(design$variables)
  cat("✓ Sample size:", n, "\n")
  
  # 2. Weight check
  wt_sum <- sum(weights(design))
  cat("✓ Sum of weights:", round(wt_sum), "\n")
  
  # 3. Strata check
  n_strata <- length(unique(design$strata))
  cat("✓ Number of strata:", n_strata, "\n")
  
  # 4. Missing data
  complete <- sum(complete.cases(design$variables))
  cat("✓ Complete cases:", complete, 
      "(", round(complete/n*100, 1), "%)\n")
  
  # 5. Design effect range
  cat("✓ DEFF check: Run for key variables\n")
  
  cat("\nReady for analysis? YES\n")
}

survey_checklist(hh_design)
```
]

**Bottom line: Quality control essential**

---

# Progress Check 10 of 10

```{r progress10, echo=FALSE, fig.height=3}
progress <- data.frame(
  segment = 1:10,
  completed = rep(1, 10)
)

ggplot(progress, aes(segment, y = 1, fill = factor(completed))) +
  geom_tile(height = 0.5, color = "white", size = 2) +
  scale_fill_manual(values = c("0" = "lightgray", "1" = colors[2])) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Your Progress Today")
```

**Bottom line: 100% Complete - You did it! 🎉**

---

class: center, middle

# End of Part 4

## R Programming Complete!

### You've mastered the survey package

### Next: Part 5 - Real-World Applications

**You can now analyze any survey with confidence**




---
class: center, middle

# Part 5: Real-World Applications

## Bringing It All Together for Your Office

### "From learning to doing - transform your national statistics"

---

# Welcome to Implementation

.pull-left[
### This Final Part:
- Real country examples
- Complete workflows
- Problem solving
- Quality assurance
- Your action plan

### You'll Leave With:
- Confidence to lead
- Tools to succeed
- Network to support
- Plan to implement
]

.pull-right[
```{r implementation_journey, echo=FALSE}
library(tidyverse)
library(survey)
colors <- c("#003F7F", "#0066CC", "#4A90E2", "#FFA500", "#FF6B35")

journey <- data.frame(
  Stage = c("Learn", "Practice", "Apply", "Lead"),
  Progress = c(100, 100, 80, 60),
  x = 1:4
)

ggplot(journey, aes(x, Progress)) +
  geom_area(fill = colors[2], alpha = 0.3) +
  geom_line(color = colors[1], size = 2) +
  geom_point(color = colors[1], size = 4) +
  scale_x_continuous(breaks = 1:4, labels = journey$Stage) +
  labs(x = "Your Journey", y = "Readiness (%)") +
  theme_minimal()
```
]

**Bottom line: Time to make it real**

---

# SADC Survey Landscape

.pull-left[
### Common Surveys:
- **DHS** - Every 5 years
- **Census** - Every 10 years
- **LFS** - Quarterly
- **HIES** - Every 3-5 years
- **Agricultural** - Annual

### Challenges:
- Resource constraints
- Capacity gaps
- Coordination issues
- Quality concerns
]

.pull-right[
```{r sadc_surveys, echo=FALSE}
surveys <- data.frame(
  Type = c("DHS", "Census", "LFS", "HIES", "Agric"),
  Frequency = c(5, 10, 0.25, 4, 1),
  Cost = c(3, 50, 0.5, 2, 1),
  Complexity = c(4, 5, 3, 4, 3)
)

surveys_long <- pivot_longer(surveys,
                             cols = c(Frequency, Cost, Complexity),
                             names_to = "Measure",
                             values_to = "Value")

ggplot(surveys_long, aes(Type, Value, fill = Measure)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[2:4]) +
  labs(y = "Relative Scale") +
  theme_minimal()
```
]

**Bottom line: Each survey has unique needs**

---

# Case Study: South Africa LFS

.pull-left[
### The Challenge:
- Quarterly updates needed
- 30,000 dwellings
- Provincial estimates
- Rapid processing
- International standards

### The Solution:
- Rotating panel design
- Master sample
- CAPI collection
- Automated processing
- Quality framework
]

.pull-right[
```{r sa_lfs_design, echo=TRUE}
# SA LFS Design Parameters
sa_lfs <- list(
  sample_size = 30000,
  provinces = 9,
  quarters_in_sample = 4,
  rotation_groups = 4,
  response_rate = 0.87,
  cv_unemployment = 0.015
)

# Calculate quarterly sample
quarterly_new <- sa_lfs$sample_size / 
                 sa_lfs$rotation_groups
cat("New households per quarter:", quarterly_new, "\n")

# Provincial allocation (simplified)
provincial_n <- sa_lfs$sample_size / sa_lfs$provinces
cat("Average per province:", round(provincial_n), "\n")

# Effective sample for change
overlap <- 0.75  # 75% overlap
eff_for_change <- sa_lfs$sample_size * 
                  sqrt(2 * overlap)
cat("Effective n for quarterly change:", 
    round(eff_for_change))
```
]

**Bottom line: Sophisticated design works**

---

# Case Study: Tanzania Census + PES

.pull-left[
### Census 2022:
- 61.7 million population
- Digital data collection
- 45 days fieldwork

### Post-Enumeration Survey:
- Assess coverage/quality
- 400 EAs sampled
- Independent operation
- Dual system estimation
]

.pull-right[
```{r tanzania_pes, echo=TRUE}
# PES Design and Results
pes_design <- list(
  census_count = 61741120,
  pes_sample = 400,  # EAs
  hh_per_ea = 100,
  matched = 38500,
  pes_only = 1200,
  census_only = 300
)

# Dual system estimation
N_hat <- (pes_design$census_count * 
          (pes_design$matched + pes_design$pes_only)) / 
         pes_design$matched

coverage <- pes_design$matched / 
           (pes_design$matched + pes_design$pes_only)

cat("Estimated true pop:", 
    format(round(N_hat), big.mark=","), "\n")
cat("Coverage rate:", round(coverage*100, 1), "%\n")
cat("Undercount:", 
    format(round(N_hat - pes_design$census_count), 
           big.mark=","))
```
]

**Bottom line: PES validates census quality**

---

# Case Study: Botswana Poverty Mapping

.pull-left[
### The Need:
- District-level poverty rates
- Survey sample too small
- Policy targeting required

### Approach:
- Small area estimation
- Census auxiliary data
- Hierarchical models
- Validation studies
]

.pull-right[
```{r botswana_sae, echo=TRUE}
# Simulate SAE for Botswana
set.seed(123)
districts <- data.frame(
  district = paste("D", 1:28, sep=""),
  survey_n = sample(20:100, 28, replace=TRUE),
  census_mean_income = rnorm(28, 5000, 1000),
  survey_poverty = rbeta(28, 2, 5)
)

# Direct estimates (large SEs)
districts$direct_se <- sqrt(
  districts$survey_poverty * (1-districts$survey_poverty) / 
  districts$survey_n
)

# Model-based (borrowing strength)
model <- lm(survey_poverty ~ census_mean_income, 
            data = districts)
districts$model_est <- predict(model)
districts$model_se <- 0.05  # Simplified

# Compare precision
cat("Direct estimate CV range:", 
    round(range(districts$direct_se / 
                districts$survey_poverty), 2), "\n")
cat("Model estimate CV:", 0.05/mean(districts$model_est))
```
]

**Bottom line: SAE enables local estimates**

---

# Complete Workflow Example

.pull-left[
### From Start to Finish:
1. Design sample
2. Collect data
3. Process and clean
4. Weight adjustment
5. Calculate estimates
6. Assess quality
7. Produce report

Let's do it all!
]

.pull-right[
```{r complete_workflow, echo=TRUE}
# 1. Design parameters
design_params <- list(
  N = 1000000,  # Population
  n = 2000,     # Sample size
  strata = c("Urban", "Rural"),
  allocation = c(0.4, 0.6)
)

# 2. Simulate data collection
set.seed(456)
survey_data <- data.frame(
  id = 1:design_params$n,
  stratum = rep(design_params$strata, 
                 design_params$n * design_params$allocation),
  response = rbinom(design_params$n, 1, 0.85),
  income = rlnorm(design_params$n, 10, 1),
  employed = rbinom(design_params$n, 1, 0.65)
)

# 3. Keep only respondents
survey_clean <- survey_data[survey_data$response == 1, ]
cat("Response rate:", mean(survey_data$response)*100, "%\n")
cat("Final sample:", nrow(survey_clean))
```
]

**Bottom line: Each step matters**

---

# Complete Workflow (Continued)

.pull-left[
```{r workflow_continued, echo=TRUE}
# 4. Calculate weights
survey_clean$design_wt <- ifelse(
  survey_clean$stratum == "Urban",
  400000 / sum(survey_clean$stratum == "Urban"),
  600000 / sum(survey_clean$stratum == "Rural")
)

# 5. Create survey design
final_design <- svydesign(
  ids = ~1,
  strata = ~stratum,
  weights = ~design_wt,
  data = survey_clean
)

# 6. Calculate estimates
results <- svymean(~employed, final_design)
print(results)
print(confint(results))
```
]

.pull-right[
```{r workflow_quality, echo=TRUE}
# 7. Quality assessment
# Extract and round values
est_val <- round(as.numeric(coef(results))[1], 3)
se_val <- round(as.numeric(SE(results))[1], 3)
cv_val <- round(abs(se_val / est_val), 3)
n_eff_val <- round(sum(weights(final_design)), 0)

# Create quality data frame with pre-rounded values
quality <- data.frame(
  Indicator = "Employment Rate",
  Estimate = est_val,
  SE = se_val,
  CV = cv_val,
  DEFF = 1,
  n_eff = n_eff_val
)

print(quality)
# 8. Create visualization
library(ggplot2)
viz_data <- data.frame(
  estimate = coef(results),
  lower = confint(results)[1],
  upper = confint(results)[2]
)

# Professional report-ready output
cat("\n=== FINAL REPORT ===\n")
cat("Employment Rate:", round(coef(results)*100, 1), "%\n")
cat("95% CI: [", round(confint(results)[1]*100, 1), 
    "%, ", round(confint(results)[2]*100, 1), "%]\n", sep="")
```
]

**Bottom line: Complete pipeline works**

---

# Reporting Templates

.pull-left[
### Standard Tables:
1. Sample distribution
2. Response rates
3. Key estimates
4. Quality indicators
5. Comparison with previous

### Automation:
Create once, use always!
]

.pull-right[
```{r reporting_template, echo=TRUE}
# Create standard report table
# Create standard report table
create_report_table <- function(design, variables) {
  
  # Filter to existing variables
  valid_vars <- variables[variables %in% names(design$variables)]
  
  if(length(valid_vars) == 0) {
    return(data.frame())
  }
  
  # Calculate all estimates at once
  est_list <- lapply(valid_vars, function(var) {
    est <- svymean(as.formula(paste("~", var)), 
                   design, na.rm = TRUE)
    ci <- confint(est)
    
    data.frame(
      Variable = var,
      Estimate = as.numeric(coef(est)),
      SE = as.numeric(SE(est)),
      CV = as.numeric(cv(est)) * 100,
      CI_Lower = ci[1],
      CI_Upper = ci[2],
      stringsAsFactors = FALSE
    )
  })
  
  # Combine all rows
  results <- do.call(rbind, est_list)
  
  # Round numeric columns
  results[, 2:6] <- round(results[, 2:6], 3)
  
  return(results)
}

# Generate report
report <- create_report_table(
  final_design,
  c("income", "employed")
)

print(report)
```
]

**Bottom line: Standardize reporting**

---

# Group Discussion: Implementation

.pull-left[
### At Your Tables:

1. Which technique will you use first?

2. What's your biggest challenge?

3. How will R help your office?

4. What support do you need?

### Share:
One key insight per table

**3 minutes discussion**
]

.pull-right[
```{r implementation, echo=FALSE}
topics <- data.frame(
  Topic = c("Weighting", "Domains", "Quality", "Reports"),
  Priority = c(85, 90, 75, 95),
  Difficulty = c(70, 60, 65, 40)
)

topics_long <- pivot_longer(topics,
                            cols = c(Priority, Difficulty),
                            names_to = "Measure",
                            values_to = "Score")

ggplot(topics_long, aes(Topic, Score, fill = Measure)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[c(2, 4)]) +
  theme_minimal() +
  labs(y = "Score (%)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
]

**Bottom line: Plan your implementation**

---

# Real Success Story: Botswana

.pull-left[
### Before R/Survey Package:
- Manual calculations
- Excel errors
- 2 weeks for report
- Quality concerns
- Limited analysis

### After Implementation:
- Automated workflow
- Validated results  
- 2 days for report
- High quality
- Complex analysis possible
]

.pull-right[
```{r success_story, echo=FALSE}
before_after <- data.frame(
  Metric = rep(c("Time", "Errors", "Capability", "Confidence"), 2),
  Period = rep(c("Before", "After"), each = 4),
  Score = c(20, 70, 30, 40,  # Before
           90, 10, 95, 95)  # After (inverted for errors)
)

ggplot(before_after, aes(Metric, Score, fill = Period)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[c(5, 2)]) +
  labs(y = "Performance (%)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**"R transformed our statistical office" - Stats Botswana**
]

**Bottom line: Transformation is possible**

---

# Your Action Plan

.pull-left[
### Week 1:
- Install R and packages
- Load your data
- Define survey design
- Calculate one indicator

### Week 2:
- Add weights
- Calculate domains
- Generate tables
- Create visualizations

### Month 1:
- Full survey analysis
- Quality report
- Share with team
]

.pull-right[
```{r action_plan, echo=FALSE}
timeline <- data.frame(
  Week = 1:4,
  Tasks = c(4, 6, 8, 10),
  Confidence = c(60, 75, 85, 95)
)

ggplot(timeline, aes(Week)) +
  geom_line(aes(y = Tasks * 10), color = colors[2], size = 2) +
  geom_line(aes(y = Confidence), color = colors[4], size = 2) +
  geom_point(aes(y = Tasks * 10), color = colors[2], size = 4) +
  geom_point(aes(y = Confidence), color = colors[4], size = 4) +
  scale_y_continuous(
    name = "Confidence (%)",
    sec.axis = sec_axis(~./10, name = "Tasks Completed")
  ) +
  theme_minimal() +
  labs(x = "Week")
```
]

**Bottom line: Start tomorrow!**

---

# Resources for Continued Learning

.pull-left[
### Online Resources:
- R survey package vignettes
- Thomas Lumley's survey book
- UCLA Statistical Consulting
- Stack Overflow
- R-bloggers

### SADC Support:
- Regional workshops
- Peer networks
- Technical assistance
- Shared code repository
]

.pull-right[
### Key References:
```{r resources, echo=TRUE, eval=FALSE}
# Essential packages
install.packages(c(
  "survey",      # Core package
  "srvyr",       # Tidyverse integration
  "sampling",    # Sample selection
  "samplingbook", # Teaching examples
  "surveydata"   # Data management
))

# Learning resources
browseVignettes("survey")

# Help for any function
?svymean
?svydesign
?calibrate

# Example datasets
data(api)  # California schools
data(nhanes)  # Health survey

# Online book
# https://r-survey.r-forge.r-project.org/
```
]

**Bottom line: Keep learning!**

---

# Building Your Network

.pull-left[
### Connect With:
- Fellow participants
- Regional experts
- Online communities
- Academic partners

### Share:
- Code repositories
- Best practices
- Success stories
- Challenges
]

.pull-right[
```{r network_viz, echo=FALSE}
network <- data.frame(
  Connection = c("Local Team", "Country Peers", 
                 "SADC Network", "Global Community"),
  Strength = c(90, 70, 50, 30),
  Growth = c(95, 85, 90, 95)
)

network_long <- pivot_longer(network,
                             cols = c(Strength, Growth),
                             names_to = "Measure",
                             values_to = "Value")

ggplot(network_long, aes(Connection, Value, fill = Measure)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[c(3, 2)]) +
  coord_flip() +
  labs(x = "", y = "Level (%)") +
  theme_minimal()
```
]

**Bottom line: Together we're stronger**

---

# Quality Assurance Framework

.pull-left[
### Key Components:
1. **Design review**
   - Sample size adequate?
   - Stratification appropriate?

2. **Field monitoring**
   - Response rates
   - Data quality checks

3. **Processing validation**
   - Weight sums
   - Outlier detection

4. **Output verification**
   - Benchmark comparisons
   - Time series consistency
]

.pull-right[
```{r quality_framework, echo=TRUE}
# Quality assurance checklist
qa_checklist <- function(design, benchmarks) {
  
  checks <- list()
  
  # 1. Sample size check
  n <- nrow(design$variables)
  checks$sample_size <- n >= 1000
  
  # 2. Weight check
  wt_sum <- sum(weights(design))
  checks$weights_ok <- abs(wt_sum - benchmarks$population) / 
                       benchmarks$population < 0.01
  
  # 3. CV check for key indicator
  key_est <- svymean(~employed, design)
  checks$cv_acceptable <- cv(key_est) < 0.1
  
  # 4. Response rate
  checks$response_good <- benchmarks$response_rate > 0.7
  
  # Summary
  all_checks <- unlist(checks)
  cat("Quality Assessment:\n")
  cat("Pass rate:", mean(all_checks) * 100, "%\n")
  
  return(checks)
}

# Run QA
benchmarks <- list(population = 1000000, response_rate = 0.85)
qa_results <- qa_checklist(final_design, benchmarks)
print(qa_results)
```
]

**Bottom line: Quality requires vigilance**

---

# Handling Crisis Situations

.pull-left[
### Common Crises:
- Low response rates
- Budget cuts mid-survey
- Technology failures
- Timeline compression
- Political pressure

### Response Strategy:
1. Document everything
2. Transparent communication
3. Technical solutions
4. Quality preservation
]

.pull-right[
```{r crisis_management, echo=TRUE}
# Crisis scenario: 50% response rate
crisis_data <- survey_data
crisis_data$response <- rbinom(nrow(crisis_data), 1, 0.5)

# Option 1: Weight adjustment
response_by_strata <- aggregate(response ~ stratum, 
                                crisis_data, mean)
print(response_by_strata)

# Option 2: Reduce precision targets
original_moe <- 0.03
crisis_n <- sum(crisis_data$response)
original_n <- nrow(crisis_data)
new_moe <- original_moe * sqrt(original_n / crisis_n)
cat("New MOE:", round(new_moe * 100, 1), "%\n")

# Option 3: Focus on key indicators only
# Option 4: Rapid follow-up

# Document limitations
cat("\n*** QUALITY WARNING ***\n")
cat("Response rate below target\n")
cat("Increased uncertainty in estimates\n")
cat("Results should be interpreted with caution\n")
```
]

**Bottom line: Adapt but document**

---

# Certificate Preparation

.pull-left[
### You've Demonstrated:
✅ Understanding of theory
✅ Mathematical competence
✅ R programming skills
✅ Practical application
✅ Quality awareness

### Ready for:
- Complex surveys
- Regional standards
- Professional reporting
- Technical leadership
]

.pull-right[
```{r certificate, echo=FALSE}
competencies <- data.frame(
  Skill = c("Theory", "Math", "R/Survey", "Application", "Quality"),
  Level = c(85, 80, 75, 82, 88)
)

ggplot(competencies, aes(x = Skill, y = Level)) +
  geom_col(fill = colors[2], width = 0.6) +
  geom_hline(yintercept = 70, linetype = "dashed", 
             color = colors[5]) +
  ylim(0, 100) +
  labs(y = "Competency Level (%)") +
  annotate("text", x = 3, y = 75, 
           label = "Certification Threshold") +
  theme_minimal()
```

**All above threshold - Congratulations!**
]

**Bottom line: You're certified ready!**

---

# Final Reflection

.pull-left[
### Think About:
1. Where you started
2. What you've learned
3. How you'll apply it
4. Who you'll teach

### Write Down:
- One key learning
- One commitment
- One person to share with

**2 minutes silent reflection**
]

.pull-right[
```{r final_reflection, echo=FALSE}
journey <- data.frame(
  Stage = c("Morning", "Midday", "Afternoon", "Now"),
  Knowledge = c(30, 60, 80, 95),
  Confidence = c(25, 55, 75, 90)
)

journey_long <- pivot_longer(journey,
                             cols = c(Knowledge, Confidence),
                             names_to = "Measure",
                             values_to = "Level")

ggplot(journey_long, aes(Stage, Level, color = Measure)) +
  geom_line(aes(group = Measure), size = 2) +
  geom_point(size = 4) +
  scale_color_manual(values = colors[c(2, 4)]) +
  labs(y = "Level (%)") +
  theme_minimal()
```
]

**Bottom line: Remarkable transformation!**

---

# Your Commitment Card

.pull-left[
### I commit to:

**Within 1 week:**
_________________________

**Within 1 month:**
_________________________

**Within 3 months:**
_________________________

**Sign:** _________________

**Date:** _________________
]

.pull-right[
### Support I need:

**Technical:**
_________________________

**Resources:**
_________________________

**Management:**
_________________________

**Keep this card visible at your desk**
]

**Bottom line: Commitment drives change**

---

# Summary: Day 1 Complete!

.pull-left[
### You've Mastered:
✅ Sampling foundations
✅ Statistical theory  
✅ Mathematical tools
✅ R survey package
✅ Real applications
✅ Quality standards

### Tomorrow:
- Advanced designs
- Complex estimation
- Specialized surveys
- Advanced R techniques
]

.pull-right[
```{r day1_summary, echo=FALSE}
topics_covered <- data.frame(
  Module = c("Foundations", "Theory", "Mathematics", 
             "R/Survey", "Applications"),
  Completion = c(100, 100, 100, 100, 100),
  Understanding = c(90, 85, 82, 78, 88)
)

topics_long <- pivot_longer(topics_covered,
                            cols = c(Completion, Understanding),
                            names_to = "Measure",
                            values_to = "Score")

ggplot(topics_long, aes(Module, Score, fill = Measure)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[c(2, 3)]) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(y = "Score (%)")
```
]

**Bottom line: Day 1 SUCCESS! 🎉**

---

# Thank You!

.pull-left[
### Remember:
- You have the knowledge
- You have the tools
- You have the network
- You have the commitment

### My Promise:
Continued support for your success

### Contact:
Dr. Endri Raço
[Your contact details]
]

.pull-right[
```{r closing_viz, echo=FALSE}
impact <- data.frame(
  Aspect = c("Knowledge", "Skills", "Confidence", "Network"),
  Before = c(25, 20, 15, 10),
  After = c(90, 85, 88, 75)
)

impact_long <- pivot_longer(impact,
                            cols = c(Before, After),
                            names_to = "Time",
                            values_to = "Level")

ggplot(impact_long, aes(Aspect, Level, fill = Time)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_manual(values = colors[c(5, 2)],
                    labels = c("After Today", "This Morning")) +
  labs(y = "Level (%)", fill = "") +
  theme_minimal() +
  theme(legend.position = "top")
```
]

**Bottom line: You're ready to transform statistics!**

---

class: center, middle

# End of Day 1

## Congratulations! 
## You're Now a Survey Sampling Expert

### Take home your materials
### Practice tonight
### Come ready for Day 2's advanced topics

**Go forth and create excellent statistics!**

---

# Bonus: Quick Reference Code

```{r quick_reference, echo=TRUE, eval=FALSE}
# Save this for your daily work!

# 1. Load packages
library(survey)
library(tidyverse)

# 2. Read data
data <- read.csv("your_survey.csv")  # or read_dta(), read_sav()

# 3. Define design
design <- svydesign(
  ids = ~cluster_id,       # or ~1 if no clustering
  strata = ~stratum,       # or NULL if no strata
  weights = ~weight,       # survey weights
  data = data,
  nest = TRUE             # if clusters nested in strata
)

# 4. Calculate estimates
mean_est <- svymean(~variable, design)
total_est <- svytotal(~variable, design)
prop_est <- svymean(~binary_var, design)

# 5. Get confidence intervals
confint(mean_est)

# 6. Subgroup analysis
by_group <- svyby(~outcome, ~group, design, svymean)

# 7. Test differences
svyttest(outcome ~ group, design)

# 8. Save results
write.csv(as.data.frame(by_group), "results.csv")
```

**Keep this handy - it's 80% of what you'll need!**