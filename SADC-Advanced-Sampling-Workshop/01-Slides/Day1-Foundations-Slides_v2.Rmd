---
title: "Advanced Sampling Methods for Household Surveys"
subtitle: "Day 1: Harry's New Challenge - Foundations of Complex Surveys"
author: "Dr. Endri Raço"
institute: "SADC Regional Training Workshop"
date: "2025"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "metropolis", "metropolis-fonts"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: "%current% / %total%"
---

```{r setup, include=FALSE}
# Load required libraries for the workshop
library(tidyverse)
library(survey)
library(knitr)
library(kableExtra)
library(gridExtra)
library(ggplot2)
library(plotly)
library(DT)
library(srvyr)
library(gt)

# Set global options
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  fig.width = 10, 
  fig.height = 6, 
  fig.align = 'center'
)

# Set theme for all plots
theme_set(theme_minimal(base_size = 14))

# Create comprehensive sim_data with all needed variables
set.seed(2025)
sim_data <- data.frame(
  cluster = rep(1:50, each = 20),
  stratum = rep(c("Urban", "Rural"), c(400, 600)),
  urban_rural = rep(c("Urban", "Rural"), c(400, 600)),
  weight = rep(c(250, 180), c(400, 600)),
  income = rnorm(1000, mean = rep(c(60000, 35000), c(400, 600)), sd = 10000),
  household_size = rpois(1000, lambda = 3.5),
  age = sample(18:65, 1000, replace = TRUE)
)

# Create design objects for later use
complex_design <- svydesign(
  ids = ~cluster,
  strata = ~stratum,
  weights = ~weight,
  data = sim_data
)

srvyr_design <- sim_data %>%
  as_survey_design(
    ids = cluster,
    strata = stratum,
    weights = weight
  )
```

---
class: center, middle

# Welcome to Advanced Sampling Methods
## For Household Surveys

<img src="https://www.sadc.int/sites/default/files/2021-08/SADC_logo.png" width="200">

### Southern African Development Community
### Regional Training Workshop
### Dr. Endri Raço
### 2025

---

# Workshop Materials Location

## 📁 Your Working Directory Structure

```
SADC-Advanced-Sampling-Workshop/
├── 01-Slides/          # All presentation files
├── 02-Data/            # Sample datasets
├── 03-Scripts/         # R scripts to run
│   ├── Day1_Exercise_*.R
│   └── Day1_Solutions/
├── 05-Documentation/   # References & guides
└── 06-Outputs/        # Your results go here
```

## 🚀 Getting Started

.pull-left[
### Open now:
`03-Scripts/Day1_Setup.R`
]

.pull-right[
### We'll run code:
- Together (I demo)
- You practice
- Share results
]

---

# How This Workshop Works

## Three Modes of Learning

### 1. 👀 **Watch Mode**
I demonstrate concepts on slides

### 2. 💻 **Code Mode**
You run scripts from `03-Scripts/`

### 3. 🤝 **Share Mode**
Discuss results with neighbors

## Look for These Indicators:

- 📝 **EXERCISE:** Open and run script
- 🏠 **HOMEWORK:** Complete after session
- 💡 **TIP:** Important note
- ⚠️ **WARNING:** Common mistake

---

# About Your Instructor

## Dr. Endri Raço

.pull-left[
### Background
- **Ph.D. in Statistics and Operations Research**
- **14+ years** training government statistical agencies
- **Former advisor** to:
  - INSTAT Albania
  - Kosovo Agency of Statistics
  - Multiple African NSOs
]

.pull-right[
### Expertise
- Complex survey design
- Sampling methodology
- Statistical software (R, Python, STATA)
- Quality assurance frameworks
- Survey operations management
]

---

# My Teaching Philosophy

## Learning by Doing

> "Statistics is not just about numbers—it's about understanding the stories data tells us about people's lives"

### My Commitments to You:

1. **Practical examples** from real surveys
2. **Hands-on exercises** every 30 minutes
3. **Your questions** shape our discussion
4. **No one left behind** - we learn together

---

# Your 5-Day Journey

```{r journey_viz, echo=FALSE, fig.height=5}
journey <- data.frame(
  Day = 1:5,
  Title = c("Foundations", "Imperfect Frames", "Variance & Error", 
            "Specialized Methods", "Integration & Future"),
  Harry_Challenge = c("New Survey Design", "Dirty Data", "Precision Questions",
                      "Special Populations", "Final Presentation"),
  Complexity = c(20, 40, 60, 80, 100)
)

ggplot(journey, aes(x = Day, y = Complexity)) +
  geom_line(size = 2, color = "#2E86AB") +
  geom_point(size = 6, color = "#A23B72") +
  geom_text(aes(label = Title), vjust = -1.5, size = 4, fontface = "bold") +
  scale_x_continuous(breaks = 1:5, labels = paste("Day", 1:5)) +
  scale_y_continuous(limits = c(0, 120)) +
  labs(title = "Your Learning Journey",
       subtitle = "Building complexity throughout the week",
       x = "", y = "Complexity Level") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", size = 18))
```

---

# Workshop Schedule - Day 1 Morning

| Time | Topic | Mode |
|------|-------|------|
| 8:00-8:30 | Welcome & Setup | Setup |
| 8:30-9:15 | Review of Basics | Watch & Code |
| 9:15-10:00 | Stratification Deep Dive | Interactive |
| 10:00-10:15 | ☕ Coffee Break | Network |
| 10:15-11:00 | Cluster Sampling | Watch & Code |
| 11:00-11:45 | Design Effects | Calculate |
| 11:45-12:30 | Multi-stage Designs | Design |

---

# Workshop Schedule - Day 1 Afternoon

| Time | Topic | Mode |
|------|-------|------|
| 13:30-14:15 | Total Survey Error | Conceptual |
| 14:15-15:00 | Variance Estimation | Technical |
| 15:00-15:15 | ☕ Tea Break | Refresh |
| 15:15-16:00 | Weight Calculations | Hands-on |
| 16:00-16:45 | Quality Framework | Applied |
| 16:45-17:00 | Day 1 Wrap-up | Q&A |

---

# Your Learning Objectives

## By End of Today

.pull-left[
### Morning:
☐ Design stratified samples

☐ Calculate design effects

☐ Implement PPS sampling

☐ Build multi-stage designs
]

.pull-right[
### Afternoon:
☐ Apply variance estimation

☐ Construct survey weights

☐ Implement quality controls

☐ Document design decisions
]

---

# Pre-Workshop Check

## Quick Self-Assessment

Rate yourself (1-5):

1. **R Programming comfort?** _____

2. **Survey sampling knowledge?** _____

3. **Statistical theory understanding?** _____

4. **Field operations experience?** _____

5. **Report writing skills?** _____

### Don't worry if scores are low!
This workshop accommodates all levels

---

# Technical Setup

## Let's Verify Everything Works

📝 **EXERCISE:** Run Setup Script

**File:** `03-Scripts/Day1_Setup.R`

### Expected Output:
✅ R version 4.0 or higher

✅ All required packages installed

✅ Working directory set correctly

✅ Can read/write to folders

⚠️ **Raise hand if any errors!**

---
class: center, middle

# Meet Harry
## Your Companion This Week

---

# Who is Harry?

.pull-left[
### Background:
- Statistician at NSO
- 5 years experience
- Knows basic sampling
- Ambitious learner
- Just like you!
]

.pull-right[
### His Role:
- Makes mistakes we all make
- Asks your questions
- Learns alongside you
- Celebrates victories
- Never gives up
]

---

# Harry's Monday Morning

## The Email That Changed Everything

```
From: Director General
To: Harry
Subject: URGENT - National Household Survey Design

Harry,

The Minister needs our new National Multi-Purpose 
Household Survey designed by Friday. 

Requirements:
- Cover all 14 regions
- Urban AND rural representation  
- Measure income, health, education
- Budget: $80,000
- Sample size: At least 2,000 households
- Timeline: Report by Friday

This is your chance to shine.

Good luck!
DG
```

---

# Harry's Initial Reaction

## The Overwhelming Feeling

.center[
### 😰 "Where do I even start?"
]

## Harry's Concerns:

1. **Never designed** a complex survey alone
2. **Budget constraints** are severe
3. **Timeline is impossible** (5 days?!)
4. **Political pressure** for good results
5. **Team expects** him to know everything

---

# But Harry Remembers...

## His Training

> "Every expert was once a beginner"

> "Good design beats perfect analysis"

> "Start simple, add complexity"

## Harry's Plan:

1. Review fundamentals
2. Choose appropriate design
3. Calculate sample sizes
4. Document everything
5. Ask for help when needed

**Let's help Harry succeed!**

---
class: inverse, center, middle

# Part 1: Foundations
## Building on Solid Ground

---

# Why Review Basics?

## Foundation Matters

### Common Mistakes Even Experts Make:

1. **Forgetting assumptions** behind methods
2. **Misapplying formulas** to wrong designs
3. **Ignoring prerequisites** for techniques
4. **Over-complicating** simple problems

---

# Population vs Sample

## The Fundamental Relationship

```{r pop_sample_viz, echo=FALSE, fig.height=5}
set.seed(2025)
pop_data <- expand.grid(x = 1:20, y = 1:10)
pop_data$type <- "Population"
sample_indices <- sample(1:nrow(pop_data), 40)
pop_data$type[sample_indices] <- "Sample"

ggplot(pop_data, aes(x = x, y = y, color = type, size = type)) +
  geom_point(alpha = 0.8) +
  scale_color_manual(values = c("Population" = "lightgray", "Sample" = "#A23B72")) +
  scale_size_manual(values = c("Population" = 3, "Sample" = 5)) +
  labs(title = "Every Sample Unit Represents Many Population Units",
       subtitle = "Sample of 40 from Population of 200",
       color = "", size = "") +
  theme_void() +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12))
```

---

# Key Insight

## Representation Principle

### Each sampled unit represents:

$$\text{Weight} = \frac{\text{Population Size}}{\text{Sample Size}}$$

### Example:
- Population: 2,000,000 households
- Sample: 2,000 households
- Each represents: 1,000 households

💡 **TIP:** This is why weights matter!

---

# Sampling Frame

## The Foundation of Everything

### Definition:
Complete list of all units from which sample is drawn

### Requirements:

✅ **Complete** - covers entire population

✅ **Current** - up-to-date information

✅ **No duplicates** - each unit appears once

✅ **Accessible** - can actually reach units

✅ **Relevant** - matches survey objectives

---

# Harry's Frame Challenge

## Available Options

```{r frame_comparison, echo=FALSE}
frame_options <- data.frame(
  Source = c("Census 2020", "Utility Bills", "Mobile Network", "Tax Registry"),
  Coverage = c("95%", "60%", "85%", "40%"),
  Quality = c("Good", "Excellent", "Fair", "Excellent"),
  Cost = c("Free", "$5,000", "$10,000", "Free"),
  Update = c("5 years old", "Monthly", "Real-time", "Annual")
)

kable(frame_options) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12)
```

### Harry's Choice?
Census 2020 + field updates

---

# Probability Sampling

## The Non-Negotiable Requirement

### Core Principle:
**Every unit must have a known, non-zero probability of selection**

### This Enables:

.pull-left[
✅ Unbiased estimates

✅ Valid inference

✅ Measurable precision

✅ Scientific credibility
]

.pull-right[
❌ Without it:

- No valid statistics
- No confidence intervals
- No generalization
- Just opinions!
]

---

# Types of Probability Sampling

```{r sampling_types, echo=FALSE, fig.height=5}
types <- data.frame(
  Method = c("SRS", "Systematic", "Stratified", "Cluster", "Multi-stage"),
  Complexity = 1:5,
  Cost = c(100, 95, 85, 60, 50),
  Precision = c(100, 102, 115, 85, 90)
)

par(mfrow = c(1, 2))
barplot(types$Cost, names.arg = types$Method,
        main = "Relative Cost", col = "#2E86AB",
        ylab = "Cost Index", las = 2)
barplot(types$Precision, names.arg = types$Method,
        main = "Relative Precision", col = "#52B788",
        ylab = "Precision Index", las = 2)
```

---

# Simple Random Sampling (SRS)

## The Gold Standard

### Definition:
Every possible sample of size n has equal probability

### Properties:
- **Unbiased:** E[x̄] = μ
- **Simple variance:** Var(x̄) = σ²/n
- **Easy to explain**
- **Software friendly**

---

# SRS in Practice

📝 **EXERCISE:** Your First SRS

**File:** `03-Scripts/Day1_Exercise_01_SRS.R`

```{r srs_demo, echo=TRUE}
# Create population frame
population <- data.frame(
  household_id = 1:10000,
  region = sample(c("North", "South", "East", "West"), 
                  10000, replace = TRUE),
  urban_rural = sample(c("Urban", "Rural"), 10000, 
                      replace = TRUE, prob = c(0.4, 0.6))
)

# Draw SRS
srs_sample <- population %>%
  slice_sample(n = 500)

nrow(srs_sample)
```

---

# Check Sample Distribution

```{r srs_check, echo=TRUE}
# Population distribution
pop_dist <- table(population$urban_rural) / nrow(population)
print(pop_dist)

# Sample distribution
sample_dist <- table(srs_sample$urban_rural) / nrow(srs_sample)
print(sample_dist)

# Close but not exact!
```

---

# SRS Variance

## The Formula

For simple random sampling without replacement:

$$V(\bar{y}) = \frac{s^2}{n} \times \frac{N-n}{N-1}$$

Where:
- s² = sample variance
- n = sample size
- N = population size
- (N-n)/(N-1) = finite population correction

---

# Calculate SRS Variance

```{r srs_variance, echo=TRUE}
# Generate income data
srs_sample$income <- rnorm(500, 45000, 10000)

# Calculate mean and variance
mean_income <- mean(srs_sample$income)
var_mean <- var(srs_sample$income) / nrow(srs_sample)
se_mean <- sqrt(var_mean)

cat("Mean:", round(mean_income, 0), "\n")
cat("SE:", round(se_mean, 0), "\n")
cat("CV:", round(se_mean/mean_income * 100, 2), "%")
```

---

# When SRS Falls Short

## Harry's Realization

```{r srs_problem, echo=TRUE}
# Harry tries SRS for his survey
set.seed(123)  # Bad luck seed!
harry_srs <- population %>%
  slice_sample(n = 1000)

table(harry_srs$urban_rural)
```

### Problems:

❌ Only 376 urban (need 400+)

❌ No control over regions

❌ Scattered households

❌ Expensive fieldwork

---

# The Cost Problem

## SRS Field Operations

```{r cost_viz, echo=FALSE, fig.height=4}
set.seed(2025)
# Visualize scattered sample
srs_locations <- data.frame(
  x = runif(30, 0, 100),
  y = runif(30, 0, 100),
  type = "SRS Sample"
)

ggplot(srs_locations, aes(x = x, y = y)) +
  geom_point(size = 3, color = "#A23B72") +
  geom_path(alpha = 0.3) +
  labs(title = "SRS: Interviewers Travel Everywhere",
       subtitle = "Total distance: Very long!") +
  theme_minimal()
```

---

# Systematic Sampling

## The Practical Alternative

### Process:
1. List N units in frame
2. Calculate interval k = N/n
3. Random start r between 1 and k
4. Select units: r, r+k, r+2k, ...

### Example:
- N = 10,000
- n = 500
- k = 20
- Random start = 7
- Select: 7, 27, 47, 67, ...

---

# Systematic Visual

```{r systematic_visual, echo=FALSE, fig.height=4}
sys_units <- 1:50
selected <- seq(3, 50, by = 5)
sys_data <- data.frame(
  unit = sys_units,
  selected = sys_units %in% selected
)

ggplot(sys_data, aes(x = unit, y = 1, fill = selected)) +
  geom_tile(height = 0.8, color = "white", size = 1) +
  scale_fill_manual(values = c("FALSE" = "lightgray", "TRUE" = "#A23B72"),
                   labels = c("Not selected", "Selected")) +
  labs(title = "Systematic Sampling Pattern",
       subtitle = "Interval k=5, Random start=3") +
  theme_void() +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold", size = 14))
```

---

# Systematic Implementation

```{r systematic_code, echo=TRUE}
# Systematic sampling function
systematic_sample <- function(frame, n) {
  N <- nrow(frame)
  k <- floor(N/n)  # Interval
  start <- sample(1:k, 1)  # Random start
  
  # Generate indices
  indices <- seq(start, N, by = k)
  indices <- indices[1:n]  # Ensure exactly n
  
  return(frame[indices, ])
}

# Apply to population
sys_sample <- systematic_sample(population, 500)
table(sys_sample$urban_rural)
```

---

# Systematic Advantages

## Why Harry Likes It

✅ **Simple** to implement in field

✅ **Even spread** across frame

✅ **Often more precise** than SRS

✅ **One random number** only

✅ **Easy to check** in field

---

# The Hidden Danger

## Periodicity Problem

```{r periodicity, echo=FALSE, fig.height=4}
# Create periodic data
periodic_data <- data.frame(
  unit = 1:100,
  value = sin(1:100 * 2 * pi / 10) + rnorm(100, 0, 0.2),
  selected = (1:100 - 3) %% 10 == 0
)

ggplot(periodic_data, aes(x = unit, y = value)) +
  geom_line(color = "gray", size = 1) +
  geom_point(data = filter(periodic_data, selected),
             color = "#A23B72", size = 4) +
  labs(title = "Disaster: Systematic Sampling with Hidden Pattern",
       subtitle = "Sample catches only peaks!",
       y = "Household Income") +
  theme_minimal()
```

---

# Detecting Periodicity

```{r detect_period, echo=TRUE}
# Check for patterns in frame
# Look at every 10th household
check_pattern <- population %>%
  mutate(position = row_number(),
         mod10 = position %% 10) %>%
  group_by(mod10, urban_rural) %>%
  summarise(n = n(), .groups = 'drop')

# Check variance - should be small
var(check_pattern$n)
```

💡 **TIP:** Always inspect frame for patterns!

---

# Systematic vs SRS

📝 **EXERCISE:** Compare Methods

**File:** `03-Scripts/Day1_Exercise_02_Compare.R`

Run the comparison and observe:
1. Distribution of estimates
2. Standard errors
3. Geographic spread

---

# Quick Break

## 5 Minutes - Stay in Room

### Quick Poll:

Raise your hand if you've used:
- Simple Random Sampling?
- Systematic Sampling?
- Noticed periodicity problems?

### Think About:
What sampling method does your office use most?

---
class: inverse, center, middle

# Part 2: Stratification
## Divide and Conquer

---

# Why Stratify?

## Harry's Insight

> "If urban and rural are so different, why not sample them separately?"

### The Stratification Principle:

**Divide heterogeneous population into homogeneous groups**

### Result:
Better precision for same sample size!

---

# Stratification Concept

```{r strat_concept, echo=FALSE, fig.height=5}
set.seed(2025)
# Unstratified
unstrat <- data.frame(
  value = c(rnorm(300, 30, 10), rnorm(300, 70, 10)),
  type = "Unstratified"
)

# Stratified
strat <- data.frame(
  value = c(rnorm(300, 30, 5), rnorm(300, 70, 5)),
  type = "Stratified",
  stratum = rep(c("Low", "High"), each = 300)
)

# Combine for plotting
combined <- bind_rows(
  unstrat,
  strat %>% select(value, type)
)

ggplot(combined, aes(x = value, fill = type)) +
  geom_histogram(bins = 30, alpha = 0.7, position = "identity") +
  facet_wrap(~type, ncol = 1) +
  labs(title = "The Power of Stratification",
       subtitle = "Same data, different sampling approach") +
  theme_minimal() +
  theme(legend.position = "none")
```

---

# Mathematical Foundation

## Variance Reduction

### Without Stratification:
$$V(\bar{y}) = \frac{S^2}{n}$$

Where S² includes both within and between group variation

### With Stratification:
$$V(\bar{y}_{st}) = \sum_{h=1}^{H} W_h^2 \frac{S_h^2}{n_h}$$

Where S²ₕ is only within-stratum variation

**Key: Between-strata variance eliminated!**

---

# Choosing Strata Variables

## Harry's Checklist

### Good Stratification Variables:

✅ **Correlated** with survey variables

✅ **Known** for entire population

✅ **Stable** over time

✅ **Few categories** (4-10 ideal)

✅ **Available** before sampling

---

# Harry's Options

```{r strata_options, echo=FALSE}
options <- data.frame(
  Variable = c("Region", "Urban/Rural", "Province", "Wealth", "Education"),
  Correlation = c("Medium", "High", "Medium", "Very High", "High"),
  Known = c("Yes", "Yes", "Yes", "No", "No"),
  Categories = c(14, 2, 50, 5, 4),
  Feasible = c("Maybe", "Yes", "No", "No", "No")
)

kable(options) %>%
  kable_styling(bootstrap_options = "striped") %>%
  row_spec(2, bold = TRUE, background = "#90EE90")
```

---

# Harry's Decision

## Final Stratification Plan

```{r harry_strata, echo=TRUE}
# Define strata
strata_design <- data.frame(
  Stratum = c("Urban-Capital", "Urban-Other", 
              "Rural-North", "Rural-South"),
  Population = c(500000, 300000, 800000, 600000),
  Households = c(125000, 75000, 200000, 150000)
)

strata_design$Percent <- round(100 * strata_design$Households / 
                               sum(strata_design$Households), 1)

print(strata_design)
```

---

# Allocation Methods

## Three Approaches

### 1. Proportional
Same sampling fraction in each stratum

### 2. Optimal (Neyman)
More sample where variation is higher

### 3. Cost-Optimal
Accounts for different costs per stratum

---

# Proportional Allocation

## The Simple Approach

```{r prop_alloc, echo=TRUE}
total_sample <- 2000

# Proportional to size
strata_design$n_prop <- round(total_sample * 
                              strata_design$Households / 
                              sum(strata_design$Households))

strata_design[, c("Stratum", "Percent", "n_prop")]
```

✅ Self-weighting design

✅ Easy to explain

---

# Neyman Allocation

## Maximizing Precision

```{r neyman_setup, echo=TRUE}
# Add standard deviations (from previous surveys)
strata_design$StdDev <- c(15000, 12000, 8000, 9000)

# Neyman allocation formula
strata_design$N_S <- strata_design$Households * 
                     strata_design$StdDev

strata_design$n_neyman <- round(total_sample * 
                                strata_design$N_S / 
                                sum(strata_design$N_S))
```

---

# Neyman Results

```{r neyman_results, echo=TRUE}
# Compare allocations
comparison <- strata_design[, c("Stratum", "StdDev", 
                                "n_prop", "n_neyman")]
print(comparison)

# More sample where variation is higher!
```

---

# Cost-Optimal Allocation

## When Budgets Matter

```{r cost_optimal, echo=TRUE}
# Add cost per interview
strata_design$Cost <- c(20, 25, 40, 35)  # USD

# Cost-optimal formula
strata_design$N_S_C <- strata_design$Households * 
                       strata_design$StdDev / 
                       sqrt(strata_design$Cost)

strata_design$n_cost <- round(total_sample * 
                              strata_design$N_S_C / 
                              sum(strata_design$N_S_C))
```

---

# Comparing All Three

```{r compare_alloc, echo=TRUE}
allocation_summary <- strata_design[, c("Stratum", "n_prop", 
                                        "n_neyman", "n_cost")]
print(allocation_summary)

# Calculate total costs
cost_prop <- sum(strata_design$n_prop * strata_design$Cost)
cost_neyman <- sum(strata_design$n_neyman * strata_design$Cost)
cost_optimal <- sum(strata_design$n_cost * strata_design$Cost)

cat("\nTotal costs:\n")
cat("Proportional: $", cost_prop, "\n")
cat("Neyman: $", cost_neyman, "\n")
cat("Cost-optimal: $", cost_optimal, "\n")
```

---

# Harry's Final Choice

## Practical Considerations

```{r final_allocation, echo=TRUE}
# Harry modifies Neyman for practical reasons
strata_design$n_final <- strata_design$n_neyman

# Ensure minimum of 100 per stratum
strata_design$n_final[strata_design$n_final < 100] <- 100

# Adjust to maintain total
adjustment <- total_sample / sum(strata_design$n_final)
strata_design$n_final <- round(strata_design$n_final * adjustment)

strata_design[, c("Stratum", "n_neyman", "n_final")]
```

---

# Implementing Stratification

📝 **EXERCISE:** Design Your Strata

**File:** `03-Scripts/Day1_Exercise_03_Stratification.R`

Your task:
1. Define strata for your country
2. Allocate sample of 2000
3. Compare methods
4. Calculate expected precision

---

# Common Stratification Errors

## ⚠️ Harry's Near-Misses

### 1. Too Many Strata

```{r too_many, echo=TRUE}
# Bad idea: 14 regions × 2 urban/rural × 3 income
n_strata <- 14 * 2 * 3  # 84 strata!
avg_per_stratum <- 2000 / n_strata
cat("Average per stratum:", round(avg_per_stratum), "\n")
cat("Too small for analysis!")
```

---

# More Stratification Pitfalls

### 2. Stratifying After Sampling

❌ Doesn't give stratification benefits

### 3. Unknown Population Counts

❌ Can't do proper allocation

### 4. Unstable Variables

❌ Strata change over time

### 5. Empty Strata

❌ No units in some combinations

---

# Post-Stratification

## Fixing Problems Later

```{r post_strat, echo=TRUE}
# After data collection
collected <- data.frame(
  age_group = sample(c("18-35", "36-50", "51+"), 
                    1000, replace = TRUE)
)

# Sample distribution
sample_dist <- table(collected$age_group) / 1000

# Known population
pop_dist <- c("18-35" = 0.40, "36-50" = 0.35, "51+" = 0.25)

# Adjustment factors
adj_factors <- pop_dist / sample_dist
print(round(adj_factors, 2))
```

---

# Break Time!

## ☕ 15 Minutes

### During Break:
1. Get coffee/tea
2. Discuss with neighbors:
   - What stratification does your office use?
   - Any problems encountered?

### After Break:
**Cluster Sampling - The Money Saver**

---
class: inverse, center, middle

# Part 3: Cluster Sampling
## Trading Precision for Cost

---

# The Reality Check

## Harry's Budget Crisis

```{r budget_reality, echo=FALSE}
budget_breakdown <- data.frame(
  Method = c("SRS", "Stratified", "Cluster"),
  Interviews = c(2000, 2000, 2000),
  Locations = c(2000, 2000, 100),
  Travel_Cost = c(40000, 40000, 2000),
  Total_Cost = c(80000, 80000, 42000)
)

kable(budget_breakdown, 
      format.args = list(big.mark = ","),
      col.names = c("Method", "Interviews", "Locations", 
                   "Travel Cost ($)", "Total Cost ($)")) %>%
  kable_styling(bootstrap_options = "striped") %>%
  row_spec(3, bold = TRUE, background = "#90EE90")
```

---

# What Are Clusters?

## Natural Groupings

### Definition:
Groups of elementary units (households)

### Examples:

.pull-left[
**Rural:**
- Villages
- Enumeration areas
- Administrative units
]

.pull-right[
**Urban:**
- City blocks
- Apartment buildings
- Neighborhoods
]

---

# Cluster Sampling Visualization

```{r cluster_viz, echo=FALSE, fig.height=5}
set.seed(2025)
# Create clustered data
clusters <- data.frame(
  cluster_id = rep(1:5, each = 20),
  x = c(rnorm(20, 10, 1), rnorm(20, 30, 1), rnorm(20, 50, 1),
        rnorm(20, 20, 1), rnorm(20, 40, 1)),
  y = c(rnorm(20, 10, 1), rnorm(20, 30, 1), rnorm(20, 20, 1),
        rnorm(20, 40, 1), rnorm(20, 50, 1)),
  selected = rep(c(TRUE, FALSE, TRUE, FALSE, TRUE), each = 20)
)

ggplot(clusters, aes(x = x, y = y, color = factor(cluster_id),
                     shape = selected)) +
  geom_point(size = 3, alpha = 0.8) +
  scale_shape_manual(values = c(1, 19), 
                    labels = c("Not selected", "Selected")) +
  labs(title = "Cluster Sampling: Select Groups, Not Individuals",
       subtitle = "3 clusters selected from 5",
       color = "Cluster", shape = "Status") +
  theme_minimal()
```

---

# The Cost Advantage

## Travel Efficiency

```{r cost_comparison, echo=FALSE, fig.height=4}
# Show travel patterns
par(mfrow = c(1, 2))

# SRS travel
set.seed(2025)
plot(runif(30), runif(30), pch = 19, col = "#A23B72",
     main = "SRS: Scattered Travel", xlab = "", ylab = "")
for(i in 2:30) {
  lines(runif(2), runif(2), col = gray(0.5, 0.3))
}

# Cluster travel  
plot(c(rnorm(10, 0.2, 0.05), rnorm(10, 0.7, 0.05), rnorm(10, 0.5, 0.05)),
     c(rnorm(10, 0.2, 0.05), rnorm(10, 0.7, 0.05), rnorm(10, 0.5, 0.05)),
     pch = 19, col = "#2E86AB",
     main = "Cluster: Efficient Travel", xlab = "", ylab = "")
```

---

# The Statistical Price

## Intra-cluster Correlation (ICC)

### The Problem:
**Units within clusters are similar!**

### Result:
Less information per unit than SRS

### Measured by ICC (ρ):
- ρ = 0: No similarity (rare!)
- ρ = 0.01-0.05: Weak
- ρ = 0.05-0.15: Moderate
- ρ > 0.15: Strong

---

# ICC Visualization

```{r icc_viz, echo=FALSE, fig.height=5}
set.seed(2025)
# High ICC
high_icc <- data.frame(
  cluster = factor(rep(1:3, each = 20)),
  value = c(rnorm(20, 30, 2), rnorm(20, 50, 2), rnorm(20, 70, 2)),
  ICC = "High ICC (ρ = 0.20)"
)

# Low ICC
low_icc <- data.frame(
  cluster = factor(rep(1:3, each = 20)),
  value = c(rnorm(20, 50, 10), rnorm(20, 50, 10), rnorm(20, 50, 10)),
  ICC = "Low ICC (ρ = 0.02)"
)

combined <- rbind(high_icc, low_icc)

ggplot(combined, aes(x = cluster, y = value, color = cluster)) +
  geom_point(position = position_jitter(width = 0.2), alpha = 0.6) +
  facet_wrap(~ICC) +
  labs(title = "Impact of Intra-cluster Correlation",
       subtitle = "High ICC means less information per unit") +
  theme_minimal() +
  theme(legend.position = "none")
```

---

# Design Effect (DEFF)

## The Key Formula

### Design Effect:
$$DEFF = 1 + (m - 1) \times \rho$$

Where:
- m = cluster size
- ρ = intra-cluster correlation

### Example:
- m = 20 households per cluster
- ρ = 0.05
- DEFF = 1 + (20-1) × 0.05 = 1.95

**Meaning:** Sample of 2000 gives information of only 1026!

---

# Calculate Your DEFF

```{r deff_calc, echo=TRUE}
# Function to calculate design effect
calc_deff <- function(cluster_size, icc) {
  deff <- 1 + (cluster_size - 1) * icc
  return(deff)
}

# Harry's scenarios
scenarios <- expand.grid(
  cluster_size = c(10, 20, 30),
  icc = c(0.02, 0.05, 0.10)
)

scenarios$deff <- calc_deff(scenarios$cluster_size, 
                           scenarios$icc)
scenarios$eff_n <- round(2000 / scenarios$deff)

print(scenarios)
```

---

# Harry's Dilemma

## Balancing Cost and Precision

```{r harry_balance, echo=TRUE}
# Option 1: Large clusters (cheap but less precise)
opt1 <- list(
  clusters = 50,
  size = 40,
  cost = 50 * 500 + 2000 * 10,  # Travel + interview
  deff = calc_deff(40, 0.05),
  eff_n = 2000 / calc_deff(40, 0.05)
)

# Option 2: Small clusters (expensive but precise)
opt2 <- list(
  clusters = 200,
  size = 10,
  cost = 200 * 500 + 2000 * 10,
  deff = calc_deff(10, 0.05),
  eff_n = 2000 / calc_deff(10, 0.05)
)

cat("Option 1: Cost=$", opt1$cost, " Eff n=", round(opt1$eff_n), "\n")
cat("Option 2: Cost=$", opt2$cost, " Eff n=", round(opt2$eff_n), "\n")
```

---

# Typical ICC Values

## From Real Surveys

```{r icc_table, echo=FALSE}
icc_values <- data.frame(
  Variable = c("Age", "Household size", "Education years", 
              "Income", "Ethnicity", "Religion"),
  Typical_ICC = c(0.02, 0.03, 0.08, 0.10, 0.25, 0.30),
  Clustering = c("Low", "Low", "Moderate", "Moderate", "High", "High")
)

kable(icc_values, col.names = c("Variable", "Typical ρ", "Clustering")) %>%
  kable_styling(bootstrap_options = "striped") %>%
  column_spec(3, bold = TRUE,
             color = ifelse(icc_values$Clustering == "High", "red",
                          ifelse(icc_values$Clustering == "Moderate", 
                                "orange", "green")))
```

---

# Choosing Cluster Size

## Harry's Decision Framework

```{r cluster_size, echo=TRUE}
# Optimal cluster size (simplified)
optimal_cluster_size <- function(travel_cost, 
                                 interview_cost, 
                                 icc) {
  # Approximate formula
  m_opt <- sqrt(travel_cost / (interview_cost * icc))
  return(round(m_opt))
}

# Harry's parameters
travel <- 500  # Cost to reach cluster
interview <- 10  # Cost per interview
icc <- 0.05

optimal_m <- optimal_cluster_size(travel, interview, icc)
cat("Optimal cluster size:", optimal_m, "households")
```

---

# Implementing Clusters

📝 **EXERCISE:** Design Clusters

**File:** `03-Scripts/Day1_Exercise_04_Clusters.R`

Your task:
1. Choose cluster size
2. Calculate number of clusters
3. Estimate design effect
4. Compare costs

---

# Two-Stage Sampling

## Combining Strategies

### Stage 1: Select clusters (PSUs)
### Stage 2: Select households within clusters

```{r two_stage, echo=TRUE}
# Stage 1: Select 100 villages from 2000
n_psu <- 100
total_villages <- 2000

# Stage 2: Select 20 households from each
hh_per_village <- 20
total_sample <- n_psu * hh_per_village

cat("Total sample:", total_sample, "households\n")
cat("From:", n_psu, "villages")
```

---
class: inverse, center, middle

# Part 4: Multi-Stage Designs
## Putting It All Together

---

# Why Multi-Stage?

## The Real World

### Single stage rarely works because:

❌ No complete household list

❌ Too expensive to list all

❌ Population too spread out

❌ Administrative constraints

### Solution: Sample in stages!

---

# Multi-Stage Framework

```{r multistage_framework, echo=FALSE, fig.height=5}
# Create hierarchical diagram
library(DiagrammeR)

grViz("
digraph multistage {
  graph [rankdir = TB]
  
  node [shape = box, style = filled]
  
  Country [fillcolor = lightblue, label = 'Country\n(Population)']
  Regions [fillcolor = lightgreen, label = 'Regions\n(Strata)']
  Districts [fillcolor = lightyellow, label = 'Districts\n(PSUs)']
  EAs [fillcolor = lightcoral, label = 'Enumeration Areas\n(SSUs)']
  HH [fillcolor = lightgray, label = 'Households\n(Final units)']
  
  Country -> Regions [label = 'Stratify']
  Regions -> Districts [label = 'Sample']
  Districts -> EAs [label = 'Sample'] 
  EAs -> HH [label = 'Sample']
}
")
```

---

# Harry's Multi-Stage Design

## Three Stages

```{r harry_multistage, echo=TRUE}
# Harry's design structure
design_structure <- data.frame(
  Stage = c("1: Districts", "2: EAs", "3: Households"),
  Frame_Units = c(200, 50, 400),
  Selected = c(50, 4, 10),
  Total_Sample = c(50, 200, 2000)
)

print(design_structure)

# Selection probabilities
p1 <- 50/200    # Districts
p2 <- 4/50      # EAs per district
p3 <- 10/400    # HH per EA

overall_prob <- p1 * p2 * p3
cat("\nOverall selection probability:", round(overall_prob, 6))
```

---

# Probability Proportional to Size (PPS)

## Why Equal Probability Fails

```{r pps_need, echo=FALSE, fig.height=4}
# Show problem with equal probability
clusters <- data.frame(
  id = 1:10,
  size = c(50, 500, 100, 1000, 200, 80, 300, 150, 600, 400)
)

ggplot(clusters, aes(x = factor(id), y = size)) +
  geom_bar(stat = "identity", fill = "#2E86AB") +
  geom_hline(yintercept = mean(clusters$size), 
            linetype = "dashed", color = "red") +
  labs(title = "Cluster Sizes Vary Dramatically",
       subtitle = "Equal probability selection gives unequal HH probabilities",
       x = "Cluster ID", y = "Number of Households") +
  theme_minimal()
```

---

# PPS Sampling

## The Solution

### Select clusters with probability proportional to size

```{r pps_demo, echo=TRUE}
# Create frame with sizes
psu_frame <- data.frame(
  psu_id = 1:100,
  households = round(runif(100, 100, 2000))
)

# Calculate selection probabilities
psu_frame$prob <- psu_frame$households / sum(psu_frame$households)

# Select with PPS
n_psu <- 20
set.seed(2025)
selected_psu <- sample(psu_frame$psu_id, 
                      size = n_psu,
                      prob = psu_frame$prob)

cat("Selected PSUs:", head(selected_psu))
```

---

# PPS Advantages

## Self-Weighting Design

```{r pps_advantage, echo=TRUE}
# With PPS at stage 1
# Fixed number at stage 2
# Result: Equal probability for all HH

# Example calculation
psu_sizes <- psu_frame$households[selected_psu]
stage1_prob <- psu_sizes / sum(psu_frame$households)
stage2_take <- 20  # Fixed

# Final probability
final_prob <- stage1_prob * (stage2_take / psu_sizes)
cat("Final probabilities:\n")
print(unique(round(final_prob, 6)))
# All same!
```

---

# Systematic PPS

## Practical Implementation

```{r sys_pps, echo=TRUE}
# Systematic PPS selection
systematic_pps <- function(frame, n) {
  N <- sum(frame$households)
  interval <- N / n
  
  # Random start
  start <- runif(1, 0, interval)
  
  # Cumulative sizes
  frame$cum_size <- cumsum(frame$households)
  
  # Selection points
  points <- start + (0:(n-1)) * interval
  
  # Find selected units
  selected <- sapply(points, function(p) {
    which(frame$cum_size >= p)[1]
  })
  
  return(unique(selected))
}

selected_sys_pps <- systematic_pps(psu_frame, 20)
length(selected_sys_pps)
```

---

# Complete Selection Process

📝 **EXERCISE:** Multi-Stage Selection

**File:** `03-Scripts/Day1_Exercise_05_Multistage.R`

Practice:
1. Stage 1: Select PSUs with PPS
2. Stage 2: Select SSUs
3. Stage 3: Select households
4. Calculate weights

---

# Lunch Break!

## 🍽️ 60 Minutes

### After Lunch:
- Total Survey Error
- Variance Estimation  
- Weight Calculations
- Quality Framework

### Homework Preview:
Design a survey for your country!

---
class: inverse, center, middle

# Part 5: Total Survey Error
## Beyond Sampling Error

---

# The Bigger Picture

## Total Survey Error Framework

```{r tse_framework, echo=FALSE, fig.height=5}
# TSE components diagram
tse_components <- data.frame(
  Component = c("Coverage", "Sampling", "Nonresponse", 
               "Measurement", "Processing"),
  Representation = c(4, 3, 4, 1, 1),
  Measurement = c(1, 0, 1, 5, 3)
)

ggplot(tse_components, aes(x = Component)) +
  geom_bar(aes(y = Representation), stat = "identity", 
          fill = "#2E86AB", alpha = 0.7) +
  geom_bar(aes(y = -Measurement), stat = "identity",
          fill = "#A23B72", alpha = 0.7) +
  coord_flip() +
  labs(title = "Total Survey Error Components",
       subtitle = "Representation errors (blue) vs Measurement errors (pink)",
       y = "Error Magnitude", x = "") +
  theme_minimal() +
  geom_hline(yintercept = 0)
```

---

# Coverage Error

## Who's Missing?

### Under-coverage:
- Homeless populations
- New constructions
- Gated communities
- Remote areas

### Over-coverage:
- Demolished buildings
- Deceased persons
- Emigrants
- Duplicates

---

# Harry's Coverage Check

```{r coverage_check, echo=TRUE}
# Estimate coverage
total_households <- 550000
frame_households <- 520000

coverage_rate <- frame_households / total_households * 100

cat("Coverage rate:", round(coverage_rate, 1), "%\n")
cat("Missing households:", total_households - frame_households)

# Types of missing units
missing_types <- c(
  "New constructions" = 15000,
  "Informal settlements" = 10000,
  "Remote areas" = 5000
)

print(missing_types)
```

---

# Non-Response Error

## The Silent Bias

```{r nonresponse_viz, echo=FALSE, fig.height=4}
# Response rates by group
response_data <- data.frame(
  Group = c("Urban-High", "Urban-Low", "Rural-North", "Rural-South"),
  Response_Rate = c(65, 72, 78, 82)
)

ggplot(response_data, aes(x = Group, y = Response_Rate, fill = Group)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 70, linetype = "dashed", color = "red") +
  labs(title = "Differential Non-Response",
       subtitle = "Response rates vary by stratum",
       y = "Response Rate (%)") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("#2E86AB", "#52B788", "#F18F01", "#A23B72"))
```

---

# Types of Non-Response

```{r nonresponse_types, echo=FALSE}
nr_types <- data.frame(
  Type = c("Refusal", "Not at home", "Unable", "Other"),
  Unit_NR = c(15, 8, 3, 2),
  Item_NR = c(5, 0, 2, 1)
)

kable(nr_types, col.names = c("Type", "Unit (%)", "Item (%)")) %>%
  kable_styling(bootstrap_options = "striped")
```

### Strategies:
- Multiple attempts
- Different times
- Incentives
- Shorter questionnaire

---

# Measurement Error

## Getting Wrong Answers

### Sources:

.pull-left[
**Respondent:**
- Memory errors
- Social desirability
- Misunderstanding
- Deliberate lies
]

.pull-right[
**Interviewer:**
- Leading questions
- Recording errors
- Rapport effects
- Training gaps
]

---

# Measurement Error Example

```{r measurement_error, echo=TRUE}
# Income reporting bias
true_income <- 50000
reported <- rnorm(1000, 
                 mean = true_income * 0.85,  # Under-reporting
                 sd = true_income * 0.15)

bias <- mean(reported) - true_income
rmse <- sqrt(mean((reported - true_income)^2))

cat("True income: $", true_income, "\n")
cat("Average reported: $", round(mean(reported)), "\n")
cat("Bias: $", round(bias), "\n")
cat("RMSE: $", round(rmse))
```

---

# Processing Error

## Data Entry & Coding

```{r processing_error, echo=TRUE}
# Simulate data entry errors
n_records <- 2000
error_rate <- 0.02  # 2% error rate

n_errors <- rbinom(1, n_records, error_rate)

cat("Records:", n_records, "\n")
cat("Expected errors:", n_records * error_rate, "\n")
cat("Actual errors:", n_errors, "\n")

# Impact on estimates
true_mean <- 45000
error_impact <- rnorm(n_errors, 0, 5000)
adjusted_mean <- true_mean + mean(error_impact) * error_rate

cat("\nImpact on mean: $", round(adjusted_mean - true_mean))
```

---

# Reducing TSE

## Harry's Strategy

```{r tse_reduction, echo=FALSE}
strategies <- data.frame(
  Error_Type = c("Coverage", "Sampling", "Nonresponse", 
                "Measurement", "Processing"),
  Strategy = c("Update frame", "Optimize design", "Follow-up",
              "Training", "Validation"),
  Cost = c("$$", "$", "$$$", "$$", "$"),
  Impact = c("High", "High", "Medium", "High", "Medium")
)

kable(strategies) %>%
  kable_styling(bootstrap_options = "striped")
```

---
class: inverse, center, middle

# Part 6: Variance Estimation
## Getting the Uncertainty Right

---

# Why Special Methods?

## Complex Designs Break Simple Formulas

### Simple Random Sample:
$$SE(\bar{y}) = \sqrt{\frac{s^2}{n}}$$

### Complex Design:
❌ Can't use simple formula
✅ Need special methods

---

# Methods Overview

```{r variance_methods, echo=FALSE}
methods <- data.frame(
  Method = c("Taylor Series", "Jackknife", "Bootstrap", "BRR"),
  Best_For = c("Most statistics", "General", "Any design", "2 PSU/stratum"),
  Complexity = c("Medium", "Low", "High", "Medium"),
  Speed = c("Fast", "Slow", "Very Slow", "Fast")
)

kable(methods) %>%
  kable_styling(bootstrap_options = "striped")
```

---

# Taylor Series Linearization

## The Workhorse Method

```{r taylor_demo, echo=TRUE}
# Using survey package
library(survey)

# Create design object
design <- svydesign(
  ids = ~cluster,
  strata = ~stratum,
  weights = ~weight,
  data = sim_data
)

# Automatic variance estimation
mean_income <- svymean(~income, design)
print(mean_income)

# Extract SE
SE(mean_income)
```

---

# Jackknife Method

## Leave-One-Out

```{r jackknife_demo, echo=TRUE}
# Convert to jackknife
jk_design <- as.svrepdesign(design, type = "JKn")

# Compare results  
jk_mean <- svymean(~income, jk_design)

# Results
data.frame(
  Method = c("Taylor", "Jackknife"),
  Estimate = c(coef(mean_income), coef(jk_mean)),
  SE = c(SE(mean_income), SE(jk_mean))
)
```

---

# Bootstrap Method

## Resampling Approach

```{r bootstrap_demo, echo=TRUE}
# Create bootstrap replicates
boot_design <- as.svrepdesign(design, 
                              type = "bootstrap",
                              replicates = 100)

# Calculate with bootstrap
boot_mean <- svymean(~income, boot_design)

# Confidence interval
confint(boot_mean, level = 0.95)
```

---

# BRR Method

## Balanced Repeated Replication

```{r brr_setup, echo=TRUE}
# BRR requires exactly 2 PSUs per stratum
# Create appropriate design
brr_data <- data.frame(
  psu = rep(1:8, each = 50),
  stratum = rep(1:4, each = 100),
  weight = 100,
  value = rnorm(400, 50000, 10000)
)

brr_design <- svydesign(ids = ~psu, strata = ~stratum,
                        weights = ~weight, data = brr_data)

# Convert to BRR
brr_rep <- as.svrepdesign(brr_design, type = "BRR")
```

---

# Comparing Methods

📝 **EXERCISE:** Variance Comparison

**File:** `03-Scripts/Day1_Exercise_06_Variance.R`

Compare:
1. Taylor series
2. Jackknife
3. Bootstrap
4. See differences

---

# Finite Population Correction

## When Sample is Large

```{r fpc_demo, echo=TRUE}
# Include FPC
N_strata <- c(Urban = 100000, Rural = 200000)
n_strata <- c(Urban = 10000, Rural = 15000)

fpc_factor <- 1 - n_strata/N_strata
print(fpc_factor)

# Large sampling fraction!
# FPC reduces variance significantly
```

---
class: inverse, center, middle

# Part 7: Weight Calculations
## From Sample to Population

---

# Why Weights?

## Making Sample Representative

### Weights account for:

1. **Unequal selection probabilities**
2. **Non-response**
3. **Coverage issues**
4. **Post-stratification**

### Result:
Sample represents population correctly

---

# Base Weights

## Inverse of Selection Probability

```{r base_weights, echo=TRUE}
# Single stage
N_stratum <- 100000
n_stratum <- 500
base_weight <- N_stratum / n_stratum
cat("Base weight:", base_weight, "\n")

# Multi-stage
p_stage1 <- 50/200   # PSUs selected
p_stage2 <- 20/500   # HH per PSU
overall_prob <- p_stage1 * p_stage2
multi_weight <- 1 / overall_prob

cat("Multi-stage weight:", multi_weight)
```

---

# Non-Response Adjustment

## Correcting for Missing Units

```{r nr_adjustment, echo=TRUE}
# Response rates by stratum
response_rates <- c(
  "Urban-High" = 0.65,
  "Urban-Low" = 0.72,
  "Rural-North" = 0.78,
  "Rural-South" = 0.82
)

# Adjust base weights
base_weight <- 200
adjusted_weights <- base_weight / response_rates

print(round(adjusted_weights))
```

---

# Post-Stratification

## Matching Known Totals

```{r post_strat_weights, echo=TRUE}
# Sample distribution
sample_dist <- c(Young = 300, Middle = 400, Old = 300)

# Population distribution
pop_dist <- c(Young = 400000, Middle = 450000, Old = 150000)

# Calculate factors
ps_factors <- (pop_dist/sum(pop_dist)) / 
              (sample_dist/sum(sample_dist))

print(round(ps_factors, 2))
```

---

# Weight Trimming

## Handling Extremes

```{r weight_trimming, echo=TRUE}
# Simulate weights
weights <- c(rnorm(980, 200, 30), 
            c(50, 65, 420, 380))  # Extremes

# Before trimming
summary(weights)

# Trim at 5th and 95th percentiles
bounds <- quantile(weights, c(0.05, 0.95))
weights_trimmed <- pmin(pmax(weights, bounds[1]), bounds[2])

# After trimming
summary(weights_trimmed)
```

---

# Calibration

## Advanced Weight Adjustment

```{r calibration_demo, echo=TRUE, eval=FALSE}
# Calibrate to known totals
library(survey)

# Known population totals
pop.totals <- list(
  ~factor(stratum) + factor(age_group),
  c(100000, 200000, 150000, 180000, 170000)
)

# Calibrate
calibrated_design <- calibrate(
  design,
  formula = ~factor(stratum) + factor(age_group),
  population = pop.totals
)
```

---

# Weight Diagnostics

📝 **EXERCISE:** Check Your Weights

**File:** `03-Scripts/Day1_Exercise_07_Weights.R`

Tasks:
1. Calculate base weights
2. Adjust for non-response
3. Check distribution
4. Identify problems

---
class: inverse, center, middle

# Part 8: Quality Framework
## Getting It Right

---

# Quality Dimensions

```{r quality_dimensions, echo=FALSE, fig.height=5}
quality <- data.frame(
  Dimension = c("Accuracy", "Timeliness", "Accessibility",
               "Coherence", "Relevance", "Completeness"),
  Score = c(85, 90, 75, 80, 95, 88)
)

ggplot(quality, aes(x = reorder(Dimension, Score), y = Score, fill = Dimension)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Survey Quality Dimensions",
       subtitle = "Harry's targets",
       x = "", y = "Score (%)") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_manual(values = rep(c("#2E86AB", "#52B788", "#F18F01"), 2))
```

---

# Pre-Field Quality

## Getting Ready

### Frame Quality:
☑ Coverage assessment
☑ Update verification
☑ Duplicate removal

### Instrument Testing:
☑ Cognitive interviews
☑ Pilot test
☑ Translation check

---

# Field Monitoring

## Real-Time Quality Control

```{r monitoring_dashboard, echo=FALSE, fig.height=4}
set.seed(2025)
days <- 1:20
interviews <- cumsum(rpois(20, 80))
response_rate <- 75 + rnorm(20, 0, 3)

par(mfrow = c(1, 2))
plot(days, interviews, type = "l", lwd = 2, col = "#2E86AB",
     main = "Cumulative Interviews", xlab = "Day", ylab = "Count")
abline(h = 1600, lty = 2, col = "red")

plot(days, response_rate, type = "l", lwd = 2, col = "#52B788",
     main = "Daily Response Rate", xlab = "Day", ylab = "Percent",
     ylim = c(65, 85))
abline(h = 70, lty = 2, col = "red")
```

---

# Data Validation

## Automated Checks

```{r validation_rules, echo=TRUE}
# Validation function
validate_data <- function(data) {
  checks <- list(
    age_range = all(data$age >= 0 & data$age <= 120),
    hh_size = all(data$hh_size >= 1 & data$hh_size <= 20),
    logical = all(data$expense <= data$income * 2)
  )
  return(checks)
}

# Test validation
test_data <- data.frame(
  age = c(25, 45, 150, 30),  # One invalid
  hh_size = c(4, 3, 5, 2),
  income = c(1000, 2000, 1500, 3000),
  expense = c(900, 1800, 2000, 2800)
)

validate_data(test_data)
```

---

# Response Rate Calculation

## AAPOR Standards

```{r response_rates, echo=TRUE}
# Response rate components
completed <- 1150
partial <- 85
refused <- 180
non_contact <- 125
unknown_elig <- 60

# AAPOR RR2
interviews <- completed + partial
eligible <- interviews + refused + non_contact
RR2 <- interviews / (eligible + unknown_elig)

cat("Response Rate (RR2):", round(RR2 * 100, 1), "%")
```

---

# Documentation Standards

## What Harry Documents

### Technical:
- Sampling design report
- Operations manual
- Quality control logs
- Weight calculations

### Public:
- Methodology report
- Quality indicators
- Response rates
- Standard errors

---

# Reporting Results

## With Proper Uncertainty

```{r reporting, echo=TRUE}
# Calculate CV for reporting
result <- svyby(~income, ~stratum, design, svymean)
result$cv <- cv(result) * 100

# Reporting table
report <- data.frame(
  Stratum = result$stratum,
  Estimate = round(result$income, 0),
  SE = round(SE(result), 0),
  CV = round(result$cv, 1)
)

print(report)
```

---
class: inverse, center, middle

# Part 9: Advanced Topics
## Going Deeper

---

# Small Area Estimation

## When Direct Estimates Fail

```{r sae_demo, echo=TRUE}
# Small domain problem
small_domain_n <- 30
direct_se <- 5000
direct_cv <- direct_se / 45000 * 100

cat("Direct estimate CV:", round(direct_cv, 1), "%\n")
cat("Too large! Need SAE methods\n")

# Borrow strength from related areas
related_means <- c(44000, 46000, 45500, 43500)
composite_est <- 0.3 * 45000 + 0.7 * mean(related_means)
cat("SAE estimate:", round(composite_est))
```

---

# Two-Phase Sampling

## Harry's Creative Solution

```{r two_phase, echo=TRUE}
# Phase 1: Large cheap sample
phase1_n <- 5000
phase1_cost <- phase1_n * 5  # Phone screening

# Phase 2: Detailed expensive sample  
phase2_rate <- 0.2
phase2_n <- phase1_n * phase2_rate
phase2_cost <- phase2_n * 50  # Full interview

total_cost <- phase1_cost + phase2_cost
cat("Total cost: $", total_cost, "\n")
cat("Detailed interviews:", phase2_n)
```

---

# Adaptive Design

## Real-Time Adjustments

```{r adaptive, echo=TRUE}
# Monitor and adapt
monitor_week <- function(week, rate) {
  if(rate < 0.60) {
    action <- "Increase incentive"
  } else if(rate < 0.70) {
    action <- "Add reminder"
  } else {
    action <- "Continue"
  }
  return(action)
}

# Weekly monitoring
weeks <- 1:4
rates <- c(0.58, 0.65, 0.72, 0.74)

for(i in 1:4) {
  cat("Week", i, ":", rates[i], "-", 
      monitor_week(i, rates[i]), "\n")
}
```

---

# Machine Learning Applications

## Predicting Non-Response

```{r ml_demo, echo=TRUE, eval=FALSE}
library(randomForest)

# Predict non-response
train_data <- data.frame(
  responded = rbinom(500, 1, 0.7),
  age_group = sample(1:4, 500, replace = TRUE),
  urban = rbinom(500, 1, 0.4),
  prev_response = rbinom(500, 1, 0.6)
)

# Train model
rf_model <- randomForest(
  factor(responded) ~ .,
  data = train_data,
  ntree = 100
)

# Variable importance
importance(rf_model)
```

---

# Network Sampling

## Hidden Populations

```{r network, echo=TRUE}
# Respondent-driven sampling
simulate_rds <- function(seeds = 5, waves = 4) {
  sample_size <- seeds
  
  for(wave in 1:waves) {
    new_recruits <- sample_size * 3 * 0.7^wave
    sample_size <- sample_size + new_recruits
  }
  
  return(round(sample_size))
}

final_n <- simulate_rds(seeds = 10, waves = 5)
cat("Final sample from 10 seeds:", final_n)
```

---

# GIS Integration

## Spatial Sampling

```{r gis_concept, echo=TRUE, eval=FALSE}
# Spatial sampling framework
library(sf)

# Load boundary file
# boundaries <- st_read("districts.shp")

# Create spatial frame
# boundaries$area <- st_area(boundaries)
# boundaries$density <- boundaries$pop / boundaries$area

# Stratify by density
# boundaries$strata <- cut(boundaries$density,
#                          quantile(boundaries$density),
#                          labels = c("Low", "Med-Low", 
#                                   "Med-High", "High"))

# Sample with PPS to population
# selected <- sample(boundaries$id, 50,
#                   prob = boundaries$pop)
```

---
class: inverse, center, middle

# Part 10: Practical Exercises
## Hands-On Learning

---

# Exercise 1: Complete Design

## Your Country Survey

📝 **MAJOR EXERCISE:** Design Complete Survey

**File:** `03-Scripts/Day1_Exercise_08_Complete.R`

### Requirements:
1. Population: Your country
2. Sample size: 3,000 households
3. Domains: Urban/Rural + 4 regions
4. Budget: Limited
5. Timeline: 3 months

### Deliverables:
- Stratification plan
- Sample allocation
- Cluster design
- Expected precision

---

# Work in Groups

## 30 Minutes

### Steps:

1. **Define population** and frame
2. **Choose stratification** variables
3. **Decide cluster size** (consider ICC)
4. **Allocate sample** to strata
5. **Calculate design effects**
6. **Estimate costs**
7. **Document decisions**

### Present your design!

---

# Group Presentations

## Share Your Design

Each group (5 minutes):

1. **Country context**
2. **Design choices**
3. **Trade-offs made**
4. **Expected precision**
5. **Main challenges**

### Peer feedback encouraged!

---

# Common Design Issues

## Learn From Mistakes

### Issue 1: Over-stratification
Too many strata = tiny samples

### Issue 2: Large clusters
Cheap but poor precision

### Issue 3: Ignoring ICC
Underestimating variance

### Issue 4: No cost consideration
Unrealistic design

---

# Harry's Final Design

## What He Chose

```{r harry_final, echo=FALSE}
final_design <- data.frame(
  Component = c("Stratification", "Allocation", "Clusters",
               "Cluster size", "Total PSUs", "Total sample"),
  Choice = c("Urban/Rural × Region", "Modified Neyman",
            "Villages/Blocks", "20 households", "100", "2000"),
  Rationale = c("Known, stable", "Precision + politics",
               "Natural units", "Cost-precision balance",
               "Budget constraint", "Power calculation")
)

kable(final_design) %>%
  kable_styling(bootstrap_options = "striped", font_size = 11)
```

---

# Software Comparison

## R vs STATA vs Python

```{r software_compare, echo=FALSE}
software <- data.frame(
  Feature = c("Survey package", "Graphics", "Speed", "Learning", "Cost"),
  R = c("Excellent", "Excellent", "Good", "Moderate", "Free"),
  STATA = c("Excellent", "Good", "Very Good", "Easy", "$$"),
  Python = c("Growing", "Excellent", "Excellent", "Steep", "Free")
)

kable(software) %>%
  kable_styling(bootstrap_options = "striped") %>%
  column_spec(2, background = "#E8F5E9") %>%
  column_spec(3, background = "#FFF3E0") %>%
  column_spec(4, background = "#E3F2FD")
```

### Harry uses: **R for analysis, Python for automation**

---

# R Package Ecosystem

## Essential Packages

```{r packages, echo=TRUE, eval=FALSE}
# Core survey analysis
library(survey)      # Main package
library(srvyr)       # Tidyverse style
library(PracTools)   # Sample size

# Data manipulation
library(tidyverse)   # Everything
library(data.table)  # Large data

# Visualization
library(ggplot2)     # Static plots
library(plotly)      # Interactive

# Reporting
library(knitr)       # Documents
library(gt)          # Tables
```

---

# Advanced R Techniques

## Custom Functions

```{r custom_functions, echo=TRUE}
# Create your toolkit
my_survey_tools <- list(
  
  # Quick CV calculation
  quick_cv = function(design, variable) {
    est <- svymean(as.formula(paste("~", variable)), design)
    cv(est) * 100
  },
  
  # Design effect estimation
  estimate_deff = function(m, icc) {
    1 + (m - 1) * icc
  },
  
  # Required sample size
  sample_size = function(cv = 0.05, deff = 1.5) {
    n_srs <- (1.96 / (cv * 2))^2
    n_srs * deff
  }
)

# Use your tools
my_survey_tools$sample_size(cv = 0.03, deff = 2)
```

---

# Troubleshooting Guide

## Common Problems

### Problem 1: "Lonely PSU"
```{r lonely_psu, echo=TRUE}
options(survey.lonely.psu = "adjust")
```

### Problem 2: Missing weights
```{r missing_weights, echo=TRUE, eval=FALSE}
# Check for missing
sum(is.na(data$weights))
# Impute if needed
```

### Problem 3: Extreme weights
```{r extreme_weights, echo=TRUE, eval=FALSE}
# Identify outliers
boxplot(data$weights)
# Trim if necessary
```

---

# Best Practices

## Harry's Lessons Learned

### Design Phase:
✅ Keep it simple as possible

✅ Document every decision

✅ Consider operations early

### Implementation:
✅ Train thoroughly

✅ Monitor constantly

✅ Be ready to adapt

### Analysis:
✅ Use appropriate methods

✅ Report uncertainty

✅ Archive everything

---

# Resources for Learning

## Continue Your Journey

### Books:
- Lohr: "Sampling: Design and Analysis"
- Lumley: "Complex Surveys"

### Online:
- UCLA Statistical Consulting
- Penn State STAT 506

### Communities:
- R-survey mailing list
- Cross Validated (Stack Exchange)

---

# Day 1 Homework

## 🏠 Complete Before Tomorrow

**File:** `03-Scripts/Day1_Homework.R`

### Tasks:

1. **Redesign** Harry's survey with your approach
2. **Calculate** sample sizes for 5% CV
3. **Compare** three allocation methods
4. **Estimate** field costs
5. **Document** your rationale

### Submit:
- R script with comments
- 1-page summary
- Questions for tomorrow

---

# Quick Recap

## What We Covered Today

### Morning:
✅ Sampling fundamentals

✅ Stratification power

✅ Cluster trade-offs

✅ Multi-stage designs

### Afternoon:
✅ Total Survey Error

✅ Variance estimation

✅ Weight construction

✅ Quality framework

---

# Harry's Reflection

## End of Day 1

> "I started this morning terrified. Now I have a complete design!"

### Harry's Key Insights:

1. **Good design beats perfect analysis**
2. **Document everything**
3. **Consider operations early**
4. **Precision costs money**
5. **Perfect is enemy of good**

---

# Your Reflection

## Take 2 Minutes

Write down:

1. **Most valuable** thing you learned
2. **Most confusing** concept
3. **How you'll apply** this
4. One **question** for tomorrow

### Share with your neighbor!

---

# Preview: Day 2

## Tomorrow's Challenge

### Harry Discovers His Frame is Terrible!

- 30% coverage missing
- Duplicates everywhere
- Wrong geographic codes
- Outdated information

### Topics:
- Frame assessment
- Dual frame methods
- Area sampling
- Updating procedures
- Quality metrics

---

# Thank You!

## See You Tomorrow at 8:00

### Tonight:
- Review today's code
- Try homework exercises
- Rest well!

### Tomorrow:
- Bring your frame problems
- Real-world solutions
- Advanced techniques

## Questions?

### Stay for individual consultations

---

# Appendix A: Code Reference

## All Functions Used Today

```{r reference, echo=TRUE, eval=FALSE}
# Sampling
slice_sample()         # SRS
systematic_sample()    # Systematic
svydesign()           # Define design

# Estimation
svymean()             # Means
svytotal()            # Totals
svyby()               # Domains
svyquantile()         # Percentiles

# Variance
as.svrepdesign()      # Replication
SE()                  # Standard errors
cv()                  # CV calculation
confint()             # Confidence intervals

# Weights
calibrate()           # Calibration
trimWeights()         # Trimming
```

---

# Appendix B: Formulas

## Key Equations

### Design Effect:
$DEFF = 1 + (m-1)\rho$

### Neyman Allocation:
$n_h = n \frac{N_h S_h}{\sum N_h S_h}$

### Standard Error (complex):
$SE(\bar{y}) = \sqrt{V(\bar{y})} \times \sqrt{DEFF}$

### Sample Size:
$n = \frac{DEFF \times z^2 \times p(1-p)}{d^2}$

---

# Appendix C: Troubleshooting

## Error Messages

### "Lonely PSU"
```{r error1, echo=TRUE, eval=FALSE}
options(survey.lonely.psu = "adjust")
# OR
options(survey.lonely.psu = "certainty")
```

### "Negative weights"
```{r error2, echo=TRUE, eval=FALSE}
data$weight[data$weight < 0] <- 0
# Investigate why!
```

### "Singular matrix"
```{r error3, echo=TRUE, eval=FALSE}
# Too many parameters
# Reduce model complexity
```

---

# Case Study: Rwanda DHS 2019-20

## Real-World Application

```{r rwanda_case, echo=FALSE}
rwanda_design <- data.frame(
  Stage = c("Stratification", "PSU Selection", "Household Selection"),
  Details = c("30 districts × Urban/Rural", "492 EAs selected", "26 HH per EA"),
  Sample = c("60 strata", "492 clusters", "12,792 households")
)

kable(rwanda_design) %>%
  kable_styling(bootstrap_options = "striped")
```

### Challenges Faced:
- Hilly terrain
- Scattered settlements
- Multiple languages

---

# Case Study Results

## What Rwanda Achieved

```{r rwanda_results, echo=TRUE}
# Response rates by residence
rwanda_response <- data.frame(
  Residence = c("Urban", "Rural", "Total"),
  Eligible = c(3245, 9547, 12792),
  Interviewed = c(3201, 9448, 12649),
  Response_Rate = c(98.6, 99.0, 98.9)
)

print(rwanda_response)

# Excellent response rates!
```

---

# Exercise: Analyze Rwanda Design

## Your Turn to Critique

📝 **EXERCISE:** Rwanda Analysis

**File:** `03-Scripts/Day1_Exercise_09_Rwanda.R`

### Questions:
1. Calculate design effect given ICC = 0.08
2. What's the effective sample size?
3. How would you improve the design?
4. What are cost implications?

---

# Case Study: Nigeria MICS 2021

## Large Country Challenge

### Scale:
- Population: 206 million
- States: 36 + FCT
- Languages: 500+
- Geography: Diverse

### Design Decision:
- 2,239 EAs (clusters)
- 15 HH per EA
- Total: 33,585 households

---

# Nigeria Stratification

## Complex Framework

```{r nigeria_strata, echo=TRUE}
# Nigeria's stratification
nigeria_strata <- expand.grid(
  State = paste0("State_", 1:37),
  Residence = c("Urban", "Rural")
)

# Total strata
n_strata <- nrow(nigeria_strata)
cat("Total strata:", n_strata, "\n")

# Average sample per stratum
avg_per_stratum <- 33585 / n_strata
cat("Average per stratum:", round(avg_per_stratum))
```

---

# Nigeria: Lessons Learned

## Key Insights

### What Worked:
✅ State-level estimates possible
✅ Good urban/rural balance
✅ Manageable cluster size

### Challenges:
❌ Security concerns in some areas
❌ Seasonal accessibility
❌ Language barriers

### Innovation:
Used tablet-based data collection!

---

# Case Study: South Africa GHS 2022

## General Household Survey

```{r sa_design, echo=FALSE}
sa_framework <- data.frame(
  Component = c("Frame", "PSUs", "Sample", "Rotation"),
  Details = c("Master Sample 2013", "3,324 PSUs", "20,908 dwellings", "Quarterly")
)

kable(sa_framework) %>%
  kable_styling(bootstrap_options = "striped")
```

### Unique Features:
- Rotating panel design
- Quarterly data collection
- Provincial estimates

---

# South Africa Rotation

## Panel Design

```{r sa_rotation, echo=FALSE, fig.height=4}
# Visualize rotation
quarters <- expand.grid(
  Quarter = paste0("Q", 1:4),
  Panel = LETTERS[1:4]
)
quarters$InSample <- c(
  c(1,1,1,0), c(0,1,1,1), c(1,0,1,1), c(1,1,0,1)
)

ggplot(quarters, aes(x = Quarter, y = Panel, fill = factor(InSample))) +
  geom_tile(color = "white", size = 2) +
  scale_fill_manual(values = c("white", "#2E86AB"),
                   labels = c("Out", "In")) +
  labs(title = "South Africa GHS Rotation Pattern") +
  theme_minimal() +
  theme(legend.title = element_blank())
```

---

# Exercise: Design for Your Country

## Apply What You've Learned

📝 **MAJOR EXERCISE:** Country-Specific Design

**File:** `03-Scripts/Day1_Exercise_10_YourCountry.R`

### Create Complete Design For:
1. Your national household survey
2. Budget: $100,000
3. Timeline: 6 months
4. Required precision: CV < 5% nationally

### Include:
- Frame assessment
- Stratification plan
- Sample allocation
- Field operations
- Quality control

---

# Working Session

## 45 Minutes Group Work

### Phase 1 (20 min): Design
- Work in country teams
- Create detailed plan
- Calculate sample sizes

### Phase 2 (15 min): Cost
- Estimate all costs
- Optimize allocation
- Document trade-offs

### Phase 3 (10 min): Prepare
- Create presentation
- Highlight innovations
- Identify challenges

---

# Group Presentations

## Share Your Designs

### Each Team (5 minutes):

1. **Country context** and challenges
2. **Design choices** and rationale
3. **Sample allocation** details
4. **Expected outcomes** and precision
5. **Innovation** in your approach

### Peer Review:
- Constructive feedback
- Share similar experiences
- Suggest improvements

---

# Advanced Topic: Dual Frame Sampling

## When One Frame Isn't Enough

```{r dual_frame, echo=TRUE}
# Landline vs Mobile phone frames
landline_coverage <- 0.60  # 60% have landline
mobile_coverage <- 0.85    # 85% have mobile
overlap <- 0.50            # 50% have both

# Single frame coverage
cat("Landline only:", landline_coverage, "\n")
cat("Mobile only:", mobile_coverage, "\n")

# Dual frame coverage
dual_coverage <- landline_coverage + mobile_coverage - overlap
cat("Dual frame:", dual_coverage, "\n")
cat("Coverage gain:", dual_coverage - mobile_coverage)
```

---

# Dual Frame Design

## Implementation

```{r dual_implementation, echo=TRUE}
# Optimal allocation between frames
# Simplified version
c_L <- 20  # Cost per landline interview
c_M <- 30  # Cost per mobile interview
v_L <- 100 # Variance in landline frame
v_M <- 150 # Variance in mobile frame

# Optimal proportion from landline
prop_L <- sqrt(v_L/c_L) / (sqrt(v_L/c_L) + sqrt(v_M/c_M))
cat("Proportion from landline:", round(prop_L, 2))
cat("\nProportion from mobile:", round(1-prop_L, 2))
```

---

# Dual Frame Weights

## Complex Weight Calculation

```{r dual_weights, echo=TRUE}
# Compositing factor
lambda <- 0.5  # Equal weight to both frames

# For overlap domain (in both frames)
# Weight = lambda * w_L + (1-lambda) * w_M

# Example household in overlap
w_landline <- 1000
w_mobile <- 800
w_composite <- lambda * w_landline + (1-lambda) * w_mobile

cat("Landline weight:", w_landline, "\n")
cat("Mobile weight:", w_mobile, "\n")
cat("Composite weight:", w_composite)
```

---

# Advanced Topic: Capture-Recapture

## Estimating Hidden Populations

```{r capture_recapture_detailed, echo=TRUE}
# Two independent samples of homeless population
list_A <- 500  # Found by outreach teams
list_B <- 450  # Found at shelters
both <- 120    # Found in both lists

# Lincoln-Petersen estimate
N_est <- (list_A * list_B) / both

# Chapman estimator (less biased)
N_chapman <- ((list_A + 1) * (list_B + 1)) / (both + 1) - 1

cat("Lincoln-Petersen:", round(N_est), "\n")
cat("Chapman:", round(N_chapman), "\n")

# Confidence interval
se <- sqrt((list_A * list_B * (list_A - both) * (list_B - both)) / 
          (both^3))
ci <- N_est + c(-1.96, 1.96) * se
cat("95% CI: [", round(ci[1]), ",", round(ci[2]), "]")
```

---

# Advanced Topic: Adaptive Sampling

## For Rare Populations

```{r adaptive_sampling, echo=TRUE}
# Adaptive cluster sampling simulation
adaptive_cluster <- function(initial_n, threshold, p_rare) {
  final_n <- initial_n
  
  # Initial sample
  found_rare <- rbinom(initial_n, 1, p_rare)
  n_rare <- sum(found_rare)
  
  # For each rare unit, sample neighbors
  if(n_rare > 0) {
    # Average 4 neighbors per rare unit
    additional <- n_rare * 4
    final_n <- initial_n + additional
  }
  
  return(list(
    initial = initial_n,
    final = final_n,
    rare_found = n_rare
  ))
}

# Simulate
set.seed(2025)
result <- adaptive_cluster(50, 5, 0.05)
print(result)
```

---

# Advanced Topic: Small Area Estimation

## Model-Based Approach

```{r sae_model, echo=TRUE}
# Fay-Herriot model concept
# Direct estimates for small areas
direct_est <- c(45000, 43000, 47000, 42000, 46000)
direct_se <- c(3000, 3500, 2800, 4000, 3200)

# Auxiliary variable (e.g., census data)
auxiliary <- c(44000, 44500, 46000, 43000, 45500)

# Simple synthetic estimator
synthetic <- mean(direct_est) + 0.8 * (auxiliary - mean(auxiliary))

# Composite estimator (simplified)
gamma <- 1 / (1 + direct_se^2 / var(direct_est))
composite <- gamma * direct_est + (1 - gamma) * synthetic

# Compare
comparison <- data.frame(
  Area = 1:5,
  Direct = direct_est,
  Synthetic = round(synthetic),
  Composite = round(composite),
  SE_Direct = direct_se
)

print(comparison)
```

---

# SAE Visualization

```{r sae_viz, echo=FALSE, fig.height=5}
# Visualize SAE improvements
sae_data <- data.frame(
  Area = rep(1:10, 3),
  Estimate = c(rnorm(10, 45000, 3000),  # Direct
              rep(45000, 10) + rnorm(10, 0, 500),  # Synthetic
              rnorm(10, 45000, 1500)),  # Composite
  Method = rep(c("Direct", "Synthetic", "Composite"), each = 10),
  SE = rep(c(3000, 500, 1500), each = 10)
)

ggplot(sae_data, aes(x = factor(Area), y = Estimate, color = Method)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Estimate - SE, ymax = Estimate + SE),
                width = 0.2, alpha = 0.5) +
  labs(title = "Small Area Estimation Methods",
       subtitle = "Composite combines strength of both",
       x = "Area", y = "Income Estimate") +
  theme_minimal() +
  scale_color_manual(values = c("#A23B72", "#52B788", "#2E86AB"))
```

---

# R Package Deep Dive: survey

## Complete Functionality

```{r survey_complete, echo=TRUE, eval=FALSE}
# Full survey package capabilities
library(survey)

# 1. Design specification
dclus2 <- svydesign(id = ~dnum+snum, fpc = ~fpc1+fpc2, data = apiclus2)

# 2. Subset operations
dsub <- subset(dclus2, comp.imp == "Yes")

# 3. Regression
model <- svyglm(api00 ~ ell + meals + mobility, design = dclus2)

# 4. Quantiles
svyquantile(~api00, dclus2, c(0.25, 0.5, 0.75))

# 5. Correlations
svycor(~api00 + api99, dclus2)

# 6. Tables
svytable(~stype + comp.imp, dclus2)

# 7. Graphics
svyplot(api00 ~ api99, design = dclus2)
```

---

# R Package: srvyr

## Modern Survey Analysis

```{r srvyr_complete, echo=TRUE}
library(srvyr)

# Convert to srvyr
srvyr_des <- as_survey_design(sim_data,
                              ids = cluster,
                              strata = stratum,
                              weights = weight)

# Complex analysis pipeline
result <- srvyr_des %>%
  filter(income > 20000) %>%
  group_by(stratum, urban_rural) %>%
  summarise(
    mean_income = survey_mean(income, vartype = "ci"),
    median_income = survey_median(income),
    total_pop = survey_total(vartype = "cv"),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_income))

head(result, 3)
```

---

# Creating Publication Tables

## Professional Output

```{r publication_table, echo=TRUE}
library(gt)

# Create summary for publication
pub_summary <- srvyr_des %>%
  group_by(stratum) %>%
  summarise(
    n = n(),
    Income = survey_mean(income, vartype = c("se", "cv")),
    HH_Size = survey_mean(household_size, vartype = "se")
  )

# Format with gt
pub_summary %>%
  gt() %>%
  tab_header(
    title = "Table 1: Survey Estimates by Stratum",
    subtitle = "National Household Survey 2025"
  ) %>%
  fmt_number(columns = c(Income, Income_se, HH_Size, HH_Size_se),
            decimals = 0) %>%
  fmt_percent(columns = Income_cv, decimals = 1) %>%
  cols_label(
    n = "Sample Size",
    Income = "Mean Income ($)",
    Income_se = "SE",
    Income_cv = "CV",
    HH_Size = "Household Size",
    HH_Size_se = "SE"
  )
```

---

# Visualization Best Practices

## Showing Uncertainty

```{r viz_uncertainty, echo=TRUE, fig.height=4}
# Calculate estimates with CI
estimates <- svyby(~income, ~stratum, design, svymean)
estimates$lower <- estimates$income - 1.96 * SE(estimates)
estimates$upper <- estimates$income + 1.96 * SE(estimates)

# Professional plot
ggplot(estimates, aes(x = stratum, y = income)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +
  labs(title = "Income by Stratum with 95% Confidence Intervals",
       x = "Stratum", y = "Mean Income ($)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::dollar)
```

---

# Complex Survey Graphs

```{r complex_graphs, echo=TRUE, fig.height=4}
# Weighted density plot
library(ggplot2)

# Create weighted density
ggplot(sim_data, aes(x = income, weight = weight)) +
  geom_density(aes(fill = stratum), alpha = 0.5) +
  facet_wrap(~urban_rural) +
  labs(title = "Weighted Income Distribution",
       subtitle = "By Urban/Rural and Stratum",
       x = "Income ($)", y = "Density") +
  theme_minimal() +
  scale_x_continuous(labels = scales::dollar)
```

---

# Regression with Survey Data

## Proper Inference

```{r survey_regression_detailed, echo=TRUE}
# Complex survey regression
model <- svyglm(income ~ age + factor(urban_rural) + household_size,
                design = design)

# Summary with design-based SEs
summary(model)$coefficients[, c("Estimate", "Std. Error", "Pr(>|t|)")]
```

---

# Model Diagnostics

```{r model_diagnostics, echo=TRUE, fig.height=3.5}
# Residual plots for survey regression
residuals_df <- data.frame(
  fitted = fitted(model),
  residuals = residuals(model),
  weight = weights(design)
)

ggplot(residuals_df, aes(x = fitted, y = residuals)) +
  geom_point(aes(size = weight), alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Weighted Residual Plot",
       x = "Fitted Values", y = "Residuals") +
  theme_minimal()
```

---

# Subpopulation Analysis

## The Right Way

```{r subpop_correct, echo=TRUE}
# CORRECT: Keep full design, subset in analysis
urban_mean_correct <- svymean(~income, 
                              subset(design, urban_rural == "Urban"))

# WRONG: Don't subset the design first!
# urban_design <- subset(design, urban_rural == "Urban")
# urban_mean_wrong <- svymean(~income, urban_design)

print(urban_mean_correct)

# Domain estimation alternative
domain_means <- svyby(~income, ~urban_rural, design, svymean)
print(domain_means)
```

---

# Missing Data Strategies

## Survey-Weighted Imputation

```{r missing_data_survey, echo=TRUE}
# Create missing data
sim_data_miss <- sim_data
sim_data_miss$income[sample(1:1000, 100)] <- NA

# Check missing pattern by stratum
missing_by_strata <- sim_data_miss %>%
  group_by(stratum) %>%
  summarise(
    n_missing = sum(is.na(income)),
    pct_missing = mean(is.na(income)) * 100
  )

print(missing_by_strata)

# Simple weighted mean imputation by stratum
sim_data_miss <- sim_data_miss %>%
  group_by(stratum) %>%
  mutate(income_imp = ifelse(is.na(income),
                             weighted.mean(income, weight, na.rm = TRUE),
                             income))
```

---

# Multiple Imputation

## Advanced Approach

```{r multiple_imputation, echo=TRUE, eval=FALSE}
# Using mice with survey weights
library(mice)

# Prepare for imputation
imp_data <- sim_data_miss[, c("income", "age", "household_size", "stratum")]

# Create multiple imputations
mi <- mice(imp_data, m = 5, method = "pmm", seed = 2025)

# Analyze each imputed dataset
mi_results <- with(mi, lm(income ~ age + household_size))

# Pool results
pooled <- pool(mi_results)
summary(pooled)
```

---

# Variance Estimation Details

## Understanding Components

```{r variance_components, echo=TRUE}
# Decompose variance contributions
# Total variance = V_sampling + V_nonresponse + V_measurement

# Sampling variance
v_sampling <- SE(svymean(~income, design))^2

# Simulate nonresponse variance
response_rates <- c(0.70, 0.72, 0.68, 0.75)
v_nonresponse <- var(1/response_rates) * mean(sim_data$income)^2 / 1000

# Measurement variance (assumed)
v_measurement <- (0.05 * mean(sim_data$income))^2

# Total
v_total <- v_sampling + v_nonresponse + v_measurement

cat("Variance Components:\n")
cat("Sampling:", round(v_sampling), "(",
    round(100*v_sampling/v_total), "%)\n")
cat("Nonresponse:", round(v_nonresponse), "(",
    round(100*v_nonresponse/v_total), "%)\n")
cat("Measurement:", round(v_measurement), "(",
    round(100*v_measurement/v_total), "%)\n")
```

---

# Replication Methods Comparison

## Empirical Comparison

```{r replication_comparison, echo=TRUE}
# Compare variance estimates
methods <- c("Linearization", "Jackknife", "Bootstrap", "BRR")
estimates <- c(45234, 45234, 45235, 45234)
std_errors <- c(512, 518, 521, 515)
compute_time <- c(0.1, 2.3, 8.5, 0.8)

comparison <- data.frame(
  Method = methods,
  Estimate = estimates,
  SE = std_errors,
  Time_sec = compute_time
)

print(comparison)

# Efficiency
comparison$Relative_Efficiency <- 
  round(comparison$SE[1] / comparison$SE, 3)
print(comparison[, c("Method", "Relative_Efficiency")])
```

---

# Bootstrap Confidence Intervals

## Different Types

```{r bootstrap_ci, echo=TRUE}
# Create bootstrap design with fewer replicates for speed
boot_des <- as.svrepdesign(design, type = "bootstrap", replicates = 100)

# Calculate mean with bootstrap
boot_mean <- svymean(~income, boot_des)

# Get estimate and SE
estimate <- coef(boot_mean)
se_boot <- SE(boot_mean)

# Different CI methods
# 1. Normal approximation
ci_normal <- estimate + c(-1.96, 1.96) * se_boot

# 2. Using confint (percentile method)
ci_percentile <- confint(boot_mean, level = 0.95)

# 3. Manual calculation with t-distribution
df_approx <- degf(boot_des)
t_crit <- qt(0.975, df_approx)
ci_t <- estimate + c(-1, 1) * t_crit * se_boot

cat("Estimate:", round(estimate), "\n")
cat("Normal CI:", round(ci_normal), "\n")
cat("Percentile CI:", round(ci_percentile), "\n")
cat("t-based CI:", round(ci_t))
```

---

# Extreme Weight Detection

## Identifying Problems

```{r extreme_weights2, echo=TRUE, fig.height=4}
# Generate weights with outliers
weights_with_outliers <- c(rnorm(980, 200, 20),
                          500, 550, 50, 45, 600)

# Detection methods
# 1. Box plot rule
q1 <- quantile(weights_with_outliers, 0.25)
q3 <- quantile(weights_with_outliers, 0.75)
iqr <- q3 - q1
outliers_box <- weights_with_outliers[weights_with_outliers > q3 + 3*iqr |
                                      weights_with_outliers < q1 - 3*iqr]

# 2. CV threshold
cv_weights <- sd(weights_with_outliers) / mean(weights_with_outliers)

cat("Weight CV:", round(cv_weights, 3), "\n")
cat("Outliers detected:", length(outliers_box), "\n")
cat("Outlier values:", outliers_box)
```

---

# Weight Trimming Methods

## Comparing Approaches

```{r trimming_methods, echo=TRUE}
# Method 1: Percentile trimming
trim_percentile <- function(w, lower = 0.01, upper = 0.99) {
  bounds <- quantile(w, c(lower, upper))
  w[w < bounds[1]] <- bounds[1]
  w[w > bounds[2]] <- bounds[2]
  return(w)
}

# Method 2: CV reduction
trim_cv <- function(w, target_cv = 0.3) {
  while(sd(w)/mean(w) > target_cv) {
    w[w == max(w)] <- quantile(w, 0.99)
    w[w == min(w)] <- quantile(w, 0.01)
  }
  return(w)
}

# Apply methods
w_original <- weights_with_outliers
w_percentile <- trim_percentile(w_original)
w_cv <- trim_cv(w_original)

# Compare
cat("Original CV:", round(sd(w_original)/mean(w_original), 3), "\n")
cat("Percentile trim CV:", round(sd(w_percentile)/mean(w_percentile), 3), "\n")
cat("CV trim CV:", round(sd(w_cv)/mean(w_cv), 3))
```

---

# Calibration Practical Example

## Step-by-Step

```{r calibration_practical, echo=TRUE, eval=FALSE}
# Real calibration example
library(survey)

# Start with base design
base_design <- svydesign(ids = ~cluster, strata = ~stratum,
                         weights = ~weight, data = sim_data)

# Known population totals
pop_totals <- c(
  "(Intercept)" = 2200000,
  "stratumRural" = 1400000,
  "age" = 88000000  # Total age years
)

# Calibrate to match
calib_design <- calibrate(base_design,
                          formula = ~stratum + age,
                          population = pop_totals,
                          bounds = c(0.5, 2))  # Limit weight ratios

# Check calibration worked
svytotal(~stratum + age, calib_design)
```

---

# Quality Control Automation (Part 1)

## Setting Up Daily Checks

```{r qc_auto_setup, echo=TRUE, eval=FALSE}
# Automated quality control function
daily_qc <- function(data_file, date) {
  
  # Load data
  data <- read.csv(data_file)
  
  # Initialize report
  report <- list()
  
  # Check 1: Response rates by interviewer
  report$response_rate <- data %>%
    group_by(interviewer_id) %>%
    summarise(
      attempts = n(),
      completed = sum(status == "Complete"),
      rate = completed / attempts
    )
  
  return(report)
}
```

---

# Quality Control Automation (Part 2)

## Duration and Location Checks

```{r qc_auto_checks, echo=TRUE, eval=FALSE}
# Continue quality checks
add_duration_checks <- function(data, report) {
  
  # Check 2: Interview duration
  report$duration <- data %>%
    filter(status == "Complete") %>%
    group_by(interviewer_id) %>%
    summarise(
      avg_minutes = mean(duration),
      min_duration = min(duration),
      max_duration = max(duration),
      flag = avg_minutes < 15 | avg_minutes > 90
    )
  
  # Flag problematic interviews
  report$duration_issues <- report$duration %>%
    filter(flag == TRUE)
  
  return(report)
}
```

---

# Quality Control Automation (Part 3)

## GPS and Reporting

```{r qc_auto_report, echo=TRUE, eval=FALSE}
# GPS verification and reporting
complete_qc <- function(data_file, date) {
  
  data <- read.csv(data_file)
  report <- list()
  
  # Check 3: GPS verification
  report$gps <- data %>%
    mutate(
      in_psu = verify_gps(lat, lon, psu_id)
    ) %>%
    filter(!in_psu)
  
  # Generate HTML report
  create_html_report(report, date)
  
  # Email if issues found
  if(length(unlist(report)) > 0) {
    send_alert_email(report, date)
  }
  
  return(report)
}
```

---

# Paradata Analysis

## Learning from Process Data

```{r paradata_analysis, echo=TRUE}
# Analyze interview process data
paradata <- data.frame(
  case_id = 1:500,
  interviewer = sample(LETTERS[1:10], 500, replace = TRUE),
  attempts = rpois(500, 2.5),
  duration = rnorm(500, 35, 10),
  time_slot = sample(c("Morning", "Afternoon", "Evening"), 500, replace = TRUE),
  completed = rbinom(500, 1, 0.72)
)

# Best time for contact
time_analysis <- paradata %>%
  group_by(time_slot) %>%
  summarise(
    success_rate = mean(completed),
    avg_attempts = mean(attempts),
    avg_duration = mean(duration[completed == 1])
  )

print(time_analysis)

# Interviewer effects
interviewer_effects <- paradata %>%
  group_by(interviewer) %>%
  summarise(
    cases = n(),
    success = mean(completed),
    speed = mean(duration[completed == 1])
  ) %>%
  mutate(flag = success < 0.6 | speed < 20)

print(interviewer_effects[interviewer_effects$flag, ])
```

---

# Cost-Benefit Analysis

## Optimizing Resources

```{r cost_benefit, echo=TRUE}
# Compare design options
options <- data.frame(
  Design = c("SRS", "Stratified", "Cluster", "Two-stage"),
  Sample_Size = c(2000, 2000, 2000, 2000),
  Travel_Cost = c(40000, 38000, 8000, 10000),
  Interview_Cost = c(20000, 20000, 20000, 20000),
  Training_Cost = c(5000, 6000, 7000, 8000),
  Total_Cost = c(65000, 64000, 35000, 38000),
  DEFF = c(1.0, 0.85, 2.2, 1.9),
  Effective_n = c(2000, 2353, 909, 1053)
)

# Cost per effective interview
options$Cost_per_eff <- round(options$Total_Cost / options$Effective_n)

# Rank by efficiency
options$Rank <- rank(options$Cost_per_eff)

print(options[order(options$Rank), 
             c("Design", "Total_Cost", "Effective_n", "Cost_per_eff", "Rank")])
```

---

# Sensitive Questions

## Randomized Response

```{r randomized_response, echo=TRUE}
# Randomized response for sensitive questions
# Example: "Have you ever used drugs?"

# Setup
n <- 1000
p_sensitive <- 0.15  # True prevalence (unknown)
p_coin <- 0.7  # Probability of answering truthfully

# Simulation
true_status <- rbinom(n, 1, p_sensitive)
answer_truthfully <- rbinom(n, 1, p_coin)
forced_yes <- rbinom(n, 1, 0.5)  # If not truthful, flip coin

# Observed responses
observed <- ifelse(answer_truthfully == 1,
                  true_status,
                  forced_yes)

# Estimate true prevalence
p_yes_observed <- mean(observed)
p_estimated <- (p_yes_observed - (1-p_coin)*0.5) / p_coin

cat("Observed 'yes' rate:", round(p_yes_observed, 3), "\n")
cat("Estimated true rate:", round(p_estimated, 3), "\n")
cat("Actual true rate:", round(p_sensitive, 3))
```

---

# Non-Response Bias Assessment

## Analyzing Patterns

```{r nonresponse_bias, echo=TRUE}
# Simulate response patterns
population <- data.frame(
  income = rlnorm(10000, log(45000), 0.5),
  age = sample(18:70, 10000, replace = TRUE),
  urban = rbinom(10000, 1, 0.4)
)

# Response probability depends on income
population$p_respond <- plogis(-8 + 
                               0.00015 * population$income +
                               0.02 * population$age +
                               0.5 * population$urban)

population$responded <- rbinom(10000, 1, population$p_respond)

# Compare respondents vs non-respondents
bias_assessment <- data.frame(
  Variable = c("Income", "Age", "Urban%"),
  Population = c(mean(population$income),
                mean(population$age),
                mean(population$urban)*100),
  Respondents = c(mean(population$income[population$responded == 1]),
                 mean(population$age[population$responded == 1]),
                 mean(population$urban[population$responded == 1])*100),
  NonResp = c(mean(population$income[population$responded == 0]),
             mean(population$age[population$responded == 0]),
             mean(population$urban[population$responded == 0])*100)
)

bias_assessment$Bias <- round((bias_assessment$Respondents - 
                               bias_assessment$Population) / 
                               bias_assessment$Population * 100, 1)

print(bias_assessment)
```

---

# Frame Coverage Assessment

## Measuring Completeness

```{r coverage_assessment, echo=TRUE}
# Dual system estimation for coverage
# Main frame
frame_units <- 45000
frame_sample <- 500
frame_found <- 485  # Found in field

# Independent coverage check
check_sample <- 200
check_in_frame <- 176
check_not_frame <- 24

# Estimate total population
capture_rate <- check_in_frame / check_sample
population_est <- frame_units / capture_rate

coverage_rate <- frame_units / population_est * 100

cat("Frame units:", frame_units, "\n")
cat("Estimated population:", round(population_est), "\n")
cat("Coverage rate:", round(coverage_rate, 1), "%\n")
cat("Missing units:", round(population_est - frame_units))
```

---

# Exercise: Complete Analysis

## Putting It All Together

📝 **COMPREHENSIVE EXERCISE:** Full Analysis Pipeline

**File:** `03-Scripts/Day1_Exercise_11_Complete_Analysis.R`

### Your Tasks:
1. Load and explore data
2. Create survey design object
3. Calculate weighted estimates
4. Perform subpopulation analysis
5. Fit regression model
6. Create publication tables
7. Generate quality report

### Deliverable:
Complete HTML report with all results

---

# Real Data Practice

## Working with DHS Data

```{r dhs_practice, echo=TRUE, eval=FALSE}
# Example using DHS data structure
# Load DHS-style data
dhs_data <- read.csv("data/dhs_sample.csv")

# DHS uses specific weight variables
dhs_design <- svydesign(
  ids = ~v001,          # Cluster number
  strata = ~v023,       # Strata
  weights = ~v005/1e6,  # Weight (divided by 1,000,000)
  data = dhs_data
)

# Standard DHS indicators
# Contraceptive prevalence rate
cpr <- svymean(~contraceptive_use, 
               subset(dhs_design, v502 == 1),  # Currently married
               na.rm = TRUE)

# Total fertility rate (simplified)
tfr <- svyby(~births_last_year, ~age_group,
             subset(dhs_design, sex == "Female"),
             svymean, na.rm = TRUE)
```

---

# Reproducible Research

## Best Practices

```{r reproducible, echo=TRUE, eval=FALSE}
# Session setup for reproducibility
# Save in 00_setup.R

# Record session info
session_info <- sessionInfo()
save(session_info, file = "session_info.RData")

# Set seed globally
set.seed(2025)

# Use explicit package versions
library(checkpoint)
checkpoint("2025-01-01")

# Document data sources
data_sources <- list(
  frame = "Census 2020, updated March 2025",
  survey_data = "NSO_HHS_2025_v1.csv",
  weights = "Calculated using script 03_weights.R",
  date_processed = Sys.Date()
)

# Create project structure
dir.create("data/raw", recursive = TRUE)
dir.create("data/processed", recursive = TRUE)
dir.create("output/tables", recursive = TRUE)
dir.create("output/figures", recursive = TRUE)
dir.create("scripts", recursive = TRUE)
dir.create("docs", recursive = TRUE)
```

---

# Version Control

## Track Your Changes

```{r version_control, echo=TRUE, eval=FALSE}
# Git workflow for survey projects

# Initialize repository
# git init

# Create .gitignore
gitignore_content <- "
# Data files (confidential)
data/raw/*
data/processed/*.csv

# Individual identifiers
*_identified.csv
*_pii.csv

# Outputs
output/confidential/*

# R files
.Rhistory
.RData
.Rproj.user
"

writeLines(gitignore_content, ".gitignore")

# Track analysis scripts
# git add scripts/*.R
# git add docs/*.Rmd
# git commit -m 'Initial survey design'
```

---

# Creating Reports (Part 1)

## Automated Documentation Setup

```{r report_setup, echo=TRUE, eval=FALSE}
# R Markdown report template
library(rmarkdown)

# Create report function
generate_survey_report <- function(design, output_file) {
  
  # Create temporary Rmd with header
  rmd_header <- '---
title: "Survey Analysis Report"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
---'
  
  # Write header
  writeLines(rmd_header, "temp_report.Rmd")
}
```

---

# Creating Reports (Part 2)

## Report Content Template

```{r report_content, echo=TRUE, eval=FALSE}
# Add content sections to report
rmd_content <- '
## Survey Design
{r}
summary(design)


## Population Estimates  
{r}
# Create population total variable
design$variables$one <- 1
svytotal(~one, design)


## Key Indicators
{r}
svymean(~income, design, na.rm = TRUE)
'

# Note: In actual use, add backticks before {r}
# Append content
cat(rmd_content, file = "temp_report.Rmd", append = TRUE)
```

---

# Creating Reports (Part 3)

## Rendering and Output

```{r report_render, echo=TRUE, eval=FALSE}
# Complete the report generation
generate_full_report <- function(design, output_file) {
  
  # Create complete Rmd file
  create_rmd_template(design)
  
  # Render to HTML
  render("temp_report.Rmd", 
         output_file = output_file,
         quiet = TRUE)
  
  # Clean up temporary file
  unlink("temp_report.Rmd")
  
  cat("Report generated:", output_file)
}

# Example usage
# generate_full_report(design, "survey_report.html")
```

---

# Interactive Data Exploration

## Using Shiny with Survey Data

```{r shiny_app, echo=TRUE, eval=FALSE}
# Create interactive dashboard
library(shiny)
library(survey)

ui <- fluidPage(
  titlePanel("Survey Data Explorer"),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("variable", "Choose variable:",
                 choices = c("income", "age", "household_size")),
      selectInput("groupby", "Group by:",
                 choices = c("None", "stratum", "urban_rural"))
    ),
    
    mainPanel(
      plotOutput("plot"),
      tableOutput("summary")
    )
  )
)

server <- function(input, output) {
  output$plot <- renderPlot({
    svyhist(as.formula(paste("~", input$variable)), design)
  })
  
  output$summary <- renderTable({
    svymean(as.formula(paste("~", input$variable)), design)
  })
}

# shinyApp(ui = ui, server = server)
```

---

# Field Operations Planning

## Detailed Timeline

```{r field_timeline, echo=FALSE}
timeline <- data.frame(
  Week = 1:12,
  Phase = c(rep("Preparation", 2), rep("Training", 2), 
           rep("Pilot", 1), rep("Revision", 1),
           rep("Main Fieldwork", 5), rep("Mop-up", 1)),
  Activities = c("Frame update", "Materials prep", 
                "Interviewer training", "Supervisor training",
                "Pilot survey", "Revise instruments",
                rep("Data collection", 5), "Final visits"),
  Milestone = c("", "", "", "", "Pilot report", "Final tools",
                "", "25% complete", "50% complete", 
                "75% complete", "Field complete", "Database closed")
)

kable(timeline[, c("Week", "Phase", "Activities", "Milestone")]) %>%
  kable_styling(bootstrap_options = "striped", font_size = 11) %>%
  row_spec(c(5,6,12), bold = TRUE, background = "#FFE4B5")
```

---

# Interviewer Training Program

## Comprehensive Curriculum

### Day 1: Survey Basics
- Overview and objectives
- Ethics and confidentiality
- Professional conduct

### Day 2: Sampling Procedures  
- Understanding selection
- Household listing
- Replacement rules

### Day 3: Questionnaire
- Question by question
- Skip patterns
- Probing techniques

---

# Training Program (cont.)

### Day 4: Field Practice
- Mock interviews
- Feedback sessions
- Common problems

### Day 5: Technology
- Tablet/CAPI use
- GPS recording
- Data transmission

### Day 6: Assessment
- Written test
- Practical test
- Certification

---

# Quality Assurance Protocol

## Multi-Level System

```{r qa_protocol, echo=TRUE}
# Define quality checks at each level
qa_levels <- list(
  Interviewer = c("Self-check after each interview",
                 "Daily review of work",
                 "Correct errors immediately"),
  
  Supervisor = c("Accompany 5% of interviews",
                "Review all questionnaires",
                "Back-check 10% of households"),
  
  Coordinator = c("Random spot checks",
                 "Data consistency checks",
                 "Weekly progress reports"),
  
  Central = c("Real-time monitoring",
             "Statistical checks",
             "Final validation")
)

# Print protocol
for(level in names(qa_levels)) {
  cat(level, "responsibilities:\n")
  cat(paste("-", qa_levels[[level]], collapse = "\n"), "\n\n")
}
```

---

# Back-Check Procedures

## Verification System

```{r backcheck, echo=TRUE}
# Back-check sampling
total_interviews <- 2000
backcheck_rate <- 0.10
backcheck_n <- ceiling(total_interviews * backcheck_rate)

# What to verify
backcheck_items <- data.frame(
  Item = c("HH exists", "Respondent interviewed", 
          "HH composition", "Key indicators"),
  Priority = c("Critical", "Critical", "High", "Medium"),
  Action_if_fail = c("Reject all interviewer work",
                    "Reject interview",
                    "Re-interview",
                    "Flag for review")
)

print(backcheck_items)

cat("\nTotal back-checks needed:", backcheck_n)
```

---

# Data Management System

## From Field to Database

```{r data_flow, echo=FALSE, fig.height=4}
# Visualize data flow
par(mar = c(1,1,1,1))
plot(0, 0, type = "n", xlim = c(0, 10), ylim = c(0, 6),
     xlab = "", ylab = "", axes = FALSE, main = "Data Flow")

# Boxes
rect(0.5, 4.5, 2, 5.5)
text(1.25, 5, "Field\nCollection")

rect(3, 4.5, 4.5, 5.5)
text(3.75, 5, "Daily\nTransmission")

rect(5.5, 4.5, 7, 5.5)
text(6.25, 5, "Server\nValidation")

rect(8, 4.5, 9.5, 5.5)
text(8.75, 5, "Clean\nDatabase")

# Arrows
arrows(2, 5, 3, 5, length = 0.1, lwd = 2)
arrows(4.5, 5, 5.5, 5, length = 0.1, lwd = 2)
arrows(7, 5, 8, 5, length = 0.1, lwd = 2)

# Feedback loop
arrows(6.25, 4.5, 6.25, 2, length = 0.1, lwd = 1, lty = 2, col = "red")
arrows(6.25, 2, 1.25, 2, length = 0.1, lwd = 1, lty = 2, col = "red")
arrows(1.25, 2, 1.25, 4.5, length = 0.1, lwd = 1, lty = 2, col = "red")
text(3.75, 1.5, "Error Reports", col = "red")
```

---

# CAPI Development

## Computer-Assisted Personal Interviewing

```{r capi_setup, echo=TRUE, eval=FALSE}
# Example using SurveyCTO/ODK structure
# Form definition (simplified XLSForm)

survey_form <- data.frame(
  type = c("begin group", "text", "integer", "select_one yes_no",
          "integer", "end group"),
  name = c("household", "resp_name", "age", "employed", 
          "income", "household"),
  label = c("Household Information", "Respondent Name", 
           "Age", "Currently employed?",
           "Monthly income", ""),
  relevant = c("", "", "", "", "employed='yes'", ""),
  constraint = c("", "", ". >= 15 and . <= 100", "", 
                ". >= 0", "")
)

# Validation rules built-in
# Skip patterns automatic
# GPS captured automatically
```

---

# Data Quality Dashboards

## Real-Time Monitoring

```{r dashboard_code, echo=TRUE, eval=FALSE}
# Create monitoring dashboard
library(flexdashboard)

# Dashboard components
create_dashboard <- function(data) {
  
  # KPI boxes
  kpis <- list(
    interviews_today = sum(data$date == Sys.Date()),
    response_rate = mean(data$completed),
    avg_duration = mean(data$duration),
    data_issues = sum(data$flags)
  )
  
  # Charts
  charts <- list(
    daily_progress = plot_daily_progress(data),
    interviewer_performance = plot_interviewer(data),
    response_by_region = plot_region_response(data),
    quality_indicators = plot_quality(data)
  )
  
  # Alerts
  alerts <- check_alerts(data)
  
  return(list(kpis = kpis, charts = charts, alerts = alerts))
}
```

---

# Handling Refusals

## Conversion Strategies

```{r refusal_handling, echo=TRUE}
# Refusal conversion protocol
refusal_tree <- data.frame(
  Reason = c("Too busy", "Don't trust", "Not interested", 
            "Gatekeeper", "Privacy concerns"),
  Strategy = c("Offer flexible timing", 
              "Show official letter",
              "Explain importance",
              "Speak to decision maker",
              "Explain confidentiality"),
  Success_Rate = c(0.45, 0.30, 0.25, 0.60, 0.35)
)

print(refusal_tree)

# Calculate expected conversions
initial_refusals <- 300
expected_conversions <- sum(initial_refusals/5 * refusal_tree$Success_Rate)
cat("\nExpected conversions:", round(expected_conversions))
```

---

# Difficult Populations

## Special Strategies

### Elderly Respondents:
- Larger print materials
- Allow proxy if needed
- Extra time allocation
- Morning interviews preferred

### High-Income Areas:
- Advance letters essential
- Professional appearance
- Emphasize confidentiality
- Weekend attempts

### Remote Locations:
- Local guide necessary
- Group transportation
- Extended field periods
- Weather contingencies

---

# Proxy Interviews

## When and How

```{r proxy_rules, echo=TRUE}
# Define proxy rules
proxy_rules <- data.frame(
  Situation = c("Respondent absent", "Ill/hospitalized", 
               "Mental incapacity", "Language barrier",
               "Under 15 years"),
  Proxy_Allowed = c("After 3 visits", "Yes", "Yes", 
                   "If no translator", "Required"),
  Preferred_Proxy = c("Spouse", "Caregiver", "Caregiver",
                     "Family member", "Parent"),
  Variables_Allowed = c("Most", "Basic only", "Basic only",
                       "All", "All")
)

kable(proxy_rules) %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
```

---

# Language and Translation

## Multi-Lingual Surveys

```{r translation, echo=TRUE}
# Translation management
languages <- data.frame(
  Language = c("English", "French", "Swahili", "Local A", "Local B"),
  Households = c(45, 20, 15, 12, 8),  # Percentage
  Translation = c("Original", "Full", "Full", "Key terms", "Interpreter"),
  Quality_Check = c("--", "Back-translation", "Back-translation",
                   "Expert review", "Supervisor check")
)

print(languages)

# Translation costs
trans_costs <- data.frame(
  Method = c("Full translation", "Key terms only", "Interpreter"),
  Cost_per_lang = c(5000, 1000, 2000),
  Time_days = c(20, 5, 0)
)

print(trans_costs)
```

---

# Cultural Sensitivity

## Context Matters

### Considerations:

```{r cultural, echo=FALSE}
cultural_factors <- data.frame(
  Factor = c("Gender matching", "Religious holidays", "Market days",
            "Harvest season", "School calendar", "Local customs"),
  Impact = c("Response rate", "Availability", "Availability",
            "Migration", "Proxy needed", "Access"),
  Adjustment = c("Female interviewers for women",
                "Avoid survey dates",
                "Avoid survey dates",
                "Adjust timeline",
                "Plan for proxies",
                "Respect protocols")
)

kable(cultural_factors) %>%
  kable_styling(bootstrap_options = "striped", font_size = 11)
```

---

# Emergency Protocols

## When Things Go Wrong

### Security Issues:
1. Immediate withdrawal
2. Report to supervisor
3. Document incident
4. Assess area safety
5. Decide on continuation

### Natural Disasters:
1. Suspend operations
2. Ensure staff safety
3. Assess damage
4. Adjust sample if needed
5. Document impact

### Health Emergencies:
1. Follow health protocols
2. PPE if required
3. Remote options
4. Adjust procedures

---

# Budget Management

## Detailed Cost Breakdown

```{r budget_detailed, echo=TRUE}
# Comprehensive budget
budget <- data.frame(
  Category = c("Personnel", "Training", "Materials", 
              "Transport", "Communication", "Venue",
              "Incentives", "Data entry", "Analysis",
              "Dissemination", "Contingency"),
  Amount = c(35000, 8000, 5000, 15000, 3000, 2000,
            4000, 6000, 10000, 5000, 7000),
  Percent = NA
)

budget$Percent <- round(100 * budget$Amount / sum(budget$Amount), 1)

# Add subtotals
budget <- rbind(budget,
  data.frame(Category = "TOTAL", 
            Amount = sum(budget$Amount),
            Percent = 100))

print(budget)
```

---

# Cost Optimization

## Saving Without Sacrificing Quality

```{r cost_optimization, echo=TRUE}
# Cost-saving strategies
strategies <- data.frame(
  Strategy = c("CAPI vs Paper", "Local interviewers", 
              "Cluster optimization", "Batch training",
              "Shared transport"),
  Savings = c(8000, 5000, 12000, 3000, 4000),
  Risk = c("Technical issues", "Quality concerns",
          "Lower precision", "Less flexibility",
          "Scheduling complexity"),
  Mitigation = c("Tech support", "More supervision",
                "Increase sample", "Follow-up training",
                "Careful planning")
)

print(strategies)

cat("\nTotal potential savings: $", sum(strategies$Savings))
```

---

# Contract Management

## Working with Partners

### Key Elements:

```{r contracts, echo=FALSE}
contract_elements <- data.frame(
  Element = c("Scope of work", "Deliverables", "Timeline",
             "Payment schedule", "Quality standards", 
             "Data ownership", "Confidentiality"),
  Description = c("Detailed activities", "Specific outputs",
                 "Milestones & deadlines", "Linked to deliverables",
                 "Measurable indicators", "Clear IP rights",
                 "Data protection rules"),
  Critical = c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes")
)

kable(contract_elements) %>%
  kable_styling(bootstrap_options = "striped") %>%
  column_spec(3, bold = TRUE, color = "white", background = "#28a745")
```

---

# Risk Management

## Anticipating Problems

```{r risk_matrix, echo=TRUE}
# Risk assessment matrix
risks <- data.frame(
  Risk = c("Frame outdated", "Low response", "Weather delays",
          "Budget overrun", "Staff turnover", "Technical failure"),
  Probability = c("High", "Medium", "Low", "Medium", "Low", "Low"),
  Impact = c("High", "High", "Medium", "High", "Medium", "High"),
  Mitigation = c("Update sample", "Incentives", "Buffer time",
                "Contingency fund", "Train extras", "Backup systems")
)

# Assign risk scores
risk_score <- function(prob, impact) {
  p <- ifelse(prob == "High", 3, ifelse(prob == "Medium", 2, 1))
  i <- ifelse(impact == "High", 3, ifelse(impact == "Medium", 2, 1))
  return(p * i)
}

risks$Score <- mapply(risk_score, risks$Probability, risks$Impact)
risks <- risks[order(risks$Score, decreasing = TRUE), ]

print(risks)
```

---

# Ethical Considerations

## Protecting Respondents

### Core Principles:

1. **Informed Consent**
   - Clear explanation
   - Voluntary participation
   - Right to withdraw

2. **Confidentiality**
   - No individual identification
   - Secure data storage
   - Limited access

3. **Do No Harm**
   - Avoid sensitive topics
   - Respect boundaries
   - Provide resources if needed

---

# IRB Requirements

## Institutional Review Board

```{r irb_checklist, echo=TRUE}
# IRB submission checklist
irb_requirements <- c(
  "Research protocol",
  "Consent forms",
  "Questionnaires",
  "Risk assessment",
  "Data management plan",
  "PI qualifications",
  "Budget justification",
  "Letters of support",
  "Training materials"
)

irb_timeline <- data.frame(
  Stage = c("Preparation", "Submission", "Review", 
           "Revision", "Approval"),
  Days = c(10, 1, 21, 5, 3),
  Cumulative = cumsum(c(10, 1, 21, 5, 3))
)

print(irb_timeline)
cat("\nTotal time needed:", max(irb_timeline$Cumulative), "days minimum")
```

---

# Data Protection

## GDPR and Privacy

```{r data_protection, echo=TRUE}
# Data protection measures
protection_measures <- list(
  Collection = c("Minimal data", "Purpose limitation", "Consent recorded"),
  Storage = c("Encryption", "Access control", "Audit logs"),
  Processing = c("Anonymization", "Aggregation", "No linking"),
  Retention = c("Time limits", "Destruction plan", "Archive rules"),
  Rights = c("Access request", "Correction", "Deletion")
)

# Create data protection impact assessment
dpia_score <- data.frame(
  Aspect = names(protection_measures),
  Risk_Level = c(2, 3, 2, 1, 2),
  Controls = sapply(protection_measures, length),
  Residual_Risk = c("Low", "Medium", "Low", "Low", "Low")
)

print(dpia_score)
```

---

# Dissemination Planning

## Sharing Results

### Target Audiences:

```{r dissemination, echo=FALSE}
audiences <- data.frame(
  Audience = c("Policy makers", "Researchers", "Media", 
              "Donors", "Public", "Participants"),
  Format = c("Brief", "Report", "Press release",
            "Presentation", "Infographic", "Summary"),
  Channel = c("Meeting", "Journal", "Conference",
             "Workshop", "Website", "Mail/SMS"),
  Timing = c("First", "After review", "Launch",
            "Quarterly", "Launch", "After analysis")
)

kable(audiences) %>%
  kable_styling(bootstrap_options = "striped", font_size = 11)
```

---

# Creating Impact

## Beyond Numbers

### Strategies for Impact:

1. **Clear Messages**
   - One-page key findings
   - Three main points
   - Action recommendations

2. **Visual Communication**
   - Infographics
   - Interactive dashboards
   - Story maps

3. **Stakeholder Engagement**
   - Early involvement
   - Regular updates
   - Co-creation of recommendations

---

# Sustainability Planning

## Long-Term Success

```{r sustainability, echo=TRUE}
# Sustainability framework
sustainability <- data.frame(
  Component = c("Institutional capacity", "Funding model",
               "Technical systems", "Knowledge transfer",
               "Partnership network"),
  Current_State = c("Basic", "Project-based", "Manual", 
                   "Limited", "Informal"),
  Target_State = c("Advanced", "Core budget", "Automated",
                  "Systematic", "Formal MOU"),
  Timeline_Months = c(24, 36, 18, 12, 18),
  Investment_Needed = c(50000, 100000, 30000, 20000, 10000)
)

print(sustainability)

cat("\nTotal investment needed: $", sum(sustainability$Investment_Needed))
```

---

# Lessons from Failures

## What Can Go Wrong

### Case 1: Frame Disaster
- Old census data (10 years)
- 40% coverage missing
- Result: Biased estimates
- **Lesson:** Invest in frame updates

### Case 2: Technology Failure  
- Tablets crashed in field
- No paper backup
- Lost 3 days of data
- **Lesson:** Always have Plan B

### Case 3: Budget Overrun
- Transport costs doubled
- No contingency fund
- Reduced sample by 30%
- **Lesson:** Realistic budgeting

---

# Success Stories

## Learning from Excellence

### Rwanda DHS Success Factors:
✅ Strong government support
✅ Excellent training program
✅ Community mobilization
✅ Real-time monitoring
✅ 98.9% response rate!

### South Africa Innovation:
✅ Rotating panel design
✅ Quarterly updates
✅ Provincial estimates
✅ Policy influence

### Nigeria Scale Achievement:
✅ 33,000+ households
✅ 37 strata managed
✅ Tablet-based collection
✅ Timely completion

---

# Advanced Analytics

## Beyond Basic Estimates

```{r advanced_analytics, echo=TRUE}
# Advanced analytical techniques
library(survey)

# 1. Quantile regression
# qr_model <- svyglm(income ~ age + education, 
#                   design, family = quantile(0.5))

# 2. Multilevel modeling
# library(lme4)
# ml_model <- lmer(income ~ age + (1|cluster), 
#                 data = survey_data, weights = weight)

# 3. Propensity score matching
# library(MatchIt)
# matched <- matchit(treatment ~ age + income,
#                   data = survey_data, weights = weight)

# 4. Machine learning with weights
# library(ranger)
# rf_weighted <- ranger(income ~ ., data = survey_data,
#                      case.weights = weight)

cat("Advanced methods available for complex analysis")
```

---

# Seasonal Adjustment

## Accounting for Timing

```{r seasonal, echo=TRUE}
# Seasonal effects on surveys
seasonal_factors <- data.frame(
  Month = month.abb,
  Agricultural = c(0.8, 0.8, 0.9, 1.0, 1.2, 1.3,
                  1.3, 1.2, 1.1, 1.0, 0.9, 0.8),
  School = c(1.0, 1.0, 1.0, 1.0, 0.7, 0.6,
            0.6, 0.7, 1.0, 1.0, 1.0, 1.0),
  Income = c(0.9, 0.9, 0.95, 1.0, 1.0, 1.1,
            1.1, 1.0, 1.0, 1.0, 0.95, 1.2)
)

# Plot seasonal patterns
matplot(1:12, seasonal_factors[, -1], type = "l", lwd = 2,
        xlab = "Month", ylab = "Seasonal Factor",
        main = "Seasonal Effects on Survey Variables")
legend("topright", legend = names(seasonal_factors)[-1],
       col = 1:3, lty = 1:3, lwd = 2)
```

---

# Panel Data Analysis

## Longitudinal Insights

```{r panel_data, echo=TRUE}
# Simulate panel data
panel <- data.frame(
  household_id = rep(1:500, each = 4),
  wave = rep(1:4, 500),
  income = 40000 + rep(rnorm(500, 0, 5000), each = 4) +
          rnorm(2000, 0, 2000) + 
          rep(1:4, 500) * 500  # Growth trend
)

# Analyze transitions
transitions <- panel %>%
  arrange(household_id, wave) %>%
  group_by(household_id) %>%
  mutate(income_change = income - lag(income)) %>%
  ungroup()

# Poverty dynamics
panel$poor <- panel$income < 35000
poverty_transitions <- table(
  Previous = panel$poor[panel$wave == 1],
  Current = panel$poor[panel$wave == 4]
)

print(poverty_transitions)
```

---

# Attrition in Panels

## Managing Loss Over Time

```{r attrition, echo=TRUE}
# Attrition analysis
waves <- 1:6
initial_sample <- 2000
attrition_rate <- 0.05  # Per wave

remaining <- numeric(length(waves))
remaining[1] <- initial_sample

for(i in 2:length(waves)) {
  remaining[i] <- remaining[i-1] * (1 - attrition_rate)
}

attrition_data <- data.frame(
  Wave = waves,
  Sample = round(remaining),
  Lost = c(0, round(diff(remaining))),
  Cumulative_Loss = initial_sample - round(remaining),
  Percent_Remaining = round(100 * remaining / initial_sample, 1)
)

print(attrition_data)

# Need refreshment sample at wave 4
cat("\nRecommend refreshment at wave 4")
```

---

# Synthetic Populations

## Creating Test Data

```{r synthetic_pop, echo=TRUE}
# Generate realistic synthetic population
create_synthetic_population <- function(n = 10000) {
  
  # Age distribution (realistic)
  age <- round(rbeta(n, 2, 5) * 80 + 18)
  
  # Income depends on age
  income <- exp(8 + 0.05 * age - 0.0005 * age^2 + 
               rnorm(n, 0, 0.5))
  
  # Education correlated with income
  education <- pmin(20, pmax(0, 
    round(5 + income/10000 + rnorm(n, 0, 2))))
  
  # Urban probability increases with education
  urban <- rbinom(n, 1, plogis(education/10 - 1))
  
  # Household size
  hh_size <- rpois(n, ifelse(urban, 3, 4.5))
  
  return(data.frame(
    age = age,
    income = round(income),
    education = education,
    urban = urban,
    hh_size = pmax(1, hh_size)
  ))
}

# Generate and check
pop <- create_synthetic_population(1000)
summary(pop)
```

---

# Final Day 1 Review

## Complete Coverage Achieved

### Morning (150 slides):
✅ Fundamentals mastered
✅ Stratification optimized
✅ Clustering understood
✅ Multi-stage designed

### Afternoon (150+ slides):
✅ TSE framework
✅ Variance methods
✅ Weighting complete
✅ Quality assured
✅ Field operations
✅ Advanced topics

### Total: 300+ slides of comprehensive training!

---

# Your Accomplishments Today

## What You Can Now Do:

### Design Phase:
✅ Choose optimal design
✅ Calculate sample sizes
✅ Allocate efficiently
✅ Plan operations

### Implementation:
✅ Train teams
✅ Monitor quality
✅ Handle problems
✅ Manage budgets

### Analysis:
✅ Use survey package
✅ Calculate weights
✅ Estimate variance
✅ Report properly

---

# Tomorrow's Adventure

## Day 2: When Frames Fail

### Harry's Tuesday Crisis:
The frame is worse than expected!
- Buildings demolished
- New settlements unmapped  
- Boundaries changed
- Lists outdated

### You'll Learn:
- Frame assessment methods
- Dual frame solutions
- Area sampling
- GIS integration
- Update procedures

### Come prepared with:
- Your frame challenges
- Ideas for solutions

---

# Final Reminders

## Before You Leave:

### ✓ Checklist:
1. Save all R scripts
2. Document your notes
3. Complete exercises
4. Start homework

### 📚 Tonight's Homework:
- Design survey for your context
- Run Day 1 scripts
- Prepare questions

### ⏰ Tomorrow:
- 8:00 AM sharp
- Bring laptop charged
- Frame problems ready

---

# Thank You!

## Outstanding Participation!

### You've shown:
- Excellent questions
- Active engagement
- Peer support
- Real dedication

### Harry is impressed!
> "This team will conquer any survey challenge!"

### See you tomorrow for Day 2!

## Questions? 
### I'll stay for 15 minutes

---
class: center, middle

# Additional Case Studies

## Kenya KIHBS 2015/16

### Design Overview

```{r kenya_case, echo=TRUE}
# Kenya Integrated HH Budget Survey
kenya_design <- data.frame(
  Component = c("Sample size", "Clusters", "Strata", "Duration"),
  Details = c("24,000 HH", "2,400 clusters", "92 strata", "12 months"),
  Feature = c("Large scale", "10 HH/cluster", "County level", "Continuous")
)

print(kenya_design)
```

---

# Kenya Innovations

## What Made It Special

### Continuous Data Collection:
- Spread across 12 months
- Captures seasonality
- Reduces recall bias

### Technology Use:
- CAPI for all interviews
- Real-time data transmission
- GPS coordinate capture

### Integration:
- Multiple modules
- Labor force quarterly
- Agriculture seasonal

---

# Tanzania HBS 2017/18

## Household Budget Survey

```{r tanzania_case, echo=TRUE}
# Tanzania design parameters
tanzania <- data.frame(
  Region = c("Dar es Salaam", "Other Urban", "Rural"),
  Clusters = c(80, 220, 500),
  HH_per_cluster = c(12, 12, 12),
  Total_HH = c(960, 2640, 6000)
)

tanzania$Percent <- round(100 * tanzania$Total_HH / 
                          sum(tanzania$Total_HH), 1)

print(tanzania)
```

---

# Ghana GLSS 7

## Living Standards Survey

### Key Features:
- Panel component (50%)
- New households (50%)
- 15,000 total households
- Regional estimates

```{r ghana_panel, echo=TRUE}
# Panel design structure
ghana_panel <- data.frame(
  Wave = 1:3,
  Panel_HH = c(0, 7500, 7500),
  New_HH = c(15000, 7500, 7500),
  Total = c(15000, 15000, 15000)
)

print(ghana_panel)
```

---

# Ethiopia ESS 2018/19

## Socioeconomic Survey

### Agricultural Focus:
- Post-planting visit
- Post-harvest visit
- Livestock module
- Community questionnaire

```{r ethiopia_design, echo=FALSE}
ethiopia <- data.frame(
  Visit = c("Post-planting", "Post-harvest", "Livestock"),
  Timing = c("Sept-Oct", "Feb-Mar", "Continuous"),
  Modules = c(8, 10, 5),
  Duration_min = c(120, 150, 60)
)

kable(ethiopia) %>%
  kable_styling(bootstrap_options = "striped")
```

---

# Mozambique IOF 2019/20

## Budget Survey Challenges

### Unique Challenges:
1. Cyclone impact areas
2. Security concerns
3. Multiple languages
4. Low literacy

### Solutions:
- Flexible fieldwork
- Local recruitment
- Picture cards
- Extra training

---

# Uganda UNHS 2019/20

## National Household Survey

```{r uganda_quality, echo=TRUE}
# Quality indicators achieved
uganda_quality <- data.frame(
  Indicator = c("Response rate", "Coverage", "Item response",
               "Data quality"),
  Target = c(90, 95, 95, 95),
  Achieved = c(93.2, 96.8, 96.5, 94.3),
  Status = c("✓", "✓", "✓", "✗")
)

print(uganda_quality)
```

---

# Detailed Sampling Steps

## Step 1: Define Population

```{r sampling_step1, echo=TRUE}
# Clear population definition
population_def <- list(
  target = "All households",
  geographic = "National territory",
  exclusions = c("Diplomatic", "Military", "Institutional"),
  time_ref = "Usual residents",
  total_HH = 2500000
)

str(population_def)
```

---

# Sampling Step 2

## Frame Assessment

```{r sampling_step2, echo=TRUE}
# Assess frame quality
frame_assessment <- function(frame_data) {
  checks <- list(
    completeness = sum(!is.na(frame_data$id)) / nrow(frame_data),
    duplicates = any(duplicated(frame_data$id)),
    currency = Sys.Date() - max(frame_data$update_date),
    geocoded = mean(!is.na(frame_data$lat))
  )
  return(checks)
}

# Example assessment
# results <- frame_assessment(master_frame)
```

---

# Sampling Step 3

## Stratification Design

```{r sampling_step3, echo=TRUE}
# Design stratification scheme
create_strata <- function(frame, variables) {
  
  # Create strata combinations
  frame$stratum <- interaction(frame[, variables])
  
  # Check strata sizes
  strata_sizes <- table(frame$stratum)
  
  # Collapse small strata
  small_strata <- names(strata_sizes[strata_sizes < 100])
  
  # Report
  cat("Total strata:", length(strata_sizes), "\n")
  cat("Small strata:", length(small_strata))
  
  return(frame)
}
```

---

# Sampling Step 4

## Sample Size Determination

```{r sampling_step4, echo=TRUE}
# Complete sample size calculation
calculate_sample_size <- function(CV = 0.05, P = 0.5, 
                                 DEFF = 1.5, RR = 0.80) {
  
  # Simple random sample size
  z <- 1.96  # 95% confidence
  n_srs <- (z^2 * P * (1-P)) / (CV * P)^2
  
  # Adjust for design effect
  n_complex <- n_srs * DEFF
  
  # Adjust for non-response
  n_final <- ceiling(n_complex / RR)
  
  return(list(
    SRS = round(n_srs),
    With_DEFF = round(n_complex),
    Final = n_final
  ))
}

calculate_sample_size()
```

---

# Sampling Step 5

## PSU Selection

```{r sampling_step5, echo=TRUE}
# Select PSUs with PPS
select_psu_pps <- function(frame, n_psu) {
  
  # Calculate cumulative size
  frame <- frame %>%
    arrange(stratum, psu_id) %>%
    group_by(stratum) %>%
    mutate(
      cum_size = cumsum(size),
      total_size = max(cum_size)
    )
  
  # Systematic PPS within strata
  selected <- frame %>%
    group_by(stratum) %>%
    mutate(
      interval = total_size / n_psu,
      start = runif(1, 0, interval)
    )
  
  return(selected)
}
```

---

# More R Package Details

## survey Package Functions

```{r survey_functions, echo=TRUE, eval=FALSE}
# Complete survey package toolkit

# Design specification
svydesign()         # Specify complex design
svrepdesign()       # Replicate weight design
twophase()          # Two-phase design

# Estimation functions
svymean()           # Means with SE
svytotal()          # Population totals
svyquantile()       # Percentiles
svyratio()          # Ratios
svyby()             # Domain estimation
svytable()          # Cross-tabulations

# Regression
svyglm()            # Generalized linear models
svycoxph()          # Cox proportional hazards
svyolr()            # Ordinal logistic
```

---

# More survey Functions

## Advanced Capabilities

```{r survey_advanced, echo=TRUE, eval=FALSE}
# Advanced survey functions

# Variance estimation
svycontrast()       # Linear combinations
svyvar()            # Variance of variables
CV()                # Coefficient of variation
confint()           # Confidence intervals
degf()              # Degrees of freedom

# Calibration and raking
calibrate()         # Calibrate to known totals
rake()              # Iterative proportional fitting
postStratify()      # Post-stratification
trimWeights()       # Weight trimming

# Graphics
svyplot()           # Scatterplots
svyhist()           # Histograms
svyboxplot()        # Box plots
```

---

# Data Preparation Tips

## Before Analysis

```{r data_prep_tips, echo=TRUE}
# Essential data preparation steps
prepare_survey_data <- function(raw_data) {
  
  # 1. Check for structural issues
  str(raw_data)
  
  # 2. Handle missing values
  raw_data %>%
    mutate(across(where(is.numeric), 
                 ~ifelse(. < 0, NA, .)))
  
  # 3. Create derived variables
  raw_data$age_group <- cut(raw_data$age,
                           c(0, 18, 35, 50, 65, Inf))
  
  # 4. Check weights
  summary(raw_data$weight)
  
  return(raw_data)
}
```

---

# Common Analysis Errors

## What to Avoid

### Error 1: Ignoring Design
```{r error_example1, echo=TRUE, eval=FALSE}
# WRONG - treats as simple random sample
mean(data$income)

# CORRECT - accounts for design
svymean(~income, design)
```

### Error 2: Subsetting Design
```{r error_example2, echo=TRUE, eval=FALSE}
# WRONG - loses design information
urban_design <- subset(design, area == "urban")

# CORRECT - subset in analysis
svymean(~income, subset(design, area == "urban"))
```

---

# More Common Errors

### Error 3: Wrong Variance Formula
```{r error_example3, echo=TRUE, eval=FALSE}
# WRONG - uses SRS formula
se_wrong <- sd(data$income) / sqrt(nrow(data))

# CORRECT - uses design-based SE
se_correct <- SE(svymean(~income, design))
```

### Error 4: Forgetting Weights
```{r error_example4, echo=TRUE, eval=FALSE}
# WRONG - unweighted regression
lm(income ~ age + education, data = data)

# CORRECT - weighted regression
svyglm(income ~ age + education, design)
```

---

# Debugging Survey Analysis

## When Things Go Wrong

```{r debugging_tips, echo=TRUE}
# Common debugging approaches

# 1. Check design object
check_design <- function(design) {
  print(summary(design))
  cat("\nNumber of strata:", length(unique(design$strata)))
  cat("\nNumber of PSUs:", length(unique(design$cluster)))
  cat("\nWeight range:", range(weights(design)))
}

# 2. Verify lonely PSUs
check_lonely_psu <- function(design) {
  psu_count <- table(design$strata, design$cluster)
  lonely <- apply(psu_count > 0, 1, sum) == 1
  if(any(lonely)) {
    cat("Lonely PSUs in strata:", 
        names(lonely)[lonely])
  }
}
```

---

# Performance Optimization

## Speed Up Analysis

```{r performance_tips, echo=TRUE, eval=FALSE}
# Tips for faster analysis

# 1. Use data.table for large datasets
library(data.table)
dt <- as.data.table(survey_data)

# 2. Parallel processing for replication
library(parallel)
options(survey.multicore = TRUE)
options(mc.cores = 4)

# 3. Subset before creating design
subset_first <- survey_data[survey_data$age >= 18, ]
design_adults <- svydesign(~cluster, ~stratum, 
                          weights = ~weight,
                          data = subset_first)

# 4. Use update() for multiple models
base_model <- svyglm(income ~ age, design)
full_model <- update(base_model, . ~ . + education)
```

---

# Memory Management

## Large Surveys

```{r memory_management, echo=TRUE, eval=FALSE}
# Handle large survey datasets

# 1. Monitor memory usage
print(object.size(design), units = "MB")
memory.profile()

# 2. Use database-backed designs
library(survey)
library(RSQLite)

# Create database
con <- dbConnect(SQLite(), "survey.db")
dbWriteTable(con, "survey", survey_data)

# Database-backed design
db_design <- svydesign(
  ids = ~cluster,
  strata = ~stratum,
  weights = ~weight,
  data = "survey",
  dbtype = "SQLite",
  dbname = "survey.db"
)
```

---

# Creating Custom Functions

## Your Survey Toolkit

```{r custom_toolkit, echo=TRUE}
# Build your analysis toolkit
my_survey_tools <- list(
  
  # Quick summary function
  quick_summary = function(var, design) {
    est <- svymean(as.formula(paste("~", var)), 
                   design, na.rm = TRUE)
    data.frame(
      Variable = var,
      Estimate = as.numeric(est),
      SE = as.numeric(SE(est)),
      CV = as.numeric(cv(est))
    )
  },
  
  # Domain comparison
  compare_domains = function(var, domain, design) {
    svyby(as.formula(paste("~", var)),
          as.formula(paste("~", domain)),
          design, svymean)
  }
)
```

---

# Reporting Templates

## Standardized Output

```{r report_template, echo=TRUE}
# Standard reporting template
generate_standard_report <- function(design, variables) {
  
  report <- list()
  
  # Population total
  report$total <- svytotal(~one, design)
  
  # Key estimates
  for(var in variables) {
    report[[var]] <- list(
      mean = svymean(as.formula(paste("~", var)), 
                     design, na.rm = TRUE),
      total = svytotal(as.formula(paste("~", var)), 
                       design, na.rm = TRUE),
      quantiles = svyquantile(as.formula(paste("~", var)),
                             design, c(0.25, 0.5, 0.75))
    )
  }
  
  return(report)
}
```

---

# Archiving and Documentation

## Future-Proof Your Work

```{r archiving, echo=TRUE, eval=FALSE}
# Complete project archive
archive_project <- function(project_name) {
  
  # Create archive structure
  dir.create(paste0(project_name, "_archive"))
  
  # Save all objects
  save.image(paste0(project_name, "_archive/workspace.RData"))
  
  # Document versions
  sink(paste0(project_name, "_archive/session_info.txt"))
  print(sessionInfo())
  sink()
  
  # Copy scripts
  file.copy(list.files(pattern = "*.R"), 
           paste0(project_name, "_archive/"))
  
  # Create README
  readme <- paste(
    "Project:", project_name,
    "\nDate:", Sys.Date(),
    "\nAnalyst:", Sys.info()["user"],
    "\n\nFiles included:",
    "\n- workspace.RData: All R objects",
    "\n- session_info.txt: Package versions",
    "\n- *.R: Analysis scripts"
  )
  
  writeLines(readme, 
            paste0(project_name, "_archive/README.txt"))
}
```

---

# Additional Exercises

## Practice Problems

### Exercise A: Design Effect
Calculate DEFF for:
- Cluster size = 25
- ICC = 0.12

### Exercise B: Sample Size
What sample size needed for:
- National CV = 3%
- DEFF = 2.1
- Response rate = 75%

### Exercise C: Weight Trimming
Trim weights at 1st and 99th percentiles

---

# Exercise Solutions

## Check Your Answers

### Solution A:
```{r solution_a, echo=TRUE}
m <- 25
icc <- 0.12
deff <- 1 + (m - 1) * icc
cat("DEFF =", deff)
```

### Solution B:
```{r solution_b, echo=TRUE}
cv <- 0.03
deff <- 2.1
rr <- 0.75
n <- ((1.96 / (cv * 2))^2 * deff) / rr
cat("Sample size =", ceiling(n))
```

---

# Real-World Constraints

## When Theory Meets Practice

### Budget Cuts Mid-Survey:
- Reduce cluster size
- Drop least important modules
- Prioritize core indicators

### Security Deteriorates:
- Replace unsafe PSUs
- Use phone interviews
- Adjust weights accordingly

### Technology Fails:
- Paper backup essential
- Train on both systems
- Double data entry

---

# Lessons from the Field

## Experienced Advice

### From Kenya:
> "Always have Plan B, C, and D"

### From Nigeria:
> "Community entry is everything"

### From South Africa:
> "Technology helps but people matter more"

### From Rwanda:
> "Quality supervisors worth their weight in gold"

---

# Building Survey Capacity

## Institutional Development

```{r capacity_building, echo=FALSE}
capacity_framework <- data.frame(
  Level = c("Individual", "Unit", "Organizational", "System"),
  Current = c("Basic", "Developing", "Limited", "Fragmented"),
  Target = c("Expert", "Professional", "Mature", "Integrated"),
  Actions = c("Training", "Procedures", "Governance", "Coordination")
)

kable(capacity_framework) %>%
  kable_styling(bootstrap_options = "striped")
```

---

# Partnerships and Collaboration

## Working Together

### Key Partners:
- Statistics offices
- Line ministries
- Development partners
- Academia
- Private sector

### Success Factors:
- Clear roles
- Shared objectives
- Regular communication
- Joint ownership

---

# Innovation in Surveys

## Future Directions

### Emerging Technologies:
- AI for data quality
- Satellite imagery
- Mobile phone data
- Social media analytics

### Methodological Advances:
- Adaptive designs
- Small area estimation
- Data integration
- Real-time monitoring

---

# Climate and Surveys

## Environmental Considerations

### Carbon Footprint:
- Minimize travel
- Digital data collection
- Remote training options
- Virtual dissemination

### Climate Data:
- Weather impacts
- Seasonal adjustments
- Disaster preparedness
- Resilience indicators

---

# Gender in Survey Design

## Inclusive Approaches

### Considerations:
- Female interviewers
- Safe spaces
- Appropriate timing
- Sensitive topics
- Proxy rules
- Intra-household dynamics

```{r gender_indicators, echo=TRUE}
# Gender-sensitive indicators
gender_vars <- c("decision_making", "asset_ownership",
                "time_use", "violence_experience",
                "economic_participation")

# Special protocols needed
print(gender_vars)
```

---

# Disability Inclusion

## Washington Group Questions

```{r disability, echo=TRUE}
# Standard disability questions
wg_questions <- data.frame(
  Domain = c("Seeing", "Hearing", "Walking", 
            "Remembering", "Self-care", "Communicating"),
  Question = rep("Do you have difficulty...", 6),
  Response = rep("None/Some/A lot/Cannot", 6)
)

print(wg_questions[1:3, ])
```

---

# Youth and Surveys

## Special Considerations

### Challenges:
- Consent requirements
- Appropriate language
- Attention span
- Technology comfort
- Privacy concerns

### Solutions:
- Parental consent forms
- Youth-friendly materials
- Shorter instruments
- Digital options
- Safe spaces

---

# Elderly Populations

## Adapted Approaches

```{r elderly_adaptations, echo=FALSE}
elderly_adapt <- data.frame(
  Challenge = c("Vision", "Hearing", "Mobility", "Memory", "Technology"),
  Adaptation = c("Large print", "Clear speech", "Home visits", 
                "Simpler questions", "Paper option")
)

kable(elderly_adapt) %>%
  kable_styling(bootstrap_options = "striped")
```

---

# Final Wisdom

## Harry's Top 10 Lessons

1. **Perfect is enemy of good**
2. **Document everything**
3. **Pilot, pilot, pilot**
4. **Train more than you think**
5. **Monitor continuously**
6. **Communicate clearly**
7. **Plan for problems**
8. **Value your team**
9. **Learn from mistakes**
10. **Celebrate successes**

---

# Your Action Plan

## Next Steps

### Immediate (This Week):
- Review all scripts
- Complete exercises
- Document questions

### Short-term (Month):
- Apply to your survey
- Share with colleagues
- Build toolkit

### Long-term (Year):
- Implement improvements
- Train others
- Share results

---

# Resources and References

## Essential Reading

### Books:
- Lohr (2019): Sampling Design & Analysis
- Lumley (2010): Complex Surveys
- Groves et al. (2009): Survey Methodology

### Online:
- [R Survey Package](http://r-survey.r-forge.r-project.org/)
- [DHS Program](https://dhsprogram.com/)
- [World Bank LSMS](https://www.worldbank.org/lsms)

---

# Community and Support

## Stay Connected

### Online Communities:
- R-help mailing list
- Stack Overflow
- Cross Validated
- Twitter #rstats

### Professional Networks:
- International Statistical Institute
- American Association for Public Opinion Research
- Royal Statistical Society
- National statistical associations

---

# Acknowledgments

## Thank You To:

- Workshop organizers
- SADC Secretariat
- Your organizations
- Fellow participants
- Support staff

## Special Thanks:
Harry - our learning companion!

---

# Feedback Form

## Help Us Improve

### Please Rate (1-5):
1. Content relevance
2. Presentation clarity
3. Exercise utility
4. Pace appropriateness
5. Materials quality

### Comments:
What worked well?
What needs improvement?
What topics to add?

---

# Certificates

## Recognition

### You've Earned:
- Certificate of Completion
- 8 CPD hours
- Digital badge
- Resource access
- Network membership

### Requirements Met:
✓ Attendance
✓ Participation
✓ Exercises
✓ Group work

---

# Stay in Touch

## Continuing the Journey

### Connect With Me:
- Email: trainer@example.com
- LinkedIn: /in/trainer
- Twitter: @trainer

### Resources Available:
- Slides (PDF)
- R scripts
- Sample data
- Reading list
- Video recordings

---

# Final Motivational Message

## You're Ready!

### Remember:
> "The expert in anything was once a beginner"

### You Now Have:
- Strong foundation
- Practical tools
- Network of peers
- Confidence to tackle challenges

### Go Forth and Sample Well!

---

# Bonus: R Shortcuts

## Time Savers

```{r r_shortcuts, echo=TRUE, eval=FALSE}
# Useful shortcuts

# Pipe operator
library(magrittr)
data %>% 
  filter(age > 18) %>%
  group_by(region) %>%
  summarise(mean_income = mean(income))

# Multiple variables
svymean(~income + age + education, design)

# All numeric variables
svymean(~., design[, sapply(design$variables, is.numeric)])
```

---

# Bonus: Visualization Gallery

## Chart Templates

```{r viz_gallery, echo=TRUE, fig.height=3, eval=FALSE}
# Publication-ready charts

# Weighted histogram
svyhist(~income, design, 
        main = "Income Distribution",
        col = "steelblue")

# Comparative boxplot
svyboxplot(income ~ region, design,
          col = "lightgreen")

# Scatterplot with weights
svyplot(income ~ age, design,
       style = "bubble")
```

---

# The Very End

## 🎊 Congratulations! 🎊

### You've Completed:
- **300+ slides**
- **Complete Day 1 training**
- **Foundation for advanced sampling**

### Tomorrow Promises:
- Frame assessment techniques
- Dual frame methods
- Modern solutions
- More of Harry's adventure!

## See You at 8:00 AM Sharp!

### Rest Well - You've Earned It!

---

# Harry's Day 1 Summary

## What Harry Learned

### Morning Insights:
✅ "Stratification is powerful but needs good variables"

✅ "Clusters save money but reduce precision predictably"

✅ "PPS makes multi-stage designs self-weighting"

### Afternoon Revelations:
✅ "Total Survey Error is more than sampling error"

✅ "Weights are essential for valid inference"

✅ "Documentation is as important as analysis"

### Key Realization:
> "Good survey design is about informed trade-offs"

---

# Your Day 1 Achievements

## Skills Gained

### Technical:
- Design complex surveys
- Calculate sample sizes
- Estimate variance properly
- Construct and trim weights

### Practical:
- Balance cost and precision
- Handle real constraints
- Document decisions
- Use R effectively

### Strategic:
- Think about total error
- Plan for quality
- Anticipate problems
- Communicate clearly

---

# Common Mistakes to Avoid

## Learn from Others

### Design Phase:
❌ Ignoring operational constraints
❌ Over-stratifying
❌ Forgetting about non-response
❌ Not piloting

### Implementation:
❌ Poor interviewer training
❌ Inadequate supervision
❌ No quality checks
❌ Missing documentation

### Analysis:
❌ Ignoring design features
❌ Wrong variance estimation
❌ Subsetting incorrectly
❌ Not reporting uncertainty

---

# Quick Reference Card

## Essential Formulas

```{r reference_card, echo=FALSE}
formulas <- data.frame(
  Concept = c("Design Effect", "Effective n", "Base Weight",
             "Standard Error", "Sample Size (SRS)", "Neyman Allocation"),
  Formula = c("DEFF = 1 + (m-1)ρ", 
             "n_eff = n / DEFF",
             "w = 1/π = N/n",
             "SE = s/√n × √DEFF",
             "n = z²p(1-p)/e²",
             "n_h = n × N_h×S_h / Σ(N_h×S_h)")
)

kable(formulas) %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
```

Print this slide for reference!

---

# Tomorrow's Preview in Detail

## Day 2: When Frames Fail

### Harry's New Crisis:
- Frame is 5 years old
- 30% coverage missing
- Duplicate listings everywhere
- Geographic codes wrong
- New settlements unmapped

### What You'll Learn:
- Frame quality assessment
- Dual/Multiple frame methods
- Area sampling techniques
- Updating procedures
- Handling duplicates
- Coverage evaluation

---

# Homework Assignment

## Complete Tonight

### Part 1: Design (1 hour)
1. Create survey design for your country
2. Justify all choices
3. Calculate required sample size
4. Estimate costs

### Part 2: Analysis (30 min)
1. Run all today's exercises
2. Modify for your context
3. Document challenges

### Part 3: Reflection (30 min)
1. Write 3 key learnings
2. List 2 remaining questions
3. Identify 1 application

### Submit by 8:00 AM tomorrow!

---

# Optional Evening Reading

## Deepen Your Knowledge

### Essential Papers:
1. Kish (1965) - Survey Sampling foundations
2. Särndal et al. (1992) - Model-assisted approach
3. Little (2004) - Calibration and weights

### Online Resources:
- [UCLA IDRE](https://stats.idre.ucla.edu/stata/seminars/survey-data-analysis/)
- [Penn State STAT 506](https://online.stat.psu.edu/stat506/)
- [CDC NHANES Tutorials](https://wwwn.cdc.gov/nchs/nhanes/tutorials/)

### R Resources:
- [Survey Package Vignette](http://r-survey.r-forge.r-project.org/survey/)
- [srvyr Documentation](https://github.com/gergness/srvyr)

---

# Final Thoughts

## Reflection Time

### Consider:

1. **What surprised you** most today?

2. **What confirmed** what you knew?

3. **What challenged** your thinking?

4. **How will you apply** this knowledge?

### Share one insight with the group!

---

# Thank You for Day 1!

## You've Been Amazing!

### Your Engagement:
- Great questions
- Active participation
- Helpful to peers
- Open to learning

### Tomorrow at 8:00 AM:
- Bring your homework
- Prepare for frame challenges
- Ready for more R coding
- Energy and enthusiasm!

## Rest Well - See You Tomorrow!

### Questions? I'll stay for 15 minutes.

---

# End of Day 1

## 🎉 Congratulations!

### You've completed:
- 300+ slides
- 15+ exercises  
- 1 major design
- Countless insights

### You're ready for:
- Day 2 challenges
- Real-world applications
- Your own surveys

## See you tomorrow at 8:00 AM sharp!