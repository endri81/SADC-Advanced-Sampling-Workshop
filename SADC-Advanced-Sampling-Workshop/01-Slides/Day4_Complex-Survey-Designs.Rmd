---
title: "Day 4: Complex Survey Designs"
subtitle: "Complete Lecture Materials"
author: "SADC Survey Sampling Workshop"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, default-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: "%current%/%total%"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  fig.width = 10, 
  fig.height = 5,
  fig.align = "center",
  cache = FALSE,
  comment = "#>"
)

# Load required packages
library(tidyverse)
library(survey)
library(knitr)
library(kableExtra)
library(convey)

# Set theme for plots
theme_set(theme_minimal(base_size = 12))

# Create complex survey data for examples
set.seed(2024)
n_regions <- 5
n_districts <- 20
n_psus <- 500
n_households <- 10000

# Generate hierarchical structure
complex_data <- data.frame(
  region = rep(1:n_regions, each = n_households/n_regions),
  district = rep(1:n_districts, each = n_households/n_districts),
  psu_id = rep(1:n_psus, each = n_households/n_psus),
  household_id = 1:n_households,
  stringsAsFactors = FALSE
)

# Add stratification variables
complex_data$urban <- rbinom(n_households, 1, 0.4)
complex_data$stratum <- paste0("R", complex_data$region, "_", 
                               ifelse(complex_data$urban, "U", "R"))

# Add outcome variables with complex patterns
region_effects <- rnorm(n_regions, 0, 5000)
district_effects <- rnorm(n_districts, 0, 3000)
psu_effects <- rnorm(n_psus, 0, 2000)

complex_data$income <- 50000 + 
  region_effects[complex_data$region] +
  district_effects[complex_data$district %% n_districts + 1] +
  psu_effects[complex_data$psu_id] +
  complex_data$urban * 10000 +
  rnorm(n_households, 0, 8000)

complex_data$income <- pmax(complex_data$income, 5000)

# Add demographic variables
complex_data$age <- sample(18:80, n_households, replace = TRUE)
complex_data$education <- pmin(20, rpois(n_households, 10))
complex_data$employed <- rbinom(n_households, 1, 
                                plogis(-2 + 0.05 * complex_data$age - 
                                      0.001 * complex_data$age^2))

# Multi-phase indicators
complex_data$phase1 <- rbinom(n_households, 1, 0.2)
complex_data$phase2 <- ifelse(complex_data$phase1, 
                              rbinom(sum(complex_data$phase1), 1, 0.5), 0)

# Panel wave indicator
complex_data$panel_wave <- sample(0:4, n_households, replace = TRUE, 
                                 prob = c(0.4, 0.2, 0.15, 0.15, 0.1))

# Domain indicators
complex_data$domain_age <- cut(complex_data$age, 
                               breaks = c(0, 30, 50, 100),
                               labels = c("Young", "Middle", "Older"))
complex_data$small_domain <- interaction(complex_data$region, complex_data$domain_age)
```

---
class: center, middle, inverse

# Day 4: Complex Survey Designs

## Bringing It All Together

### "Real surveys are never simple"

---

## Welcome to Day 4!

### Journey So Far:

Day 1: Foundations & SRS ✅  
Day 2: Stratified Sampling ✅  
Day 3: Cluster & Multi-stage ✅  
**Day 4: Complex Designs** 👈  
Day 5: Special Topics & Future  

**Bottom line:** Today we integrate everything

---

## Today's Roadmap

```{r day4_schedule, echo=FALSE}
schedule <- data.frame(
  Part = 1:5,
  Time = c("08:00-09:30", "09:45-11:15", "11:30-13:00", 
          "14:00-15:30", "15:45-17:00"),
  Topic = c("Complex Integration", "Multi-phase & Domain",
           "Panel & Longitudinal", "Calibration & Estimation",
           "Case Studies & Practice"),
  Slides = c("1-75", "76-150", "151-225", "226-300", "301-350")
)

kable(schedule) %>%
  kable_styling(bootstrap_options = "striped", font_size = 14)
```

**Bottom line:** Master complexity today

---

## What Makes Designs Complex?

### Multiple features combined:

- ✅ Stratification AND clustering
- ✅ Multiple stages AND phases
- ✅ Unequal probabilities throughout
- ✅ Panel AND cross-sectional
- ✅ Multiple frames
- ✅ Complex estimation needs

**Bottom line:** Real surveys combine everything

---

## Complexity in Practice

```{r complexity_viz, echo=FALSE, fig.height=5}
complexity_levels <- data.frame(
  Design = c("SRS", "Stratified", "Cluster", "Two-stage",
            "Stratified\nCluster", "Multi-phase\nPanel"),
  Components = c(1, 2, 2, 3, 4, 6),
  Difficulty = c(1, 2, 3, 4, 5, 8)
)

ggplot(complexity_levels, aes(x = Components, y = Difficulty)) +
  geom_point(size = 10, color = "darkblue") +
  geom_text(aes(label = Design), size = 4, color = "white") +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(breaks = 1:8) +
  labs(x = "Number of Design Components",
       y = "Implementation Difficulty",
       title = "Survey Complexity Increases Non-linearly") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Complexity compounds

---

## Our Complex Dataset Structure

```{r show_complex_data, echo=FALSE}
structure_summary <- c(
  Total_Records = nrow(complex_data),
  Regions = length(unique(complex_data$region)),
  Districts = length(unique(complex_data$district)),
  PSUs = length(unique(complex_data$psu_id)),
  Strata = length(unique(complex_data$stratum)),
  Panel_Waves = length(unique(complex_data$panel_wave[complex_data$panel_wave > 0]))
)

data.frame(
  Metric = names(structure_summary),
  Count = structure_summary
) %>%
  kable() %>%
  kable_styling(font_size = 14)
```

**Bottom line:** Hierarchical structure with multiple features

---

## Design Effect Decomposition

```{r deff_decomposition, echo=FALSE}
deff_components <- data.frame(
  Component = c("Clustering", "Unequal weights", "Stratification",
               "Multi-stage", "Combined"),
  Contribution = c(1.8, 1.3, 0.85, 1.2, 2.34),
  Direction = c("Increase", "Increase", "Decrease", "Increase", "Net")
)

kable(deff_components) %>%
  kable_styling(font_size = 14) %>%
  row_spec(5, bold = TRUE, background = "#f39c12")
```

**Bottom line:** Net DEFF = product of components

---

## Visualizing Combined Effects

```{r combined_effects, echo=FALSE, fig.height=5}
effects <- data.frame(
  Stage = c("Base", "+Cluster", "+Weights", "+Strata", "Final"),
  DEFF = c(1.0, 1.8, 2.34, 1.99, 1.99),
  x = 1:5
)

ggplot(effects, aes(x = x, y = DEFF)) +
  geom_line(size = 2, color = "red") +
  geom_point(size = 5) +
  geom_hline(yintercept = 1, linetype = "dashed", alpha = 0.5) +
  scale_x_continuous(breaks = 1:5, labels = effects$Stage) +
  scale_y_continuous(breaks = seq(1, 2.5, 0.5)) +
  labs(x = "", y = "Design Effect",
       title = "Cumulative Design Effect") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Each feature adds complexity

---

## Combined Stratified-Cluster Design

### Most common complex design:

1. **Stratify** by region/characteristics
2. **Select PSUs** within strata (PPS)
3. **Select households** within PSUs
4. **Apply weights** for all stages

**Bottom line:** Standard for national surveys

---

## Implementation Example

```{r implementation_example, echo=FALSE}
design_params <- data.frame(
  Level = c("National", "Strata", "PSUs", "Households"),
  Count = c(1, 10, 500, 10000),
  Selection = c("Census", "Census", "PPS", "SRS"),
  Probability = c("1.0", "1.0", "Varies", "0.04")
)

kable(design_params) %>%
  kable_styling(font_size = 14)
```

**Bottom line:** Clear hierarchy

---

## Calculate Complex Weights

```{r complex_weights, echo=TRUE}
# Weight calculation for stratified cluster
strata_pop <- c(Urban = 3000000, Rural = 2000000)
strata_psus <- c(Urban = 200, Rural = 300)
selected_psus <- c(Urban = 60, Rural = 90)
hh_per_psu <- 20

# Stage 1: PSU weights
psu_prob_urban <- selected_psus["Urban"] / strata_psus["Urban"]
psu_prob_rural <- selected_psus["Rural"] / strata_psus["Rural"]

# Stage 2: HH weights (assume 100 HH per PSU average)
hh_prob <- hh_per_psu / 100

# Combined weights
weight_urban <- 1 / (psu_prob_urban * hh_prob)
weight_rural <- 1 / (psu_prob_rural * hh_prob)

c(Urban_Weight = round(weight_urban),
  Rural_Weight = round(weight_rural))
```

**Bottom line:** Weights vary by stratum

---

## Multi-Phase Designs

### When to use phases:

- **Phase 1:** Cheap, broad screening
- **Phase 2:** Expensive, detailed measurement

Examples:
- Health screening → Clinical exam
- Short form → Long form
- Self-report → Validation

**Bottom line:** Cost-efficient for expensive measures

---

## Two-Phase Design

```{r two_phase_design, echo=FALSE}
phase_design <- data.frame(
  Phase = c("Phase 1", "Phase 2"),
  Sample_Size = c(5000, 500),
  Cost_per_Unit = c("$20", "$200"),
  Total_Cost = c("$100,000", "$100,000"),
  Information = c("Basic demographics", "Detailed health exam")
)

kable(phase_design) %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Equal cost, different information

---

## Phase 1 Selection

```{r phase1_selection, echo=FALSE}
set.seed(2024)
n_phase1 <- 1000
phase1_sample <- complex_data[sample(nrow(complex_data), n_phase1), ]

phase1_summary <- data.frame(
  Measure = c("Mean Age", "% Urban", "% Employed"),
  Value = c(round(mean(phase1_sample$age), 1),
           round(mean(phase1_sample$urban) * 100, 1),
           round(mean(phase1_sample$employed) * 100, 1))
)

kable(phase1_summary) %>%
  kable_styling(font_size = 14)
```

**Bottom line:** Phase 1 gives basic info

---

## Phase 2 Selection

```{r phase2_selection, echo=FALSE}
# Select phase 2 based on phase 1 info
phase1_sample$phase1_strata <- cut(phase1_sample$income,
                                   breaks = quantile(phase1_sample$income),
                                   labels = c("Q1", "Q2", "Q3", "Q4"))

n_per_stratum <- 25
phase2_sample <- phase1_sample %>%
  group_by(phase1_strata) %>%
  sample_n(min(n(), n_per_stratum))

phase2_summary <- c(
  Phase1_n = nrow(phase1_sample),
  Phase2_n = nrow(phase2_sample),
  Selection_Rate = round(nrow(phase2_sample)/nrow(phase1_sample), 3)
)

data.frame(
  Metric = names(phase2_summary),
  Value = phase2_summary
) %>%
  kable() %>%
  kable_styling(font_size = 14)
```

**Bottom line:** Phase 2 subsample of Phase 1

---

## Two-Phase Weights

```{r two_phase_weights, echo=TRUE}
# Calculate two-phase weights
N <- nrow(complex_data)  # Population size
n1 <- 1000               # Phase 1 sample size
n2 <- 100                # Phase 2 sample size

# Phase 1 weight
w1 <- N / n1

# Phase 2 weight (conditional on Phase 1)
w2_given_1 <- n1 / n2

# Combined weight
w_total <- w1 * w2_given_1

c(Phase1_Weight = w1,
  Phase2_Given_1 = w2_given_1,
  Combined_Weight = w_total)
```

**Bottom line:** Multiply phase weights

---

## Efficiency of Two-Phase Sampling

```{r two_phase_efficiency, echo=FALSE, fig.height=5}
costs <- expand.grid(
  n1 = seq(100, 2000, 100),
  n2_ratio = c(0.1, 0.2, 0.3)
)
costs$n2 <- costs$n1 * costs$n2_ratio
costs$total_cost <- costs$n1 * 20 + costs$n2 * 200
costs$variance <- 1 / costs$n2  # Simplified

ggplot(costs, aes(x = total_cost/1000, y = 1/variance, 
                   color = factor(n2_ratio))) +
  geom_line(size = 1.5) +
  labs(x = "Total Cost ($1000s)", y = "Precision",
       color = "Phase 2\nRatio",
       title = "Two-Phase Efficiency") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Optimal phase 2 ratio ≈ 0.2

---

## Domain Estimation

### The challenge:

- Survey designed for national estimates
- Need reliable provincial/subgroup estimates
- Some domains very small
- Direct estimates unreliable

**Bottom line:** Planned domains vs unplanned

---

## Domain Sample Sizes

```{r domain_sizes, echo=FALSE}
set.seed(2024)
domain_counts <- complex_data %>%
  sample_n(2000) %>%
  group_by(region, domain_age) %>%
  summarise(n = n(), .groups = 'drop') %>%
  arrange(n)

head(domain_counts, 8) %>%
  kable() %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Some domains too small

---

## Small Domain Problem

```{r small_domain_problem, echo=FALSE, fig.height=5}
domain_counts %>%
  ggplot(aes(x = n, y = reorder(paste(region, domain_age), n))) +
  geom_bar(stat = "identity", fill = "coral") +
  geom_vline(xintercept = 30, linetype = "dashed", color = "red", size = 1) +
  labs(x = "Sample Size", y = "Domain",
       title = "Many Domains Below Minimum (n=30)") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Direct estimation fails for small domains

---

## Solutions for Small Domains

```{r small_domain_solutions, echo=FALSE}
solutions <- data.frame(
  Approach = c("Increase sample", "Collapse domains", 
              "Model-based", "Small area estimation"),
  Pros = c("Simple", "Larger n", "Borrows strength", "Best precision"),
  Cons = c("Expensive", "Loss of detail", "Assumptions", "Complex"),
  When = c("Affordable", "Similar domains", "Good covariates", "Critical needs")
)

kable(solutions) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Multiple strategies available

---

## Multi-Frame Surveys

### Using multiple sampling frames:

```{r multi_frame, echo=FALSE}
frames <- data.frame(
  Frame = c("Area frame", "List frame", "Phone frame"),
  Coverage = c("100%", "80%", "60%"),
  Quality = c("Good", "Excellent", "Fair"),
  Cost = c("High", "Medium", "Low"),
  Use = c("Rural areas", "Urban registered", "Quick surveys")
)

kable(frames) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Combine frames for better coverage

---

## Dual Frame Design

```{r dual_frame, echo=FALSE, fig.height=5}
# Create Venn diagram for dual frame
library(ggplot2)
library(ggforce)

# Create data for two circles
df <- data.frame(
  x = c(-0.5, 0.5),
  y = c(0, 0),
  r = c(1, 1),
  frame = c("Frame A", "Frame B")
)

ggplot() +
  geom_circle(data = df, aes(x0 = x, y0 = y, r = r, fill = frame), 
              alpha = 0.3, size = 1) +
  scale_fill_manual(values = c("blue", "red")) +
  annotate("text", x = -1.2, y = 0, label = "Frame A\nonly", size = 5) +
  annotate("text", x = 1.2, y = 0, label = "Frame B\nonly", size = 5) +
  annotate("text", x = 0, y = 0, label = "Overlap", size = 5) +
  coord_fixed() +
  theme_void() +
  labs(title = "Dual Frame Coverage") +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 16))
```

**Bottom line:** Handle overlap carefully

---

## Panel Survey Designs

### Cross-section vs Panel:

```{r panel_comparison, echo=FALSE}
comparison <- data.frame(
  Aspect = c("Design", "Sample", "Measurement", "Analysis", "Cost"),
  Cross_Section = c("New sample each time", "Independent", 
                   "Status at time point", "Levels only", "Lower"),
  Panel = c("Same units over time", "Dependent", 
           "Change over time", "Levels + change", "Higher")
)

kable(comparison) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Panels measure change

---

## Fixed Panel Design

```{r fixed_panel, echo=FALSE, fig.height=5}
waves <- 1:5
sample_size <- c(1000, 920, 850, 780, 720)

data.frame(Wave = waves, Size = sample_size) %>%
  ggplot(aes(x = Wave, y = Size)) +
  geom_line(size = 2, color = "darkblue") +
  geom_point(size = 4) +
  scale_x_continuous(breaks = 1:5) +
  labs(y = "Sample Size",
       title = "Fixed Panel: Attrition Over Time") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Same units, decreasing size

---

## Rotating Panel Design

```{r rotating_panel, echo=FALSE}
# Create rotation pattern
rotation <- matrix(c(
  1,1,1,1,0,0,0,0,
  0,1,1,1,1,0,0,0,
  0,0,1,1,1,1,0,0,
  0,0,0,1,1,1,1,0
), nrow = 4, byrow = TRUE)

rownames(rotation) <- paste("Panel", 1:4)
colnames(rotation) <- paste("Wave", 1:8)

kable(rotation) %>%
  kable_styling(font_size = 12) %>%
  column_spec(which(rotation[1,] == 1) + 1, background = "lightblue")
```

**Bottom line:** Systematic replacement pattern

---

## Summary: Part 1

### You've learned:

✅ How to combine design features  
✅ Multi-phase sampling benefits  
✅ Domain estimation challenges  
✅ Multi-frame approaches  
✅ Panel design concepts  

**Bottom line:** Integration is key!

---
class: center, middle, inverse

# End of Part 1

## Ready for more complexity?

### Break: 15 minutes

---

---

## Welcome Back!

### Part 2: Panel and Longitudinal Designs

Ready for:
- Panel survey concepts
- Rotation patterns
- Attrition handling
- Change estimation
- Complex calibration

**Bottom line:** Master temporal complexity

---

## Why Panel Surveys?

### Cross-section vs Panel:

```{r panel_comparison_76, echo=FALSE}
comparison <- data.frame(
  Aspect = c("Design", "Sample", "Measurement", "Analysis", "Cost"),
  Cross_Section = c("New sample each time", "Independent", 
                   "Status at time point", "Levels only", "Lower"),
  Panel = c("Same units over time", "Dependent", 
           "Change over time", "Levels + change", "Higher")
)

kable(comparison) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Panels measure change

---

## Types of Panel Designs

```{r panel_types_77, echo=FALSE}
panel_types <- data.frame(
  Type = c("Fixed Panel", "Rotating Panel", "Split Panel", "Access Panel"),
  Description = c("Same units all waves", "Gradual replacement",
                 "Part fixed, part rotating", "Volunteer pool"),
  Example = c("Birth cohort", "Labour Force Survey", 
             "EU-SILC", "Online panels"),
  Advantage = c("Pure change", "Reduces burden", "Flexibility", "Quick")
)

kable(panel_types) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Different types for different needs

---

## Fixed Panel Design

```{r fixed_panel_78, echo=FALSE, fig.height=5}
waves <- 1:5
sample_size <- c(1000, 920, 850, 780, 720)

data.frame(Wave = waves, Size = sample_size) %>%
  ggplot(aes(x = Wave, y = Size)) +
  geom_line(size = 2, color = "darkblue") +
  geom_point(size = 4) +
  scale_x_continuous(breaks = 1:5) +
  scale_y_continuous(limits = c(0, 1100)) +
  labs(y = "Sample Size",
       title = "Fixed Panel: Attrition Over Time") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Same units, decreasing size

---

## Rotating Panel Design

```{r rotating_panel_79, echo=FALSE}
# Create rotation pattern function
simulate_rotation <- function(n_waves = 8, n_panels = 4) {
  rotation_matrix <- matrix(0, nrow = n_panels, ncol = n_waves)
  for(i in 1:n_panels) {
    start <- i
    end <- min(start + n_panels - 1, n_waves)
    rotation_matrix[i, start:end] <- 1
  }
  return(rotation_matrix)
}

rotation <- simulate_rotation(8, 4)
rownames(rotation) <- paste("Panel", 1:4)
colnames(rotation) <- paste("Wave", 1:8)

kable(rotation) %>%
  kable_styling(font_size = 12) %>%
  column_spec(which(rotation[1,] == 1) + 1, background = "lightblue")
```

**Bottom line:** Systematic replacement pattern

---

## Rotation Pattern Visualization

```{r rotation_viz_80, echo=FALSE, fig.height=5}
rotation_df <- as.data.frame(rotation) %>%
  mutate(Panel = 1:4) %>%
  pivot_longer(-Panel, names_to = "Wave", values_to = "InSample") %>%
  mutate(Wave = as.numeric(gsub("Wave ", "", Wave)))

ggplot(rotation_df, aes(x = Wave, y = Panel, fill = factor(InSample))) +
  geom_tile(color = "white", size = 1) +
  scale_fill_manual(values = c("white", "darkblue"), 
                   labels = c("Out", "In")) +
  scale_x_continuous(breaks = 1:8) +
  scale_y_continuous(breaks = 1:4) +
  labs(fill = "Status", title = "4-Wave Rotating Panel Pattern") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Each panel in for 4 waves

---

## Overlap Between Waves

```{r overlap_calculation_81, echo=FALSE}
# Calculate overlap between consecutive waves
calculate_overlap <- function(rotation_matrix) {
  n_waves <- ncol(rotation_matrix)
  overlap <- numeric(n_waves - 1)
  for(i in 1:(n_waves - 1)) {
    overlap[i] <- sum(rotation_matrix[, i] * rotation_matrix[, i + 1]) / 
                  sum(rotation_matrix[, i])
  }
  return(overlap)
}

overlap <- calculate_overlap(rotation)

data.frame(
  Transition = paste("Wave", 1:7, "→", 2:8),
  Overlap = paste0(round(overlap * 100), "%")
) %>%
  kable() %>%
  kable_styling(font_size = 14)
```

**Bottom line:** 75% overlap maintains stability

---

## Panel Sample Size Planning

```{r panel_size_planning_82, echo=FALSE}
initial_n <- 1000
attrition_rate <- 0.10  # 10% per wave
n_waves <- 5

sample_sizes <- numeric(n_waves)
sample_sizes[1] <- initial_n

for(i in 2:n_waves) {
  sample_sizes[i] <- sample_sizes[i-1] * (1 - attrition_rate)
}

data.frame(
  Wave = 1:n_waves,
  Expected_n = round(sample_sizes),
  Attrition = c(0, rep(paste0(attrition_rate*100, "%"), n_waves-1)),
  Cumulative_Loss = paste0(round((1 - sample_sizes/initial_n) * 100), "%")
) %>%
  kable() %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Plan for 35-40% total attrition

---

## Initial Wave Oversample

```{r oversample_planning_83, echo=TRUE}
# Calculate initial oversample needed
target_final <- 500  # Need 500 in final wave
expected_retention <- 0.65  # 65% retained
initial_needed <- ceiling(target_final / expected_retention)

c(Target_Final_Wave = target_final,
  Expected_Retention = paste0(expected_retention * 100, "%"),
  Initial_Sample_Needed = initial_needed,
  Oversample = initial_needed - target_final)
```

**Bottom line:** Start with 770 to end with 500

---

## Attrition Patterns

```{r attrition_patterns_84, echo=FALSE, fig.height=5}
set.seed(2024)
waves <- 1:6
constant <- 100 * (0.9)^(waves-1)
decreasing <- 100 * (0.95)^(waves-1) * (0.98)^(waves-1)
increasing <- 100 - cumsum(c(0, 5, 8, 12, 15, 20))

attrition_data <- data.frame(
  Wave = rep(waves, 3),
  Retained = c(constant, decreasing, increasing),
  Pattern = rep(c("Constant", "Decreasing", "Increasing"), each = 6)
)

ggplot(attrition_data, aes(x = Wave, y = Retained, color = Pattern)) +
  geom_line(size = 1.5) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Set1") +
  scale_y_continuous(limits = c(0, 100)) +
  labs(y = "% Retained",
       title = "Different Attrition Patterns") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Early waves most critical

---

## Attrition Bias

```{r attrition_bias_85, echo=FALSE}
attrition_profile <- data.frame(
  Characteristic = c("Young adults", "Mobile", "Low education",
                    "Urban", "High income"),
  Attrition_Risk = c("High", "High", "Medium", "Medium", "Low"),
  Impact = c("Age bias", "Stability bias", "Education bias",
            "Geographic bias", "Income bias"),
  Solution = c("Targeted retention", "Tracking", "Simplify",
              "Local interviewers", "Incentives")
)

kable(attrition_profile) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Attrition not random

---

## Retention Strategies

```{r retention_strategies_86, echo=FALSE}
strategies <- data.frame(
  Strategy = c("Incentives", "Tracking", "Engagement", 
              "Flexibility", "Relationship"),
  Implementation = c("Increasing payments", "Update contacts",
                    "Newsletters, results", "Multiple modes",
                    "Same interviewer"),
  Effectiveness = c("High", "High", "Medium", "Medium", "High"),
  Cost = c("High", "Medium", "Low", "Medium", "Medium")
)

kable(strategies) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Multiple strategies needed

---

## Panel Conditioning

### Respondents change behavior:

```{r panel_conditioning_87, echo=FALSE, fig.height=5}
waves <- 1:5
true_value <- rep(50, 5)
reported <- c(50, 48, 45, 44, 43)  # Decreasing due to conditioning

data.frame(Wave = waves, True = true_value, Reported = reported) %>%
  pivot_longer(-Wave, names_to = "Type", values_to = "Value") %>%
  ggplot(aes(x = Wave, y = Value, color = Type)) +
  geom_line(size = 1.5) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Set2") +
  labs(y = "Reported Value",
       title = "Panel Conditioning Effect") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Participation affects responses

---

## Testing for Conditioning

```{r test_conditioning_88, echo=TRUE}
# Compare panel to fresh cross-section
comparison_test <- data.frame(
  Wave = c(3, 3),
  Sample = c("Panel Wave 3", "Fresh Cross-section"),
  Mean = c(45.2, 49.8),
  SE = c(1.2, 1.5),
  n = c(750, 500)
)

# Test difference
diff <- comparison_test$Mean[2] - comparison_test$Mean[1]
se_diff <- sqrt(sum(comparison_test$SE^2))
t_stat <- diff / se_diff

c(Difference = diff,
  SE_Difference = round(se_diff, 2),
  t_statistic = round(t_stat, 2),
  Significant = abs(t_stat) > 1.96)
```

**Bottom line:** Significant conditioning detected

---

## Panel Weights

```{r panel_weights_89, echo=FALSE}
cat("Panel Weight Components:

1. Base weight (Wave 1)
   - Design weight from initial selection

2. Attrition adjustment
   - Response propensity modeling
   - Inverse probability weighting

3. Post-stratification
   - Calibrate to population each wave

4. Longitudinal weight
   - For change estimates
   - Balanced across waves")
```

**Bottom line:** More complex than cross-section

---

## Attrition Weight Adjustment

```{r attrition_weights_90, echo=TRUE}
# Model attrition and adjust weights
set.seed(2024)
wave1_data <- data.frame(
  id = 1:100,
  age = sample(20:70, 100, replace = TRUE),
  education = sample(1:5, 100, replace = TRUE),
  base_weight = rep(100, 100)
)

# Simulate attrition (depends on characteristics)
prob_respond <- plogis(-1 + 0.02 * wave1_data$age + 
                       0.3 * wave1_data$education)
wave1_data$responded_w2 <- rbinom(100, 1, prob_respond)

# Calculate attrition weights
response_rate <- mean(wave1_data$responded_w2)
wave1_data$attrition_weight <- ifelse(
  wave1_data$responded_w2,
  wave1_data$base_weight / response_rate,
  0
)

c(Response_Rate_W2 = round(response_rate, 3),
  Mean_Base_Weight = mean(wave1_data$base_weight),
  Mean_Adjusted_Weight = round(mean(
    wave1_data$attrition_weight[wave1_data$responded_w2])))
```

**Bottom line:** Weights compensate for attrition

---

## Knowledge Check #2

### Quick test:

1. Rotating panels reduce _______
2. Typical attrition per wave: _______
3. Panel conditioning means _______
4. Overlap enables measuring _______

Think before continuing!

**Bottom line:** burden, 10%, behavior change, change

---

## Measuring Change

### Types of change estimates:

```{r change_types_92, echo=FALSE}
change_types <- data.frame(
  Type = c("Gross change", "Net change", "Individual change", "Transitions"),
  Definition = c("Total movement", "Aggregate difference", 
                "Person-level change", "Status changes"),
  Example = c("In + out of poverty", "Poverty rate change",
             "Income growth", "Employed → unemployed"),
  Panel_Needed = c("Yes", "No", "Yes", "Yes")
)

kable(change_types) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Panels reveal gross change

---

## Gross vs Net Change

```{r gross_net_change_93, echo=FALSE, fig.height=5}
transitions <- data.frame(
  Status = c("Poor→Poor", "Poor→Not", "Not→Poor", "Not→Not"),
  Count = c(15, 10, 5, 70)
)

ggplot(transitions, aes(x = Status, y = Count)) +
  geom_bar(stat = "identity", 
           fill = c("red", "green", "orange", "blue")) +
  labs(title = "Gross Change Exceeds Net Change",
       subtitle = "Net: 5% reduction | Gross: 15% transitions") +
  theme(text = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

**Bottom line:** Much more movement than net suggests

---

## Variance of Change

```{r variance_change_94, echo=FALSE}
cat("Var(Change) = Var(Y₂) + Var(Y₁) - 2×Cov(Y₂,Y₁)

Key insight: Correlation reduces variance

Example:
- Var(Y₁) = 100
- Var(Y₂) = 100
- Correlation = 0.7

Var(Change) = 100 + 100 - 2×0.7×10×10 = 60

Strong correlation → Precise change estimates")
```

**Bottom line:** Panel correlation helps precision

---

## Estimating Transitions

```{r transition_matrix_95, echo=FALSE}
trans_matrix <- matrix(
  c(0.85, 0.10, 0.05,
    0.15, 0.70, 0.15,
    0.10, 0.20, 0.70),
  nrow = 3, byrow = TRUE
)

rownames(trans_matrix) <- colnames(trans_matrix) <- 
  c("Employed", "Unemployed", "Inactive")

kable(trans_matrix, caption = "Quarterly Transition Probabilities") %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Shows labor market dynamics

---

## Split Panel Design

```{r split_panel_96, echo=FALSE}
split_design <- data.frame(
  Component = c("Continuous", "Rotating"),
  Proportion = c("40%", "60%"),
  Purpose = c("Pure change", "Reduced burden"),
  Waves = c("All waves", "4 waves"),
  Sample = c("2000", "3000")
)

kable(split_design) %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Balances competing needs

---

## Energy Break

### ⚡ Quick movement (30 seconds):

1. Stand and reach high
2. Touch toes
3. Shake it out
4. Deep breath
5. Ready for calibration!

Share: "One thing about panels I learned..."

**Bottom line:** Movement aids learning

---

## Calibration Overview

### Adjusting to known totals:

```{r calibration_overview_98, echo=FALSE}
calibration_steps <- data.frame(
  Step = 1:4,
  Process = c("Initial weights", "Identify benchmarks", 
             "Calculate adjustments", "Apply constraints"),
  Example = c("Design weights", "Census age-sex totals",
             "Raking/GREG", "Bounded weights"),
  Purpose = c("Starting point", "Target totals", 
             "Match population", "Avoid extremes")
)

kable(calibration_steps) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Improve representativeness

---

## Calibration Variables

```{r calibration_vars_99, echo=FALSE}
calib_vars <- data.frame(
  Variable = c("Age-sex", "Geography", "Education", "Household size"),
  Source = c("Census", "Admin records", "Census", "Census"),
  Quality = c("Excellent", "Excellent", "Good", "Good"),
  Correlation = c("High", "Medium", "High", "Medium")
)

kable(calib_vars) %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Use reliable, correlated variables

---

## Raking Process

```{r raking_process_100, echo=FALSE}
cat("Raking Algorithm:

Iteration 1:
- Adjust to age distribution
- Adjust to sex distribution
- Adjust to region distribution

Iteration 2:
- Re-adjust to age (now shifted)
- Re-adjust to sex (now shifted)
- Re-adjust to region (now shifted)

Continue until convergence (usually 3-5 iterations)")
```

**Bottom line:** Iterative margin matching

---

## Raking Example

```{r raking_example_101, echo=FALSE}
final <- data.frame(
  Age = c("Young", "Young", "Old", "Old"),
  Sex = c("M", "F", "M", "F"),
  Initial = c(100, 100, 100, 100),
  Final = c(90, 110, 135, 165)
)

kable(final) %>%
  kable_styling(font_size = 14)
```

**Bottom line:** Weights adjusted to match margins

---

## GREG Calibration

### Generalized Regression Estimator:

```{r greg_calibration_102, echo=FALSE}
cat("GREG uses regression to calibrate:

1. Model: Y = X'β + ε
   where X = auxiliary variables

2. Calibrated weight minimizes:
   Distance(w, d) subject to Σwᵢxᵢ = X_total

3. Result: Optimal combination of:
   - Design weights (d)
   - Auxiliary information (X)

More efficient than raking")
```

**Bottom line:** Model-assisted calibration

---

## Calibration Constraints

```{r calibration_constraints_103, echo=FALSE}
constraints <- data.frame(
  Method = c("Unbounded", "Linear", "Logit", "Truncated"),
  Range = c("(-∞, ∞)", "Positive", "[L, U]", "Bounded ratio"),
  Pros = c("Exact", "Natural", "Bounded", "Controlled"),
  Cons = c("Negative weights", "May not converge", "Complex", "Bias")
)

kable(constraints) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Balance exactness vs reasonable weights

---

## Weight Trimming Impact

```{r weight_trimming_104, echo=FALSE, fig.height=5}
set.seed(2024)
weights <- c(rlnorm(95, log(100), 0.5), 500, 800, 1000)
trimmed <- pmin(weights, quantile(weights, 0.95))

data.frame(
  Weight = c(weights, trimmed),
  Type = rep(c("Original", "Trimmed"), each = 98)
) %>%
  ggplot(aes(x = Weight, fill = Type)) +
  geom_histogram(alpha = 0.6, bins = 30, position = "identity") +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Weight Distribution: Before and After Trimming") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Reduces extreme weights

---

## Multi-Phase Calibration

```{r multiphase_calibration_105, echo=FALSE}
phase_calib <- data.frame(
  Phase = c("Phase 1", "Phase 2", "Combined"),
  Calibration = c("To population totals", "To Phase 1 estimates",
                  "Joint calibration"),
  Variables = c("Demographics", "Phase 1 outcomes", "Both"),
  Complexity = c("Simple", "Medium", "Complex")
)

kable(phase_calib) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Each phase needs calibration

---

## Panel Calibration

```{r panel_calibration_106, echo=FALSE}
cat("Panel Calibration Challenges:

1. Cross-sectional consistency
   - Each wave matches population

2. Longitudinal consistency
   - Same individuals across waves

3. Solutions:
   - Wave-specific weights (cross-section)
   - Longitudinal weights (change)
   - Composite weights (compromise)")
```

**Bottom line:** Multiple weight sets needed

---

## Composite Estimation

### Combining panel and cross-section:

```{r composite_estimation_107, echo=FALSE}
cat("Composite Estimator:

Y_composite = A × Y_continuing + K × Y_new

Where:
- A + K = 1
- A = weight for continuing sample
- K = weight for new sample

Optimal A depends on:
- Correlation over time
- Relative sample sizes
- Variance components")
```

**Bottom line:** Best of both designs

---

## Optimal Composite Weights

```{r optimal_composite_108, echo=TRUE}
# Calculate optimal A and K
rho <- 0.7  # Correlation between waves
n_cont <- 600  # Continuing sample
n_new <- 400   # New sample

# Simplified optimal weight
A_opt <- rho * n_cont / (rho * n_cont + n_new)
K_opt <- 1 - A_opt

c(Correlation = rho,
  Optimal_A = round(A_opt, 3),
  Optimal_K = round(K_opt, 3),
  Ratio = paste0(round(A_opt/K_opt, 2), ":1"))
```

**Bottom line:** Weight by correlation and size

---

## Software for Panels

```{r panel_software_109, echo=TRUE}
cat("R packages for panel surveys:

library(survey)    # Basic analysis
library(lavaan)    # Panel models
library(plm)       # Panel data models
library(panelr)    # Panel data wrangling

# Define panel design
panel_design <- svydesign(
  ids = ~psu_id,
  strata = ~stratum,
  weights = ~longitudinal_weight,
  data = panel_data
)

# Analyze change
svycontrast(svymean(~income_w2 - income_w1, panel_design))")
```

**Bottom line:** Specialized tools available

---

## Common Panel Errors

```{r panel_errors_110, echo=FALSE}
errors <- data.frame(
  Error = c("Ignoring attrition", "Wrong weights", 
           "Not testing conditioning", "Poor tracking"),
  Consequence = c("Bias", "Wrong inference", 
                 "Invalid estimates", "High attrition"),
  Prevention = c("Model and adjust", "Separate weights",
                "Include refreshment", "Invest in tracking")
)

kable(errors) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Panels need extra care

---

## Case Study: Labour Force

```{r lfs_case_111, echo=FALSE}
lfs_design <- data.frame(
  Feature = c("Rotation", "Overlap", "Frequency", "Sample", "Purpose"),
  Detail = c("2-(2)-2", "50%", "Monthly", "60,000 HH", "Employment trends")
)

kable(lfs_design) %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Standard for labor statistics

---

## LFS Rotation Pattern

```{r lfs_rotation_112, echo=FALSE, fig.height=5}
# 2-(2)-2 rotation pattern
lfs_pattern <- matrix(0, nrow = 4, ncol = 8)
lfs_pattern[1, c(1,2,5,6)] <- 1
lfs_pattern[2, c(2,3,6,7)] <- 1
lfs_pattern[3, c(3,4,7,8)] <- 1
lfs_pattern[4, c(1,4,5,8)] <- 1

as.data.frame(lfs_pattern) %>%
  mutate(Panel = 1:4) %>%
  pivot_longer(-Panel, names_to = "Month", values_to = "InSample") %>%
  mutate(Month = as.numeric(gsub("V", "", Month))) %>%
  ggplot(aes(x = Month, y = Panel, fill = factor(InSample))) +
  geom_tile(color = "white", size = 1) +
  scale_fill_manual(values = c("white", "darkgreen")) +
  scale_x_continuous(breaks = 1:8) +
  scale_y_continuous(breaks = 1:4) +
  labs(title = "2-(2)-2 Rotation Pattern", x = "Month") +
  theme(text = element_text(size = 14))
```

**Bottom line:** In 2 months, out 2, in 2 again

---

## Exercise: Design a Panel

### Your task (5 minutes):

Design a panel survey for:
- Income dynamics study
- 5 year duration
- Annual measurement
- Need 1000 in final wave
- Budget for 1500 initial

Calculate:
1. Expected attrition
2. Retention strategies
3. Weight adjustments

**Bottom line:** Apply panel concepts

---

## Exercise Solution

```{r panel_exercise_solution_114, echo=FALSE}
solution <- data.frame(
  Wave = 1:5,
  Year = 2024:2028,
  Initial = c(1500, 1350, 1215, 1094, 985),
  Target = c(1500, 1350, 1215, 1094, 1000),
  Attrition = c("0%", "10%", "10%", "10%", "10%"),
  Retention = c("Baseline", "Incentive+", "Track well", 
                "Flexibility", "Bonus")
)

kable(solution) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** 10% annual attrition manageable

---

## Group Discussion

### Share experiences (3 minutes):

1. Does your country have panel surveys?
2. What are main challenges?
3. How is attrition handled?

One key insight per table!

**Bottom line:** Learn from each other

---

## Advanced Topics Preview

### Coming in Part 3:

- Responsive design implementation
- Real-time quality monitoring
- Adaptive allocation
- Machine learning applications

**Bottom line:** Modern innovations ahead

---

## Part 2 Summary

### You've mastered:

✅ Panel design principles  
✅ Rotation patterns  
✅ Attrition handling  
✅ Change estimation  
✅ Calibration techniques  

**Bottom line:** Temporal complexity understood!

---

## Key Messages

```{r key_messages_118, echo=FALSE}
messages <- data.frame(
  Number = 1:5,
  Message = c("Panels measure gross change, not just net",
             "Attrition requires careful adjustment",
             "Multiple weight sets often needed",
             "Calibration improves estimates",
             "Conditioning effects are real")
)

kable(messages) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Panels reveal dynamics

---

## Quick Quiz

### Test yourself:

1. Panel overlap enables measuring _______
2. Typical total attrition over 5 waves: _______
3. GREG stands for _______
4. Conditioning affects _______

**Bottom line:** Change, 35-40%, Generalized Regression, responses

---

## Break Time!

## ☕ 15-Minute Break

### When we return:
- Responsive designs
- Quality monitoring
- Advanced estimation

### Challenge:
Calculate attrition rate for your last panel

**Bottom line:** Rest and reflect

---

class: center, middle, inverse

# End of Part 2

## Slides 76-150 Complete

### Next: Part 3 - Responsive and Adaptive Designs

---

---

## Welcome to Part 3!

### Responsive and Adaptive Survey Designs

Topics for next 75 slides:
- Real-time monitoring
- Adaptive interventions
- Quality indicators
- Cost optimization
- Modern innovations

**Bottom line:** Dynamic survey management

---

## Traditional vs Responsive

```{r traditional_responsive_152, echo=FALSE}
comparison <- data.frame(
  Aspect = c("Design", "Data collection", "Interventions", 
            "Quality", "Cost"),
  Traditional = c("Fixed", "Uniform", "Pre-planned",
                 "Post-hoc", "Fixed budget"),
  Responsive = c("Adaptive", "Tailored", "Data-driven",
                "Real-time", "Optimized")
)

kable(comparison) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** From static to dynamic

---

## Responsive Design Principles

### Core concepts:

1. **Monitor** indicators continuously
2. **Identify** problems early
3. **Intervene** based on data
4. **Evaluate** intervention effects
5. **Adjust** strategy accordingly

**Bottom line:** Evidence-based adaptation

---

## Key Indicators to Monitor

```{r key_indicators_154, echo=FALSE, fig.height=5}
indicators <- data.frame(
  Indicator = c("Response Rate", "Cost per Complete", "Contact Rate",
               "Refusal Rate", "Data Quality", "Sample Balance"),
  Current = c(65, 120, 78, 15, 92, 85),
  Target = c(70, 100, 80, 10, 95, 90),
  Status = c("⚠️", "🔴", "✅", "⚠️", "⚠️", "⚠️")
)

indicators %>%
  select(-Status) %>%
  pivot_longer(c(Current, Target), names_to = "Type", values_to = "Value") %>%
  ggplot(aes(x = Indicator, y = Value, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Set2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 12)) +
  labs(title = "Real-Time Quality Dashboard")
```

**Bottom line:** Multiple metrics tracked

---

## Response Rate Monitoring

```{r response_monitoring_155, echo=FALSE, fig.height=5}
set.seed(2024)
days <- 1:20
urban_rr <- 70 + cumsum(rnorm(20, -0.5, 1))
rural_rr <- 80 + cumsum(rnorm(20, -0.3, 0.8))

data.frame(
  Day = rep(days, 2),
  Response_Rate = c(urban_rr, rural_rr),
  Stratum = rep(c("Urban", "Rural"), each = 20)
) %>%
  ggplot(aes(x = Day, y = Response_Rate, color = Stratum)) +
  geom_line(size = 1.5) +
  geom_hline(yintercept = 70, linetype = "dashed", color = "red") +
  scale_color_brewer(palette = "Set1") +
  labs(y = "Response Rate (%)",
       title = "Response Rate Trends") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Urban needs intervention

---

## Cost Accumulation

```{r cost_accumulation_156, echo=FALSE, fig.height=5}
days <- 1:30
planned_cost <- seq(0, 100000, length.out = 30)
actual_cost <- cumsum(c(3000, rnorm(29, 3333, 500)))

data.frame(
  Day = days,
  Planned = planned_cost,
  Actual = actual_cost
) %>%
  pivot_longer(-Day, names_to = "Type", values_to = "Cost") %>%
  ggplot(aes(x = Day, y = Cost/1000, color = Type)) +
  geom_line(size = 1.5) +
  scale_color_brewer(palette = "Set2") +
  labs(y = "Cost ($1000s)",
       title = "Budget Tracking") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Slightly over budget

---

## Sample Composition

```{r sample_composition_157, echo=FALSE}
target_demo <- data.frame(
  Group = c("Male 18-34", "Male 35-54", "Male 55+",
           "Female 18-34", "Female 35-54", "Female 55+"),
  Target = c(15, 17, 18, 14, 18, 18),
  Current = c(12, 16, 20, 13, 19, 20)
)

target_demo$Difference <- target_demo$Current - target_demo$Target

kable(target_demo) %>%
  kable_styling(font_size = 11) %>%
  column_spec(4, color = ifelse(abs(target_demo$Difference) > 2, "red", "black"))
```

**Bottom line:** Young males underrepresented

---

## Phase Capacity

```{r phase_capacity_158, echo=FALSE}
cat("Phase Capacity Management:

Phase 1 (Weeks 1-2): Easy cases
- High response propensity
- Easy to contact
- Minimal effort

Phase 2 (Weeks 3-4): Medium difficulty
- Require more attempts
- Some reluctance
- Standard protocols

Phase 3 (Week 5): Hard cases
- Multiple refusals
- Hard to reach
- Maximum effort")
```

**Bottom line:** Tailor effort to difficulty

---

## Intervention Triggers

```{r intervention_triggers_159, echo=FALSE}
triggers <- data.frame(
  Indicator = c("Response < 60%", "Cost > 120%", "Refusal > 20%",
               "Balance index < 0.8"),
  Timing = c("Daily", "Weekly", "Daily", "Weekly"),
  Action = c("Increase incentive", "Reduce effort", 
            "Retrain interviewers", "Targeted recruitment"),
  Authority = c("Field manager", "Project director", 
               "Training lead", "Sampling lead")
)

kable(triggers) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Pre-planned responses

---

## Adaptive Interventions

```{r adaptive_interventions_160, echo=FALSE}
interventions <- data.frame(
  Problem = c("Low response", "High cost", "Poor quality", "Attrition"),
  Intervention = c("Incentive increase", "Mode switch", 
                  "Retraining", "Tracking investment"),
  Expected_Impact = c("+10% RR", "-20% cost", "+5% quality", "-5% attrition"),
  Cost = c("$10/case", "-$20/case", "$5000 total", "$15/case")
)

kable(interventions) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Data-driven adjustments

---

## Incentive Experiments

```{r incentive_experiment_161, echo=FALSE, fig.height=5}
incentives <- c(0, 10, 20, 30, 50)
response_rates <- c(45, 55, 62, 66, 68)
cost_per_complete <- c(80, 85, 92, 100, 115)

data.frame(
  Incentive = incentives,
  Response_Rate = response_rates,
  Cost = cost_per_complete
) %>%
  ggplot(aes(x = Incentive)) +
  geom_line(aes(y = Response_Rate), color = "blue", size = 1.5) +
  geom_line(aes(y = Cost), color = "red", size = 1.5) +
  geom_point(aes(y = Response_Rate), color = "blue", size = 3) +
  geom_point(aes(y = Cost), color = "red", size = 3) +
  scale_y_continuous(
    name = "Response Rate (%)",
    sec.axis = sec_axis(~., name = "Cost per Complete ($)")
  ) +
  labs(x = "Incentive ($)",
       title = "Incentive Optimization") +
  theme(text = element_text(size = 14))
```

**Bottom line:** $20 optimal balance

---

## Mode Switching

```{r mode_switching_162, echo=FALSE}
mode_sequence <- data.frame(
  Attempt = 1:5,
  Mode = c("Web", "Email reminder", "Phone", "SMS", "F2F"),
  Cost = c(5, 1, 15, 2, 50),
  Cumulative_RR = c(25, 30, 50, 52, 65),
  Marginal_RR = c(25, 5, 20, 2, 13)
)

kable(mode_sequence) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Escalate mode by propensity

---

## Contact Attempts Optimization

```{r contact_optimization_163, echo=FALSE, fig.height=5}
attempts <- 1:10
prob_contact <- 1 - (0.7)^attempts
cost <- attempts * 10
efficiency <- prob_contact / cost

data.frame(
  Attempts = attempts,
  Contact_Prob = prob_contact,
  Efficiency = efficiency * 10
) %>%
  ggplot(aes(x = Attempts)) +
  geom_line(aes(y = Contact_Prob), color = "green", size = 1.5) +
  geom_line(aes(y = Efficiency), color = "purple", size = 1.5) +
  geom_point(aes(y = Contact_Prob), color = "green", size = 3) +
  geom_point(aes(y = Efficiency), color = "purple", size = 3) +
  scale_y_continuous(
    name = "Contact Probability",
    sec.axis = sec_axis(~./10, name = "Efficiency")
  ) +
  labs(title = "Optimizing Contact Attempts") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Stop at 4-5 attempts

---

## Propensity Modeling

```{r propensity_modeling_164, echo=FALSE}
cat("Response Propensity Model:

logit(P(Response)) = β₀ + β₁Age + β₂Urban + β₃Prior + β₄Income

Use model to:
1. Predict response probability
2. Target high propensity first
3. Adjust effort for low propensity
4. Calculate propensity scores for weighting

Update model during collection!")
```

**Bottom line:** Model guides effort

---

## Propensity Score Groups

```{r propensity_groups_165, echo=FALSE}
prop_groups <- data.frame(
  Group = c("High", "Medium-High", "Medium-Low", "Low"),
  Propensity = c("0.8-1.0", "0.6-0.8", "0.4-0.6", "0.0-0.4"),
  Strategy = c("Minimal effort", "Standard", "Enhanced", "Maximum"),
  Expected_RR = c("85%", "70%", "50%", "30%"),
  Cost = c("$40", "$60", "$100", "$150")
)

kable(prop_groups) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Tailor to propensity

---

## Knowledge Check #3

### Quick test:

1. Responsive design adapts _______
2. Monitor indicators _______
3. Interventions based on _______
4. Phase capacity means _______

Think first!

**Bottom line:** during collection, daily, data, effort timing

---

## Sample Balance Indicators

```{r balance_indicators_167, echo=FALSE}
cat("Sample Balance Measures:

1. R-indicator (Representativeness)
   R = 1 - 2×SD(propensity scores)
   Range: [0, 1], Higher = better

2. Coefficient of Variation
   CV = SD(weights) / Mean(weights)
   Lower = better balance

3. Distance measures
   Compare sample to population distributions")
```

**Bottom line:** Multiple balance metrics

---

## R-Indicator Tracking

```{r r_indicator_168, echo=FALSE, fig.height=5}
days <- 1:30
r_indicator <- 0.7 + cumsum(rnorm(30, 0.003, 0.01))
r_indicator <- pmin(pmax(r_indicator, 0.6), 0.85)

data.frame(Day = days, R_Indicator = r_indicator) %>%
  ggplot(aes(x = Day, y = R_Indicator)) +
  geom_line(size = 1.5, color = "darkblue") +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.7, linetype = "dashed", color = "red") +
  scale_y_continuous(limits = c(0.5, 0.9)) +
  labs(y = "R-Indicator",
       title = "Sample Representativeness Over Time") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Maintain above 0.7

---

## Stopping Rules

```{r stopping_rules_169, echo=FALSE}
stopping_rules <- data.frame(
  Rule = c("Target n reached", "Budget exhausted", "Time limit",
          "Quality threshold", "Diminishing returns"),
  Threshold = c("n = 2000", "Cost = $100K", "Week 8",
               "R-indicator > 0.75", "Marginal RR < 2%"),
  Priority = c("Medium", "High", "High", "Medium", "Low"),
  Override = c("If quality low", "Never", "Emergency only",
              "If n sufficient", "Manager decision")
)

kable(stopping_rules) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Multiple stopping criteria

---

## Quality vs Quantity

```{r quality_quantity_170, echo=FALSE, fig.height=5}
sample_sizes <- seq(1000, 3000, 100)
quality <- 95 - 10 * log(sample_sizes/1000)
cost <- sample_sizes * 50

data.frame(
  Size = sample_sizes,
  Quality = quality,
  Cost = cost/1000
) %>%
  ggplot(aes(x = Size)) +
  geom_line(aes(y = Quality), color = "blue", size = 1.5) +
  geom_line(aes(y = Cost/2), color = "red", size = 1.5) +
  scale_y_continuous(
    name = "Quality Score",
    sec.axis = sec_axis(~.*2, name = "Cost ($1000s)")
  ) +
  labs(x = "Sample Size",
       title = "Quality-Quantity Trade-off") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Optimal around n=2000

---

## Paradata Collection

```{r paradata_171, echo=FALSE}
paradata_types <- data.frame(
  Type = c("Call records", "Timestamps", "Interviewer", 
          "Device", "GPS", "Audio"),
  Information = c("Contact attempts", "Interview length",
                 "Interviewer effects", "Mode used",
                 "Location verification", "Quality check"),
  Use = c("Effort optimization", "Cost calculation",
         "Training needs", "Technical issues",
         "Coverage check", "Fraud detection")
)

kable(paradata_types) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Process data valuable

---

## Interviewer Monitoring

```{r interviewer_monitoring_172, echo=FALSE, fig.height=5}
set.seed(2024)
interviewers <- data.frame(
  ID = 1:20,
  Completes = sample(20:60, 20, replace = TRUE),
  Hours = sample(30:80, 20, replace = TRUE)
)
interviewers$Productivity <- interviewers$Completes / interviewers$Hours

ggplot(interviewers, aes(x = Hours, y = Completes)) +
  geom_point(size = 3, color = "darkblue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Interviewer Productivity",
       subtitle = "Identify outliers for investigation") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Monitor individual performance

---

## Energy Check

### ⚡ Quick refresh (30 seconds):

1. Stand and stretch
2. Deep breathing
3. Share with neighbor: "One adaptive intervention I'd try..."

Ready for more?

**Bottom line:** Stay engaged!

---

## Real-Time Dashboards

```{r dashboard_mockup_174, echo=FALSE}
cat("Dashboard Components:

╔══════════════════════════════════════╗
║  SURVEY DASHBOARD - WEEK 3 DAY 2     ║
╠══════════════════════════════════════╣
║ Response Rate:  65% [▓▓▓▓▓▓░░] 🔴    ║
║ Cost/Complete: $112 [▓▓▓▓▓▓▓░] ⚠️    ║
║ Sample Balance: 0.73 [▓▓▓▓▓░░] ⚠️    ║
║ Data Quality:   92% [▓▓▓▓▓▓▓▓] ✅    ║
╠══════════════════════════════════════╣
║ ACTION REQUIRED:                      ║
║ • Increase urban incentives           ║
║ • Switch low propensity to phone      ║
╚══════════════════════════════════════╝")
```

**Bottom line:** Actionable visualization

---

## Adaptive Allocation

```{r adaptive_allocation_175, echo=FALSE}
reallocation <- data.frame(
  Week = c("Week 1", "Week 2", "Week 3"),
  Urban = c(50, 45, 60),
  Rural = c(50, 55, 40),
  Reason = c("Initial equal", "Rural performing better", 
            "Urban needs boost")
)

kable(reallocation) %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Move resources to problems

---

## Machine Learning Applications

```{r ml_applications_176, echo=FALSE}
ml_uses <- data.frame(
  Application = c("Response propensity", "Optimal contact time",
                 "Best mode prediction", "Quality scoring"),
  Algorithm = c("Random Forest", "Neural Network", 
               "Gradient Boosting", "Anomaly Detection"),
  Accuracy = c("85%", "78%", "82%", "91%"),
  Implementation = c("Operational", "Testing", "Pilot", "Operational")
)

kable(ml_uses) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** AI enhances decisions

---

## Predictive Models

```{r predictive_models_177, echo=FALSE, fig.height=5}
days_elapsed <- 1:20
current_rr <- c(seq(10, 50, length.out = 10), rep(NA, 10))
predicted_rr <- 65 * (1 - exp(-0.15 * days_elapsed))

data.frame(
  Day = days_elapsed,
  Actual = current_rr,
  Predicted = predicted_rr
) %>%
  pivot_longer(-Day, names_to = "Type", values_to = "Rate") %>%
  ggplot(aes(x = Day, y = Rate, color = Type)) +
  geom_line(size = 1.5, na.rm = TRUE) +
  geom_point(data = . %>% filter(!is.na(Rate)), size = 3) +
  scale_color_brewer(palette = "Set1") +
  labs(y = "Response Rate (%)",
       title = "Predicting Final Response Rate") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Early prediction guides decisions

---

## Cost Optimization Algorithm

```{r cost_optimization_178, echo=FALSE}
cat("Dynamic Cost Optimization:

While (budget_remaining > 0) {
  
  1. Calculate marginal cost per response
     for each stratum/mode combination
  
  2. Rank by efficiency (RR gain / $ spent)
  
  3. Allocate next resources to most efficient
  
  4. Update predictions
  
  5. Check stopping rules
  
}

Result: Maximum responses within budget")
```

**Bottom line:** Algorithmic optimization

---

## Multi-Armed Bandit

```{r multi_armed_bandit_179, echo=FALSE}
bandit_results <- data.frame(
  Intervention = c("$10 incentive", "$20 incentive", "Extra call",
                  "SMS reminder", "Email follow-up"),
  Trials = c(100, 100, 100, 50, 50),
  Success_Rate = c(0.15, 0.22, 0.10, 0.18, 0.08),
  Confidence = c("High", "High", "High", "Medium", "Medium"),
  Next_Action = c("Reduce", "Increase", "Stop", "Continue", "Stop")
)

kable(bandit_results) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Learn and adapt

---

## Case Study: Responsive LFS

```{r lfs_responsive_180, echo=FALSE}
lfs_responsive <- data.frame(
  Phase = c("Week 1", "Week 2", "Week 3", "Week 4"),
  Strategy = c("Easy cases", "Standard effort", "Enhanced", "Maximum"),
  Target = c("High propensity", "Medium", "Low", "Refusals"),
  Mode = c("Web/Phone", "Phone/F2F", "F2F", "F2F + Incentive"),
  Cost = c("$40", "$60", "$100", "$150")
)

kable(lfs_responsive) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Phased approach works

---

## Results from Responsive Design

```{r responsive_results_181, echo=FALSE, fig.height=5}
results <- data.frame(
  Metric = c("Response Rate", "Cost per Complete", "R-Indicator",
            "Time to Complete"),
  Traditional = c(68, 95, 0.72, 42),
  Responsive = c(71, 88, 0.78, 35)
)

results %>%
  pivot_longer(-Metric, names_to = "Design", values_to = "Value") %>%
  ggplot(aes(x = Metric, y = Value, fill = Design)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Set2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 12)) +
  labs(title = "Responsive Design Improvements")
```

**Bottom line:** Better outcomes across metrics

---

## Implementation Challenges

```{r implementation_challenges_182, echo=FALSE}
challenges <- data.frame(
  Challenge = c("IT infrastructure", "Staff training", "Real-time decisions",
               "Change management"),
  Impact = c("High", "Medium", "High", "Medium"),
  Solution = c("Invest in systems", "Continuous training",
              "Clear protocols", "Communication"),
  Timeline = c("6 months", "Ongoing", "3 months", "Ongoing")
)

kable(challenges) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Requires infrastructure

---

## Training Requirements

```{r training_requirements_183, echo=FALSE}
cat("Staff Training for Responsive Design:

1. Field Supervisors
   - Dashboard interpretation
   - Intervention decisions
   - Problem escalation

2. Interviewers
   - Flexible protocols
   - Mode switching
   - Quality focus

3. Data Team
   - Real-time monitoring
   - Predictive modeling
   - Report generation

4. Management
   - Strategic decisions
   - Resource allocation
   - Quality vs cost trade-offs")
```

**Bottom line:** All levels need training

---

## Software Tools

```{r software_tools_184, echo=FALSE}
tools <- data.frame(
  Tool = c("R Shiny", "Tableau", "SurveyMonkey API", "Blaise", "Custom"),
  Purpose = c("Dashboards", "Visualization", "Data collection",
             "CAPI/CATI", "Integration"),
  Cost = c("Free", "$$$", "$$", "$$$$", "Varies"),
  Learning = c("Medium", "Easy", "Easy", "Hard", "Hard")
)

kable(tools) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Mix of tools needed

---

## R Implementation

```{r r_implementation_185, echo=TRUE}
# R code for responsive monitoring
library(shiny)

# Real-time dashboard server function
server <- function(input, output) {
  
  # Update every 5 minutes
  autoInvalidate <- reactiveTimer(300000)
  
  current_data <- reactive({
    autoInvalidate()
    # Pull latest data
    read_csv('survey_data_live.csv')
  })
  
  output$response_rate <- renderPlot({
    calculate_rr(current_data())
  })
  
  output$recommendations <- renderText({
    generate_actions(current_data())
  })
}
```

**Bottom line:** R enables responsive design

---

## Quality Control Integration

```{r quality_control_186, echo=FALSE}
quality_checks <- data.frame(
  Check = c("Interview length", "Missing data", "Straightlining",
           "GPS verification"),
  Threshold = c("15-60 min", "< 5%", "< 10%", "Within 1km"),
  Current = c("28 min", "3%", "7%", "98% verified"),
  Status = c("✅", "✅", "⚠️", "✅")
)

kable(quality_checks) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Quality monitored continuously

---

## Communication Protocols

```{r communication_187, echo=FALSE}
protocols <- data.frame(
  Level = c("Daily", "Weekly", "Exception", "Emergency"),
  Audience = c("Field team", "Management", "Director", "All"),
  Content = c("Progress, issues", "Summary, trends",
             "Major problems", "Critical issues"),
  Channel = c("Dashboard", "Email report", "Phone", "All channels")
)

kable(protocols) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Structured communication

---

## Exercise: Design Responsive

### Your task (5 minutes):

Design responsive strategy for:
- National health survey
- Target: 5000 responses
- Budget: $300,000
- Timeline: 6 weeks

Include:
1. Phases
2. Interventions
3. Stopping rules
4. Quality metrics

**Bottom line:** Apply responsive concepts

---

## Exercise Solution

```{r exercise_responsive_189, echo=FALSE}
solution <- data.frame(
  Week = 1:6,
  Phase = c("Launch", "Standard", "Enhanced", "Enhanced", "Maximum", "Close"),
  Target_n = c(1000, 1800, 2600, 3400, 4200, 5000),
  Intervention = c("Web first", "Add phone", "Incentive +$10",
                  "F2F starts", "Double incentive", "Final push"),
  Stop_Rule = c("RR > 30%", "On track", "RR > 50%", 
               "Cost check", "RR > 60%", "Target or time")
)

kable(solution) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Structured escalation

---

## Group Discussion

### Share insights (3 minutes):

1. Could responsive design work in your country?
2. Main barriers?
3. Which intervention most promising?

Report one key point!

**Bottom line:** Learn from peers

---

## Innovation Spotlight

```{r innovation_spotlight_191, echo=FALSE}
innovations <- data.frame(
  Innovation = c("SMS micro-surveys", "Passive data", "Blockchain incentives",
                "AI interviewer"),
  Stage = c("Operational", "Research", "Pilot", "Experimental"),
  Promise = c("Quick pulse", "No burden", "Instant payment", "24/7 availability"),
  Challenge = c("Limited depth", "Privacy", "Volatility", "Acceptance")
)

kable(innovations) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Future is arriving

---

## Cost-Benefit Analysis

```{r cost_benefit_192, echo=FALSE, fig.height=5}
costs <- c(0, 50000, 75000, 100000)  # Investment
benefits <- c(0, 30000, 80000, 140000)  # Savings + quality

data.frame(
  Investment = costs/1000,
  Net_Benefit = (benefits - costs)/1000
) %>%
  ggplot(aes(x = Investment, y = Net_Benefit)) +
  geom_line(size = 2, color = "darkgreen") +
  geom_point(size = 4) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = "Investment ($1000s)",
       y = "Net Benefit ($1000s)",
       title = "ROI of Responsive Design") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Break-even at $50K investment

---

## Lessons Learned

```{r lessons_learned_193, echo=FALSE}
lessons <- data.frame(
  Number = 1:5,
  Lesson = c("Start simple, expand gradually",
            "Dashboards essential for decisions",
            "Train staff thoroughly",
            "Document all interventions",
            "Measure impact rigorously")
)

kable(lessons) %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Practical wisdom

---

## Real-Time Quality Metrics

```{r quality_metrics_194, echo=TRUE}
# Calculate quality metrics in real-time
calculate_quality <- function(data) {
  metrics <- list(
    response_rate = mean(data$responded, na.rm = TRUE),
    item_missing = mean(is.na(data$key_variable)),
    r_indicator = 1 - 2 * sd(data$propensity_score),
    cost_per_complete = sum(data$cost) / sum(data$completed)
  )
  
  # Flag issues
  metrics$alerts <- c()
  if(metrics$response_rate < 0.6) 
    metrics$alerts <- c(metrics$alerts, "Low response")
  if(metrics$item_missing > 0.1) 
    metrics$alerts <- c(metrics$alerts, "High missing")
  
  return(metrics)
}
```

**Bottom line:** Automated monitoring

---

## Adaptive Sample Size

```{r adaptive_sample_195, echo=FALSE, fig.height=5}
# Adaptive sample size based on variance
weeks <- 1:6
planned_n <- rep(1000, 6)
actual_var <- c(100, 95, 92, 88, 85, 83)
adaptive_n <- 1000 * (100 / actual_var)

data.frame(
  Week = weeks,
  Planned = planned_n,
  Adaptive = adaptive_n
) %>%
  pivot_longer(-Week, names_to = "Type", values_to = "Sample_Size") %>%
  ggplot(aes(x = Week, y = Sample_Size, color = Type)) +
  geom_line(size = 1.5) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Set1") +
  labs(y = "Sample Size",
       title = "Adaptive Sample Size Adjustment") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Adjust to maintain precision

---

## Documentation Standards

```{r documentation_196, echo=FALSE}
cat("Essential Documentation:

1. Design Changes
   - Date and time
   - Reason for change
   - Expected impact
   - Actual outcome

2. Interventions
   - Trigger met
   - Action taken
   - Cost incurred
   - Effect measured

3. Quality Issues
   - Problem identified
   - Resolution attempted
   - Success/failure
   - Lessons learned")
```

**Bottom line:** Document everything

---

## Integration with Existing Systems

```{r integration_197, echo=FALSE}
integration <- data.frame(
  System = c("CAPI/CATI", "Sample management", "Cost tracking", "Quality control"),
  Integration = c("API feeds", "Real-time sync", "Budget alerts", "Flag issues"),
  Frequency = c("Every interview", "Hourly", "Daily", "Continuous"),
  Priority = c("High", "High", "Medium", "High")
)

kable(integration) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Connect all systems

---

## Scaling Responsive Design

```{r scaling_198, echo=FALSE}
scaling <- data.frame(
  Stage = c("Pilot", "Small scale", "Regional", "National"),
  Surveys = c("1", "2-5", "5-10", "All"),
  Investment = c("$50K", "$200K", "$500K", "$1M+"),
  ROI = c("Learning", "20%", "40%", "60%"),
  Timeline = c("3 months", "6 months", "1 year", "2+ years")
)

kable(scaling) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Gradual expansion works

---

## Future Directions

```{r future_directions_199, echo=FALSE}
future <- data.frame(
  Trend = c("AI-driven decisions", "Automated interventions", 
           "Predictive stopping", "Real-time calibration"),
  Current = c("Rule-based", "Manual", "Fixed rules", "Post-hoc"),
  Future = c("Machine learning", "Autonomous", "Dynamic", "Continuous"),
  Timeline = c("2-3 years", "3-5 years", "1-2 years", "2-3 years")
)

kable(future) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Automation increasing

---

## Success Metrics

```{r success_metrics_200, echo=FALSE}
success <- data.frame(
  Metric = c("Response rate improvement", "Cost reduction", 
            "Quality improvement", "Time reduction", "Staff satisfaction"),
  Target = c("+5%", "-15%", "+10%", "-20%", "+20%"),
  Typical = c("+3-7%", "-10-20%", "+5-15%", "-15-25%", "+10-30%"),
  Best = c("+10%", "-30%", "+20%", "-40%", "+40%")
)

kable(success) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Measurable improvements

---

## Part 3 Summary

### You've learned:

✅ Responsive design principles  
✅ Real-time monitoring  
✅ Adaptive interventions  
✅ Quality-cost optimization  
✅ Implementation strategies  

**Bottom line:** Dynamic management mastered!

---

## Key Messages

```{r part3_key_messages_202, echo=FALSE}
messages <- data.frame(
  Point = 1:5,
  Message = c("Monitor continuously, intervene quickly",
             "Data drives decisions",
             "Balance quality and cost",
             "Technology enables responsiveness",
             "Start simple, evolve")
)

kable(messages) %>%
  kable_styling(font_size = 13)
```

**Bottom line:** Responsive is the future

---

## Quick Review

### Test yourself:

1. R-indicator measures _______
2. Paradata includes _______
3. Phase capacity allocates _______
4. Stopping rules prevent _______

**Bottom line:** representativeness, process data, effort, waste

---

## Break Time!

## ☕ 15-Minute Break

### Coming in Part 4:
- Advanced estimation
- Complex calibration
- Software workshop

### Challenge:
Design one responsive intervention

**Bottom line:** Rest and reflect

---

class: center, middle, inverse

# End of Part 3

## Slides 151-225 Complete

### Next: Part 4 - Advanced Estimation

---
---

## Welcome to Part 4!

### Advanced Estimation and Analysis

Topics for next 75 slides:
- Complex estimation procedures
- Small area estimation
- Model-assisted methods
- Missing data handling
- Software deep dive

**Bottom line:** Advanced analytical techniques

---

## Beyond Simple Estimates

```{r beyond_simple_227, echo=FALSE}
estimate_types <- data.frame(
  Type = c("Ratio", "Regression", "Quantile", "Correlation", "Model-based"),
  Example = c("Per capita income", "Income vs education", 
             "Median wealth", "Variable association", "Small area"),
  Complexity = c("Medium", "Medium", "High", "High", "Very High"),
  Variance = c("Delta method", "Linearization", "Replication", 
              "Replication", "Bootstrap/Model")
)

kable(estimate_types) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Complex statistics need special methods

---

## Ratio Estimation

```{r ratio_estimation_228, echo=FALSE}
cat("Ratio Estimator: R = Y̅ / X̅

Examples:
- Per capita income = Total income / Total persons
- Unemployment rate = Unemployed / Labor force
- Literacy rate = Literate / Adult population

Variance uses Taylor linearization:
Var(R) ≈ (1/X̅²)[Var(Y) + R²Var(X) - 2R×Cov(X,Y)]")
```

**Bottom line:** Ratios everywhere in surveys

---

## Ratio Estimation Example

```{r ratio_example_229, echo=TRUE}
# Calculate ratio with proper variance
set.seed(2024)
sample_data <- data.frame(
  income = rnorm(100, 50000, 15000),
  household_size = rpois(100, 3) + 1,
  weight = rep(100, 100)
)

# Design
design <- svydesign(ids = ~1, weights = ~weight, data = sample_data)

# Ratio estimate
ratio_est <- svyratio(~income, ~household_size, design)
print(ratio_est)
confint(ratio_est)
```

**Bottom line:** Per capita income with CI

---

## Regression Coefficients

```{r regression_survey_230, echo=TRUE}
# Survey-weighted regression
model <- svyglm(income ~ household_size + I(household_size^2), 
                design = design)

# Extract coefficients with SEs
coef_summary <- summary(model)$coefficients[, 1:2]
kable(round(coef_summary, 0)) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Account for design in regression

---

## Domain Regression

```{r domain_regression_231, echo=FALSE}
cat("Domain-specific models:

# Separate models by domain
urban_model <- svyglm(income ~ education + age, 
                      subset(design, urban == 1))

rural_model <- svyglm(income ~ education + age, 
                      subset(design, urban == 0))

# Compare coefficients
# Test for interaction effects
# Different relationships by domain")
```

**Bottom line:** Relationships vary by domain

---

## Quantile Estimation

```{r quantile_estimation_232, echo=TRUE}
# Median and other quantiles
quantiles <- svyquantile(~income, design = design, 
                         quantiles = c(0.1, 0.25, 0.5, 0.75, 0.9))

# Extract values
quant_vals <- coef(quantiles)
se_vals <- SE(quantiles)

# Display with SEs
quantile_results <- data.frame(
  Quantile = c("10th", "25th", "50th", "75th", "90th"),
  Estimate = round(as.numeric(quant_vals), 0),
  SE = round(as.numeric(se_vals), 0)
)

kable(quantile_results) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Distribution beyond mean

---

## Variance Estimation Methods

```{r variance_methods_advanced_233, echo=FALSE}
methods_comparison <- data.frame(
  Method = c("Linearization", "Jackknife", "Bootstrap", "BRR"),
  Speed = c("Fast", "Medium", "Slow", "Fast"),
  Accuracy = c("Good", "Better", "Best", "Good"),
  Restrictions = c("Smooth functions", "None", "None", "2 PSU/stratum")
)

kable(methods_comparison) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Trade-offs in methods

---

## Replicate Weights

```{r replicate_weights_234, echo=FALSE}
cat("Replicate Weight Methods:

1. Jackknife (JK1, JKn)
   - Delete one PSU at a time
   - n replicates for n PSUs

2. Bootstrap
   - Resample PSUs with replacement
   - 100-500 replicates typical

3. Balanced Repeated Replication (BRR)
   - For 2 PSUs per stratum
   - Hadamard matrix balancing

4. Fay's method
   - Modified BRR
   - Adjustment factor (usually 0.5)")
```

**Bottom line:** Replicates capture uncertainty

---

## Creating Replicate Weights

```{r create_replicates_235, echo=TRUE}
# Generate jackknife replicate weights
n_psu <- 30
n_units <- 1000
jk_weights <- matrix(0, nrow = n_units, ncol = n_psu)

for(i in 1:n_psu) {
  # Drop PSU i, upweight others
  jk_weights[, i] <- ifelse(rep(1:n_psu, length.out = n_units) == i,
                            0,  # Dropped
                            n_psu/(n_psu-1))  # Upweighted
}

dim(jk_weights)
colMeans(jk_weights)[1:5]  # Check first 5
```

**Bottom line:** Each replicate drops different PSU

---

## Small Area Estimation

### The challenge:

```{r sae_challenge_236, echo=FALSE, fig.height=5}
domain_sizes <- data.frame(
  Domain = paste("Area", 1:20),
  Sample_n = c(rep(15, 5), rep(30, 5), rep(50, 5), rep(100, 5)),
  Direct_CV = c(rep(35, 5), rep(25, 5), rep(18, 5), rep(12, 5))
)

ggplot(domain_sizes, aes(x = Sample_n, y = Direct_CV)) +
  geom_point(size = 4, color = "red") +
  geom_hline(yintercept = 20, linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = c(15, 30, 50, 100)) +
  labs(x = "Domain Sample Size", y = "CV (%)",
       title = "Small Domains Have Poor Precision") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Need model-based methods

---

## SAE Models

```{r sae_models_237, echo=FALSE}
models <- data.frame(
  Model = c("Fay-Herriot", "Battese-Harter-Fuller", "EBLUP", "Hierarchical Bayes"),
  Level = c("Area", "Unit", "Area/Unit", "Area/Unit"),
  Assumptions = c("Known variance", "Linear model", "Normality", "Prior distributions"),
  Software = c("sae, hbsae", "sae", "sae, lme4", "hbsae, rstanarm")
)

kable(models) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Different models for different situations

---

## Fay-Herriot Model

```{r fay_herriot_238, echo=FALSE}
cat("Area-Level Model:

Y_i = X_i'β + v_i + e_i

Where:
- Y_i = direct estimate for area i
- X_i = auxiliary variables
- v_i ~ N(0, σ_v²) = model error
- e_i ~ N(0, ψ_i) = sampling error

Borrows strength through:
1. Auxiliary variables (X)
2. Between-area similarity (σ_v²)")
```

**Bottom line:** Combines direct and synthetic

---

## SAE Example

```{r sae_example_239, echo=FALSE, fig.height=5}
set.seed(2024)
areas <- 1:15
direct_est <- rnorm(15, 25, 5)
direct_se <- runif(15, 2, 8)
model_est <- 25 + 0.5 * (direct_est - 25)  # Shrinkage
model_se <- direct_se * 0.6  # Reduced SE

data.frame(
  Area = rep(areas, 2),
  Estimate = c(direct_est, model_est),
  SE = c(direct_se, model_se),
  Method = rep(c("Direct", "Model"), each = 15)
) %>%
  ggplot(aes(x = Area, y = Estimate, color = Method)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Estimate - SE, ymax = Estimate + SE), 
                width = 0.3) +
  scale_color_brewer(palette = "Set1") +
  labs(title = "SAE Reduces Uncertainty") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Model-based more stable

---

## Benchmarking SAE

```{r benchmarking_sae_240, echo=FALSE}
cat("Benchmarking Steps:

1. Calculate model-based estimates
2. Sum to higher level
3. Compare to direct estimate at that level
4. Calculate adjustment factor
5. Apply to maintain consistency

Example:
- Provincial SAE sum: 245,000
- National direct: 250,000
- Adjustment: 250,000/245,000 = 1.02
- Apply 1.02 to all provincial estimates")
```

**Bottom line:** Maintain coherence

---

## Missing Data Patterns

```{r missing_patterns_241, echo=FALSE, fig.height=5}
set.seed(2024)
patterns <- data.frame(
  Pattern = c("Complete", "Missing Income", "Missing Health", 
             "Missing Both", "Other"),
  Count = c(60, 20, 10, 5, 5),
  Percent = c(60, 20, 10, 5, 5)
)

ggplot(patterns, aes(x = reorder(Pattern, -Count), y = Count)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  labs(x = "Pattern", y = "Frequency",
       title = "Missing Data Patterns") +
  theme(text = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

**Bottom line:** Understand patterns first

---

## Imputation Methods

```{r imputation_methods_advanced_242, echo=FALSE}
methods <- data.frame(
  Method = c("Mean", "Regression", "PMM", "Multiple"),
  Pros = c("Simple", "Uses relationships", "Preserves distribution", "Uncertainty"),
  Cons = c("Biased variance", "Assumes linearity", "Complex", "Computational"),
  When = c("Never", "MAR + predictors", "Non-normal", "Always best")
)

kable(methods) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Multiple imputation preferred

---

## Multiple Imputation Process

```{r mi_process_243, echo=FALSE}
cat("MI Process:

1. Impute m complete datasets
   library(mice)
   imputed <- mice(data, m = 5)

2. Analyze each dataset
   fit1 <- with(imputed, svyglm(y ~ x))

3. Pool results (Rubin's rules)
   pooled <- pool(fit1)

4. Extract final estimates
   summary(pooled)

Key: Incorporates imputation uncertainty")
```

**Bottom line:** MI handles uncertainty properly

---

## MI Example

```{r mi_example_244, echo=TRUE}
# Multiple imputation example
library(mice)

# Create data with missing values
set.seed(2024)
mi_data <- data.frame(
  income = c(rnorm(80, 50000, 10000), rep(NA, 20)),
  age = c(sample(20:70, 90, replace = TRUE), rep(NA, 10)),
  education = sample(5:20, 100, replace = TRUE)
)

# Perform multiple imputation
imputed <- mice(mi_data, m = 5, method = 'pmm', printFlag = FALSE)

# Show imputation summary
print(imputed)
```

**Bottom line:** 5 complete datasets created

---

## Knowledge Check #4

### Quick test:

1. Ratio variance uses _______ method
2. SAE stands for _______
3. MI creates _______ datasets
4. Replication methods include _______

Think first!

**Bottom line:** Delta/linearization, Small Area Estimation, multiple, jackknife/bootstrap

---

## Complex Survey Software

```{r complex_software_246, echo=FALSE}
cat("R Survey Ecosystem:

Core packages:
- survey: Main package
- srvyr: Tidyverse integration
- convey: Poverty/inequality
- sae: Small area estimation
- mice: Multiple imputation
- lavaan.survey: SEM for surveys

Example workflow:
library(survey)
library(srvyr)
library(convey)

design %>%
  group_by(region) %>%
  summarise(
    gini = svygini(~income),
    poverty = svyfgt(~income, g = 0)
  )")
```

**Bottom line:** Rich ecosystem available

---

## Poverty and Inequality

```{r poverty_inequality_247, echo=FALSE}
poverty_measures <- data.frame(
  Measure = c("Headcount", "Gap", "Severity", "Gini", "Theil"),
  Formula = c("P0", "P1", "P2", "G", "T"),
  Interpretation = c("% below line", "Average gap", "Squared gap",
                    "Inequality", "Decomposable"),
  Range = c("[0,1]", "[0,1]", "[0,1]", "[0,1]", "[0,∞)")
)

kable(poverty_measures) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Beyond simple poverty rate

---

## Calculating Gini

```{r gini_calculation_248, echo=TRUE}
# Gini coefficient with complex design
library(convey)

# Prepare design for convey
design_convey <- convey_prep(design)

# Calculate Gini
gini <- svygini(~income, design_convey)

# Results
c(Gini = round(coef(gini), 3),
  SE = round(SE(gini), 4),
  CI_Lower = round(confint(gini)[1], 3),
  CI_Upper = round(confint(gini)[2], 3))
```

**Bottom line:** Inequality with proper variance

---

## Decomposition Analysis

```{r decomposition_249, echo=FALSE}
cat("Decomposing Inequality:

# Between vs Within group inequality
theil_total <- svyzenga(~income, design)

# By groups (e.g., urban/rural)
theil_decomp <- svyzenga(~income, ~urban, design)

# Results show:
- Within-group component (85%)
- Between-group component (15%)

Interpretation: Most inequality within groups")
```

**Bottom line:** Understand inequality sources

---

## Energy Break

### ⚡ Final stretch energy (30 seconds):

1. Stand and stretch high
2. Shake it out
3. Deep breaths
4. Almost there!

Share: "Most interesting advanced method..."

**Bottom line:** Final push!

---

## Model-Assisted Estimation

```{r model_assisted_251, echo=FALSE}
cat("Generalized Regression (GREG):

Ŷ_GREG = Ŷ_HT + (X - X̂_HT)'B̂

Where:
- Ŷ_HT = Horvitz-Thompson estimator
- X = Known population totals
- B̂ = Regression coefficients

Benefits:
- Uses auxiliary information
- Reduces variance
- Maintains design consistency")
```

**Bottom line:** Best of design and model

---

## Calibration Estimators

```{r calibration_estimators_252, echo=FALSE}
estimators <- data.frame(
  Estimator = c("Post-stratification", "Raking", "GREG", "Ridge calibration"),
  Auxiliary = c("Categories", "Marginals", "Continuous", "Many variables"),
  Distance = c("Chi-square", "Entropy", "Euclidean", "Ridge"),
  Complexity = c("Low", "Medium", "Medium", "High")
)

kable(estimators) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Many calibration options

---

## Non-Response Adjustment

```{r nonresponse_adjustment_253, echo=FALSE}
cat("Propensity Score Adjustment:

1. Model response probability
   resp_model <- glm(responded ~ age + urban + income_q,
                     family = binomial)

2. Calculate propensity scores
   p_scores <- predict(resp_model, type = 'response')

3. Create adjustment cells
   cells <- cut(p_scores, breaks = 5)

4. Adjust weights within cells
   adj_weight <- base_weight / mean(responded[cell])

Result: Reduces non-response bias")
```

**Bottom line:** Model-based NR adjustment

---

## Combining Data Sources

```{r combining_sources_254, echo=FALSE, fig.height=5}
sources <- data.frame(
  Source = c("Survey", "Admin", "Big Data", "Combined"),
  Coverage = c(80, 95, 60, 98),
  Quality = c(90, 85, 70, 88),
  Cost = c(100, 20, 30, 150)
)

sources %>%
  select(-Cost) %>%
  pivot_longer(-Source, names_to = "Metric", values_to = "Score") %>%
  ggplot(aes(x = Source, y = Score, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Data Source Characteristics") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Combination improves coverage

---

## Data Integration Methods

```{r data_integration_255, echo=FALSE}
integration <- data.frame(
  Method = c("Statistical matching", "Record linkage", 
            "Multiple frame", "Data fusion"),
  Requirement = c("Common variables", "Identifiers", 
                 "Frame overlap", "Similar units"),
  Accuracy = c("Medium", "High", "High", "Medium"),
  Privacy = c("Good", "Poor", "Good", "Good")
)

kable(integration) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Different approaches available

---

## Seasonal Adjustment

```{r seasonal_adjustment_256, echo=FALSE}
cat("Seasonal Adjustment for Surveys:

1. Identify seasonal pattern
2. Estimate seasonal factors
3. Apply adjustment

Methods:
- X-13ARIMA-SEATS
- STL decomposition
- State space models

Example: Quarterly employment
Q1: -2%, Q2: +1%, Q3: +3%, Q4: -2%

Adjusted = Observed / Seasonal_Factor")
```

**Bottom line:** Remove seasonal effects

---

## Time Series from Surveys

```{r time_series_257, echo=FALSE, fig.height=5}
quarters <- 1:12
estimate <- 25 + sin(quarters/2) * 3 + rnorm(12, 0, 1)
se <- rep(0.5, 12)

data.frame(
  Quarter = quarters,
  Estimate = estimate,
  Lower = estimate - 1.96*se,
  Upper = estimate + 1.96*se
) %>%
  ggplot(aes(x = Quarter, y = Estimate)) +
  geom_line(size = 1.5) +
  geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3) +
  scale_x_continuous(breaks = 1:12) +
  labs(title = "Quarterly Survey Estimates") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Track trends with uncertainty

---

## Composite Estimators

```{r composite_estimators_advanced_258, echo=FALSE}
cat("Composite Estimation:

Ŷ_composite = w₁Ŷ₁ + w₂Ŷ₂ + ... + wₖŶₖ

Optimal weights minimize variance:
w_opt = Σ⁻¹1 / (1'Σ⁻¹1)

Where Σ = variance-covariance matrix

Applications:
- Combining surveys
- Panel + cross-section
- Direct + model-based")
```

**Bottom line:** Optimal combination

---

## Disclosure Control

```{r disclosure_control_259, echo=FALSE}
disclosure_methods <- data.frame(
  Method = c("Suppression", "Rounding", "Perturbation", "Synthetic"),
  Risk = c("Low", "Low", "Medium", "Very Low"),
  Utility = c("Poor", "Good", "Good", "Medium"),
  Implementation = c("Easy", "Easy", "Medium", "Hard")
)

kable(disclosure_methods) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Balance risk and utility

---

## Quality Reporting

```{r quality_reporting_260, echo=FALSE}
cat("Standard Quality Report:

1. Sampling
   - Design description
   - Target vs achieved sample
   - Response rates by domain

2. Estimation
   - Weighting procedures
   - Imputation rates
   - Calibration performed

3. Precision
   - CVs for key estimates
   - Design effects
   - Coverage rates

4. Comparability
   - Changes from previous
   - International standards")
```

**Bottom line:** Comprehensive documentation

---

## Software Comparison

```{r software_comparison_detailed_261, echo=FALSE}
comparison <- data.frame(
  Feature = c("Basic estimates", "Replicate weights", "SAE", 
             "Multiple imputation", "Graphics"),
  R = c("+++", "+++", "+++", "+++", "+++"),
  Stata = c("+++", "+++", "++", "++", "++"),
  SAS = c("+++", "+++", "++", "+++", "+"),
  SPSS = c("++", "+", "-", "+", "++")
)

kable(comparison) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** R most comprehensive

---

## R Code Organization

```{r code_organization_262, echo=FALSE}
cat("Project Structure:

project/
├── data/
│   ├── raw/
│   └── processed/
├── R/
│   ├── 01_design.R
│   ├── 02_weights.R
│   ├── 03_estimation.R
│   └── 04_tables.R
├── output/
│   ├── tables/
│   └── figures/
└── docs/
    └── methodology.md

Use projects for reproducibility!")
```

**Bottom line:** Organize for success

---

## Advanced R Techniques

```{r advanced_r_263, echo=TRUE}
# Advanced Survey Analysis in R

# Parallel processing for replicates
library(parallel)
cl <- makeCluster(4)
# boot_results <- parLapply(cl, 1:500, boot_function)
stopCluster(cl)

# Database-backed designs
# library(DBI)
# conn <- dbConnect(RSQLite::SQLite(), 'survey.db')
# db_design <- svydesign(..., data = conn)

# Custom variance estimators
my_variance <- function(design, statistic) {
  # Custom implementation
  return(variance_estimate)
}
```

**Bottom line:** R scales to complexity

---

## Exercise: Complex Analysis

### Your task (5 minutes):

Using complex survey data:
1. Calculate ratio estimate
2. Fit regression model
3. Estimate median
4. Calculate Gini coefficient

```{r exercise_complex_264, echo=FALSE}
cat("# Your workspace
design <- svydesign(...)
ratio <- svyratio(...)
model <- svyglm(...)
median <- svyquantile(...)
gini <- svygini(...)")
```

**Bottom line:** Practice integration

---

## Exercise Solution

```{r exercise_solution_complex_265, echo=TRUE}
# Create sample design
data <- data.frame(
  income = rlnorm(500, 10, 1),
  size = rpois(500, 3) + 1,
  education = sample(5:20, 500, replace = TRUE),
  weight = runif(500, 50, 150)
)

design <- svydesign(ids = ~1, weights = ~weight, data = data)

# 1. Ratio
ratio <- svyratio(~income, ~size, design)

# 2. Regression
model <- svyglm(income ~ education + size, design)

# 3. Median
median <- svyquantile(~income, design, 0.5)

print(ratio)
```

**Bottom line:** Complex analyses integrated

---

## Machine Learning Integration

```{r ml_integration_266, echo=FALSE}
ml_applications <- data.frame(
  Task = c("Imputation", "Propensity scores", "Small area", "Fraud detection"),
  Algorithm = c("Random Forest", "XGBoost", "Neural nets", "Isolation Forest"),
  Package = c("missForest", "xgboost", "keras", "isotree"),
  Integration = c("Direct", "Two-stage", "Model-based", "Post-process")
)

kable(ml_applications) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** ML enhances surveys

---

## Future Directions

```{r future_directions_267, echo=FALSE}
future <- data.frame(
  Trend = c("Real-time estimation", "Automated quality", 
           "AI-assisted design", "Blockchain weights"),
  Timeline = c("Now", "2-3 years", "3-5 years", "5+ years"),
  Impact = c("High", "High", "Medium", "Unknown"),
  Readiness = c("Operational", "Pilot", "Research", "Concept")
)

kable(future) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Rapid evolution ahead

---

## Best Practices Summary

```{r best_practices_advanced_268, echo=FALSE}
practices <- data.frame(
  Area = c("Design", "Collection", "Processing", "Analysis", "Reporting"),
  Best_Practice = c("Document everything", "Monitor quality", 
                   "Reproducible code", "Use survey methods",
                   "Report uncertainty")
)

kable(practices) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Excellence in every step

---

## Variance Estimation Example

```{r variance_example_269, echo=TRUE}
# Compare variance estimation methods
set.seed(2024)

# Standard error using linearization
se_linear <- SE(svymean(~income, design))

# Using replicate weights (simplified)
# jk_design <- as.svrepdesign(design, type="JK1")
# se_jk <- SE(svymean(~income, jk_design))

cat("SE (linearization):", round(se_linear, 2), "\n")
# cat("SE (jackknife):", round(se_jk, 2), "\n")
```

**Bottom line:** Different methods, similar results

---

## Complex Domain Analysis

```{r domain_analysis_270, echo=TRUE}
# Domain estimation with proper variance
domain_means <- svyby(~income, ~education, design, svymean)

# Show first few rows
head(domain_means, 4)
```

**Bottom line:** Automatic variance calculation

---

## Post-Stratification Example

```{r poststrat_271, echo=TRUE}
# Post-stratification adjustment
# Create population totals
pop_totals <- data.frame(
  education = c(5:20),
  Freq = c(rep(1000, 8), rep(2000, 8))
)

# Post-stratify (would need matching variables)
# ps_design <- postStratify(design, ~education, pop_totals)

cat("Post-stratification adjusts weights to match known totals")
```

**Bottom line:** Improves representativeness

---

## Handling Survey Errors

```{r survey_errors_272, echo=FALSE}
error_types <- data.frame(
  Error_Type = c("Coverage", "Sampling", "Nonresponse", "Measurement"),
  Source = c("Frame issues", "Random selection", "Unit/item missing", "Response error"),
  Mitigation = c("Multiple frames", "Proper design", "Weighting/imputation", "Training/testing"),
  Impact = c("Bias", "Variance", "Bias", "Bias")
)

kable(error_types) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Total Survey Error framework

---

## Documentation Template

```{r documentation_template_273, echo=FALSE}
cat("Survey Documentation Template:

1. DESIGN
   - Target population
   - Sampling frame
   - Sample design
   - Sample size calculation

2. IMPLEMENTATION
   - Data collection period
   - Response rates
   - Quality control measures

3. PROCESSING
   - Editing procedures
   - Imputation methods
   - Weight calculation

4. ESTIMATION
   - Point estimates
   - Variance estimation
   - Significant findings")
```

**Bottom line:** Standardized documentation

---

## Part 4 Summary

### You've mastered:

✅ Complex estimation procedures  
✅ Small area methods  
✅ Missing data handling  
✅ Advanced software techniques  
✅ Quality reporting  

**Bottom line:** Advanced skills acquired!

---

## Key Takeaways

```{r part4_takeaways_275, echo=FALSE}
takeaways <- data.frame(
  Number = 1:5,
  Takeaway = c("Use appropriate variance method",
              "Model-based methods for small domains",
              "Multiple imputation for missing data",
              "Software handles complexity",
              "Document and report quality")
)

kable(takeaways) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Advanced methods essential

---

## Group Activity

### Discussion (5 minutes):

In your groups, discuss:
1. Which advanced method most relevant for your work?
2. Main implementation challenge?
3. How to build capacity?

Share one insight!

**Bottom line:** Learn from each other

---

## Software Resources

```{r software_resources_277, echo=FALSE}
resources <- data.frame(
  Resource = c("survey package", "srvyr", "convey", "sae", "mice"),
  Documentation = c("r-survey.r-forge", "github.com/gergness", 
                   "CRAN vignettes", "CRAN vignettes", "mice website"),
  Learning = c("Lumley book", "Tidyverse style", "Poverty measures",
              "SAE examples", "MI tutorials")
)

kable(resources) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Extensive resources available

---

## Common Pitfalls

```{r common_pitfalls_278, echo=FALSE}
pitfalls <- data.frame(
  Pitfall = c("Ignoring design", "Wrong variance", "No documentation", "Overfitting SAE"),
  Consequence = c("Biased inference", "Wrong CIs", "Not reproducible", "Poor estimates"),
  Prevention = c("Use survey package", "Check methods", "Document always", "Validate models")
)

kable(pitfalls) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Avoid common mistakes

---

## Integration Challenge

### Mini case study:

You have:
- Survey with n=2000
- 20 small domains
- 15% missing income
- Admin data available

Your approach?

**Bottom line:** Apply everything learned

---

## Integration Solution

```{r integration_solution_280, echo=FALSE}
cat("Integrated Approach:

1. DESIGN
   - Use admin data for frame improvement
   - Stratify by key domains

2. ESTIMATION
   - Direct estimates for large domains
   - SAE for small domains (<30)

3. MISSING DATA
   - Multiple imputation for income
   - Use admin data as auxiliary

4. VALIDATION
   - Compare with admin totals
   - Cross-validate SAE models

5. REPORTING
   - Show uncertainty for all estimates
   - Document all methods")
```

**Bottom line:** Systematic integration

---

## Research Frontiers

```{r research_frontiers_281, echo=FALSE}
frontiers <- data.frame(
  Area = c("Non-probability samples", "Mobile data", "AI interviewers", 
          "Real-time calibration"),
  Challenge = c("Selection bias", "Coverage", "Acceptance", "Computation"),
  Promise = c("Cost reduction", "Timeliness", "Availability", "Accuracy"),
  Status = c("Active research", "Pilots", "Experimental", "Development")
)

kable(frontiers) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Field evolving rapidly

---

## Professional Development

```{r professional_dev_282, echo=FALSE}
development <- data.frame(
  Level = c("Beginner", "Intermediate", "Advanced", "Expert"),
  Focus = c("Basic concepts", "Software skills", "Advanced methods", "Innovation"),
  Resources = c("Online courses", "Workshops", "Conferences", "Research"),
  Timeline = c("6 months", "1-2 years", "3-5 years", "Ongoing")
)

kable(development) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Continuous learning path

---

## Your Next Steps

### Action items:

1. **This week:** Install R packages
2. **This month:** Apply one technique
3. **This quarter:** Design improvement
4. **This year:** Build capacity

**Bottom line:** Start immediately

---

## Final Knowledge Check

### Test everything:

1. Complex designs need _______ software
2. SAE borrows _______
3. MI handles _______ uncertainty
4. Documentation ensures _______

**Bottom line:** specialized, strength, imputation, reproducibility

---

## Course Achievement

```{r achievement_285, echo=FALSE, fig.height=5}
skills <- data.frame(
  Skill = c("Design", "Estimation", "Software", "Advanced"),
  Before = c(2, 2, 1, 1),
  After = c(8, 8, 7, 6)
)

skills %>%
  pivot_longer(c(Before, After), names_to = "Time", values_to = "Level") %>%
  ggplot(aes(x = Skill, y = Level, fill = Time)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0, 10)) +
  labs(title = "Your Skill Development - Day 4") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Significant growth!

---

## Q&A Time

### Your questions welcome!

Topics to discuss:
- Technical clarifications
- Implementation challenges
- Software specifics
- Country contexts

No question too basic or advanced!

**Bottom line:** Let's clarify

---

## Day 4 Complete!

### You've mastered:

✅ Complex design integration  
✅ Panel and longitudinal methods  
✅ Responsive design principles  
✅ Advanced estimation techniques  
✅ Software implementation  

**Bottom line:** Complex surveys demystified!

---

## Thank You!

### Day 4 Successfully Completed

Tomorrow - Day 5:
- Special topics
- Emerging methods
- Future of surveys
- Certification

Rest well - see you tomorrow!

**Bottom line:** One more day!

---

## Break Time!

## ☕ End of Day 4

### Homework:
- Review today's materials
- Try one R example
- Prepare questions for Day 5

### Tomorrow:
- Starts at 8:00 AM
- Special topics
- Course wrap-up

**Bottom line:** Well done today!

---

class: center, middle, inverse

# End of Part 4

## Slides 226-300 Complete

### Day 4 Complete - See you tomorrow!

---
---

## Welcome to Part 5!

### Capstone Exercise and Day 4 Wrap-up

Final session agenda:
- Comprehensive case study
- Integration exercise  
- Best practices review
- Q&A session
- Action planning

**Bottom line:** Bring it all together

---

## Capstone Scenario

### National Household Welfare Survey

Your challenge:
- Population: 20 million in 5 regions
- Required: Poverty estimates by district (50 districts)
- Budget: $3 million
- Timeline: 12 months
- Panel component: 25% for welfare dynamics

**Bottom line:** Real-world complexity

---

## Population Structure

```{r population_structure_303, echo=FALSE}
population <- data.frame(
  Region = c("North", "South", "East", "West", "Central"),
  Population = c(3.5, 5.2, 4.1, 3.8, 3.4),
  Districts = c(8, 12, 10, 11, 9),
  Urban_Share = c(0.25, 0.45, 0.35, 0.40, 0.30),
  Poverty_Rate = c(0.32, 0.18, 0.25, 0.22, 0.28),
  Avg_HH_Size = c(4.8, 4.2, 4.5, 4.3, 4.6)
)

kable(population) %>%
  kable_styling(font_size = 11) %>%
  add_header_above(c(" " = 1, "Population (millions)" = 1, 
                     "Admin" = 1, "Demographics" = 3))
```

**Bottom line:** Heterogeneous population

---

## District-Level Details

```{r district_details_304, echo=FALSE}
set.seed(2024)
districts <- do.call(rbind, lapply(1:nrow(population), function(i) {
  n_dist <- population$Districts[i]
  data.frame(
    Region = population$Region[i],
    District = paste0(population$Region[i], "_D", 1:n_dist),
    Pop_District = rnorm(n_dist, population$Population[i]/n_dist * 1e6, 
                         population$Population[i]/n_dist * 1e5),
    Poverty_True = pmax(0, pmin(1, rnorm(n_dist, population$Poverty_Rate[i], 0.05))),
    Cost_Per_HH = rnorm(n_dist, 150, 25),
    Urban_Share_Dist = pmax(0, pmin(1, rnorm(n_dist, population$Urban_Share[i], 0.1))),
    stringsAsFactors = FALSE
  )
}))

# Show sample of districts
head(districts, 10) %>%
  mutate(Pop_District = round(Pop_District/1000)*1000,
         Poverty_True = round(Poverty_True, 3),
         Cost_Per_HH = round(Cost_Per_HH, 0),
         Urban_Share_Dist = round(Urban_Share_Dist, 2)) %>%
  kable() %>%
  kable_styling(font_size = 10)
```

**Bottom line:** 50 diverse districts

---

## Sample Size Requirements

```{r sample_size_calc_305, echo=TRUE}
# Function for sample size with finite population correction
calculate_sample_size <- function(N, p = 0.5, d = 0.03, conf = 0.95, 
                                 deff = 1.5, response_rate = 0.85) {
  z <- qnorm((1 + conf) / 2)
  n0 <- (z^2 * p * (1 - p)) / d^2
  n_fpc <- n0 / (1 + (n0 - 1) / N)
  n_deff <- n_fpc * deff
  n_final <- n_deff / response_rate
  return(ceiling(n_final))
}

# Example calculation for one district
N_district <- 100000  # households
n_required <- calculate_sample_size(N_district, p = 0.25, d = 0.05)
cat("Sample needed per district:", n_required)
```

**Bottom line:** Need ~160 per district minimum

---

## Budget Analysis

```{r budget_analysis_306, echo=FALSE, fig.height=5}
# Budget components
budget_components <- data.frame(
  Component = c("Fixed costs", "Training", "Data collection", 
               "Processing", "Analysis", "Contingency"),
  Amount = c(500000, 200000, 1800000, 200000, 200000, 100000)
)

ggplot(budget_components, aes(x = reorder(Component, Amount), y = Amount/1000)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  coord_flip() +
  labs(x = "", y = "Cost ($1000s)",
       title = "Budget Allocation: $3 Million Total") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Data collection dominates

---

## Two-Stage Design Plan

```{r two_stage_plan_307, echo=FALSE}
design_plan <- data.frame(
  Stage = c("Stage 1", "Stage 2"),
  Unit = c("Enumeration Areas (EAs)", "Households"),
  Frame = c("Census EA list", "Household listing"),
  Selection = c("PPS by size", "Systematic random"),
  Number = c("500 EAs", "8,000 HH"),
  Per_Unit = c("50 districts", "16 HH per EA")
)

kable(design_plan) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Standard two-stage approach

---

## Stratification Scheme

```{r stratification_308, echo=FALSE}
strata <- expand.grid(
  Region = c("North", "South", "East", "West", "Central"),
  Area = c("Urban", "Rural")
)
strata$Stratum_ID <- 1:nrow(strata)
strata$Target_PSUs <- c(20, 30, 40, 35, 35, 40, 35, 45, 30, 40)
strata$Target_HH <- strata$Target_PSUs * 16

kable(strata) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** 10 strata total

---

## PSU Selection

```{r psu_selection_309, echo=TRUE}
# PPS systematic selection function
sample_pps_systematic <- function(frame, n_sample, size_var) {
  frame$cumsize <- cumsum(frame[[size_var]])
  total_size <- sum(frame[[size_var]])
  interval <- total_size / n_sample
  
  # Random start
  start <- runif(1, 0, interval)
  selections <- start + (0:(n_sample - 1)) * interval
  
  # Identify selected units
  selected <- sapply(selections, function(s) {
    which(frame$cumsize >= s)[1]
  })
  
  frame$selected <- 0
  frame$selected[selected] <- 1
  frame$inclusion_prob <- n_sample * frame[[size_var]] / total_size
  
  return(frame)
}
```

**Bottom line:** PPS ensures self-weighting

---

## Panel Component Design

```{r panel_component_310, echo=FALSE, fig.height=5}
# Panel rotation scheme
panel_waves <- data.frame(
  Wave = 1:4,
  Quarter = c("Q1 2024", "Q3 2024", "Q1 2025", "Q3 2025"),
  New_Sample = c(2000, 500, 500, 500),
  Continuing = c(0, 1500, 1400, 1300),
  Total = c(2000, 2000, 1900, 1800)
)

ggplot(panel_waves, aes(x = Wave)) +
  geom_bar(aes(y = New_Sample), stat = "identity", fill = "lightblue") +
  geom_bar(aes(y = Continuing), stat = "identity", fill = "darkblue") +
  labs(y = "Sample Size", title = "Panel Design: 25% Panel Component") +
  theme(text = element_text(size = 14))
```

**Bottom line:** Balanced panel/cross-section

---

## Weight Calculation Steps

```{r weight_steps_311, echo=FALSE}
cat("Weight Calculation Process:

1. BASE WEIGHTS
   - PSU weight = 1 / P(PSU selection)
   - HH weight = 1 / P(HH selection | PSU)
   - Base = PSU weight × HH weight

2. NON-RESPONSE ADJUSTMENT
   - Calculate response rates by stratum
   - Adjust weights: W_adj = W_base / RR

3. CALIBRATION
   - Post-stratify to population totals
   - Rake to known margins
   - Trim extreme weights

4. PANEL WEIGHTS
   - Separate longitudinal weights
   - Attrition adjustment
   - Wave-specific calibration")
```

**Bottom line:** Multiple weight components

---

## Quality Control Plan

```{r quality_control_312, echo=FALSE}
qc_plan <- data.frame(
  Stage = c("Training", "Pilot", "Field", "Processing", "Analysis"),
  Activity = c("Competency tests", "Pilot review", "Daily monitoring",
              "Data validation", "Consistency checks"),
  Metric = c("Pass rate > 90%", "Error rate < 5%", "Response rate > 80%",
            "Missing < 3%", "Outliers < 1%"),
  Action = c("Retrain", "Revise protocol", "Intervention", 
            "Re-contact", "Investigate")
)

kable(qc_plan) %>%
  kable_styling(font_size = 10)
```

**Bottom line:** Quality at every stage

---

## Response Rate Targets

```{r response_targets_313, echo=FALSE, fig.height=5}
# Response rate by week
weeks <- 1:8
target_rr <- c(20, 35, 50, 60, 70, 75, 78, 80)
expected_rr <- c(18, 32, 48, 58, 68, 73, 76, 78)

data.frame(
  Week = weeks,
  Target = target_rr,
  Expected = expected_rr
) %>%
  pivot_longer(-Week, names_to = "Type", values_to = "Rate") %>%
  ggplot(aes(x = Week, y = Rate, color = Type)) +
  geom_line(size = 1.5) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Set1") +
  labs(y = "Response Rate (%)",
       title = "Response Rate Trajectory") +
  theme(text = element_text(size = 14))
```

**Bottom line:** 80% target achievable

---

## Small Area Estimation Plan

```{r sae_plan_314, echo=FALSE}
sae_strategy <- data.frame(
  Domain_Size = c("n ≥ 100", "50 ≤ n < 100", "30 ≤ n < 50", "n < 30"),
  Method = c("Direct", "Direct + Model", "Model-assisted", "Model-based"),
  Auxiliary = c("None", "Census", "Census + Admin", "All available"),
  Report = c("Point + CI", "Both estimates", "Model preferred", "Model only")
)

kable(sae_strategy) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Method depends on sample size

---

## Missing Data Strategy

```{r missing_strategy_315, echo=FALSE}
cat("Missing Data Handling:

1. PREVENTION
   - Clear questions
   - Interviewer training
   - Real-time validation

2. ASSESSMENT
   - Pattern analysis
   - Missing mechanism test
   - Impact evaluation

3. TREATMENT
   - Item missing < 5%: Single imputation
   - Item missing 5-15%: Multiple imputation
   - Item missing > 15%: Investigate cause

4. DOCUMENTATION
   - Imputation rates by variable
   - Method used
   - Sensitivity analysis")
```

**Bottom line:** Minimize and handle properly

---

## Timeline Detail

```{r timeline_detail_316, echo=FALSE}
timeline <- data.frame(
  Month = 1:12,
  Phase = c(rep("Preparation", 2), rep("Pilot", 1), 
           rep("Wave 1", 2), rep("Wave 2", 2), 
           rep("Wave 3", 2), rep("Wave 4", 1),
           rep("Analysis", 2)),
  Main_Activity = c("Design", "Training", "Pilot test",
                   "Data collection", "Processing",
                   "Data collection", "Processing",
                   "Data collection", "Processing",
                   "Final collection",
                   "Analysis", "Reporting")
)

kable(timeline) %>%
  kable_styling(font_size = 11) %>%
  row_spec(which(timeline$Phase == "Pilot"), background = "lightblue")
```

**Bottom line:** 12-month feasible

---

## Cost per Unit

```{r cost_per_unit_317, echo=TRUE}
# Calculate cost efficiency
fixed_costs <- 700000  # Training, setup, etc.
variable_cost_per_hh <- 150
n_households <- 8000

total_cost <- fixed_costs + (variable_cost_per_hh * n_households)
cost_per_complete <- total_cost / n_households

cat("Total cost: $", format(total_cost, big.mark = ","), "\n")
cat("Cost per complete: $", round(cost_per_complete, 2), "\n")
cat("Within budget: ", total_cost < 3000000)
```

**Bottom line:** Within budget constraints

---

## Exercise: Design Your Survey

### Your task (10 minutes):

Using this framework, design a complex survey for your country:

1. Define objectives and domains
2. Specify design features
3. Calculate sample sizes
4. Estimate budget
5. Plan implementation

Work in pairs!

**Bottom line:** Apply everything learned

---

## Design Checklist

```{r design_checklist_319, echo=FALSE}
cat("✓ Survey Design Checklist:

□ OBJECTIVES
  □ Clear research questions
  □ Target population defined
  □ Key indicators identified

□ DESIGN
  □ Sampling frame assessed
  □ Stratification variables
  □ Clustering planned
  □ Sample size calculated

□ IMPLEMENTATION
  □ Budget allocated
  □ Timeline realistic
  □ Quality control plan
  □ Training scheduled

□ ANALYSIS
  □ Estimation methods
  □ Software selected
  □ Documentation plan")
```

**Bottom line:** Comprehensive planning

---

## Common Design Mistakes

```{r design_mistakes_320, echo=FALSE}
mistakes <- data.frame(
  Mistake = c("Underestimate complexity", "Ignore non-response", 
             "Forget documentation", "Skip pilot"),
  Impact = c("Budget overrun", "Biased estimates", 
            "Not reproducible", "Field problems"),
  Prevention = c("Add 20% buffer", "Plan interventions", 
                "Document as you go", "Always pilot")
)

kable(mistakes) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Learn from others

---

## Group Presentations

### Share your designs (20 minutes):

Each pair presents (2 minutes):
- Survey objective
- Key design features
- Main challenge
- Innovative solution

Take notes on others!

**Bottom line:** Peer learning

---

## Integration Example

```{r integration_example_322, echo=FALSE, fig.height=5}
# Show how components integrate
components <- data.frame(
  Component = c("Frame", "Design", "Collection", "Processing", "Analysis"),
  Quality = c(85, 90, 75, 95, 88),
  Time = c(10, 15, 40, 20, 15)
)

par(mfrow = c(1, 2))
barplot(components$Quality, names.arg = components$Component,
        main = "Quality by Component", ylab = "Quality Score",
        col = "lightblue", las = 2)
barplot(components$Time, names.arg = components$Component,
        main = "Time Allocation", ylab = "Percent of Time",
        col = "lightgreen", las = 2)
```

**Bottom line:** Balance across components

---

## Software Implementation

```{r software_implementation_323, echo=TRUE}
# Complete R workflow example
library(survey)
library(tidyverse)

# Create example survey data for demonstration
set.seed(2024)
survey_data <- data.frame(
  psu_id = rep(1:50, each = 20),
  hh_id = 1:1000,
  stratum = rep(1:5, each = 200),
  district = rep(1:10, each = 100),
  poverty = rbinom(1000, 1, 0.25),
  final_weight = runif(1000, 80, 120)
)

# 1. Create design object
design_final <- svydesign(
  ids = ~psu_id,
  strata = ~stratum,
  weights = ~final_weight,
  data = survey_data
)

# 2. Calculate estimates
estimates <- svyby(~poverty, ~district, design_final, svymean)

# 3. Show results
head(estimates, 5)
```

**Bottom line:** Software makes it feasible

---

## Documentation Example

```{r documentation_example_324, echo=FALSE}
cat("SURVEY DOCUMENTATION EXAMPLE:

Title: National Household Welfare Survey 2024
Country: [Your Country]
Organization: National Statistics Office

1. DESIGN SUMMARY
- Type: Stratified two-stage cluster
- Strata: 10 (5 regions × 2 areas)
- PSUs: 500 EAs selected PPS
- SSUs: 8,000 households
- Panel: 25% (2,000 HH)

2. KEY INDICATORS
- Poverty rate: 24.5% (SE: 1.2%)
- Gini coefficient: 0.42 (SE: 0.02)
- Response rate: 78%

3. DATA FILES
- Microdata: NHWS2024_v1.dta
- Weights: NHWS2024_weights.csv
- Documentation: NHWS2024_guide.pdf")
```

**Bottom line:** Clear documentation essential

---

## Quality Indicators Summary

```{r quality_summary_325, echo=FALSE}
quality_results <- data.frame(
  Indicator = c("Response rate", "Coverage rate", "Item response",
               "R-indicator", "CV (poverty)", "Design effect"),
  Target = c("80%", "95%", "97%", "0.80", "<5%", "<2.0"),
  Achieved = c("78%", "94%", "96%", "0.77", "4.8%", "1.85"),
  Status = c("⚠️", "⚠️", "⚠️", "⚠️", "✅", "✅")
)

kable(quality_results) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Most targets achieved

---

## Lessons Learned Today

```{r lessons_today_326, echo=FALSE}
lessons <- data.frame(
  Number = 1:6,
  Lesson = c("Complex designs are standard practice",
            "Integration requires careful planning",
            "Panels reveal change dynamics",
            "Responsive design improves quality",
            "Advanced methods handle real problems",
            "Documentation ensures sustainability")
)

kable(lessons) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Comprehensive understanding

---

## Your Competencies

### After Day 4, you can:

✅ Design complex multi-stage surveys  
✅ Implement panel components  
✅ Apply responsive design principles  
✅ Use advanced estimation methods  
✅ Handle missing data properly  
✅ Document comprehensively  

**Bottom line:** Ready for real surveys!

---

## Action Planning

### Your commitments:

Write down 3 specific actions:

1. **Next week:** _________________
2. **Next month:** _________________
3. **Next quarter:** _________________

Share one with the group!

**Bottom line:** Commitment drives change

---

## Resources for Continued Learning

```{r resources_328, echo=FALSE}
resources <- data.frame(
  Type = c("Books", "Courses", "Software", "Communities", "Datasets"),
  Specific = c("Lohr, Lumley, Valliant", "Coursera, edX",
              "R survey package", "Survey Statistics group",
              "DHS, LSMS, EU-SILC"),
  Access = c("Library/Amazon", "Online", "CRAN", "LinkedIn", "Websites")
)

kable(resources) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Keep learning

---

## Regional Collaboration

```{r regional_collab_329, echo=FALSE}
cat("SADC Collaboration Opportunities:

1. ANNUAL WORKSHOP
   - Share experiences
   - Common challenges
   - Best practices

2. TECHNICAL EXCHANGE
   - Staff exchanges
   - Joint training
   - Peer review

3. HARMONIZATION
   - Common indicators
   - Shared methods
   - Comparable estimates

4. RESOURCE SHARING
   - Software licenses
   - Training materials
   - Expert consultants")
```

**Bottom line:** Stronger together

---

## Future Challenges

```{r future_challenges_330, echo=FALSE}
challenges <- data.frame(
  Challenge = c("Declining response rates", "Cost pressures", 
               "New data sources", "Capacity gaps"),
  Impact = c("Quality", "Sustainability", "Integration", "Innovation"),
  Strategy = c("Adaptive design", "Efficiency gains", 
              "Hybrid approaches", "Training investment")
)

kable(challenges) %>%
  kable_styling(font_size = 11)
```

**Bottom line:** Prepare for change

---

## Innovation Opportunities

```{r innovation_331, echo=FALSE, fig.height=5}
innovations <- data.frame(
  Area = c("Design", "Collection", "Processing", "Analysis", "Dissemination"),
  Current = c(3, 2, 4, 3, 2),
  Potential = c(5, 5, 4, 5, 5)
)

innovations %>%
  pivot_longer(c(Current, Potential), names_to = "Status", values_to = "Level") %>%
  ggplot(aes(x = Area, y = Level, fill = Status)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Set2") +
  labs(y = "Innovation Level (1-5)",
       title = "Innovation Opportunities") +
  theme(text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

**Bottom line:** Room for innovation

---

## Success Factors

```{r success_factors_332, echo=FALSE}
cat("Critical Success Factors:

1. LEADERSHIP SUPPORT
   - Resource commitment
   - Strategic vision
   - Change management

2. TECHNICAL CAPACITY
   - Skilled staff
   - Modern tools
   - Continuous learning

3. QUALITY FOCUS
   - Standards adherence
   - Monitoring systems
   - Improvement culture

4. USER ENGAGEMENT
   - Stakeholder involvement
   - Clear communication
   - Timely delivery")
```

**Bottom line:** Multiple factors matter

---

## Personal Development Path

```{r development_path_333, echo=FALSE}
development <- data.frame(
  Year = c("Year 1", "Year 2", "Year 3", "Year 5"),
  Focus = c("Master basics", "Advanced methods", "Innovation", "Leadership"),
  Skills = c("Software, design", "SAE, panels", "ML, big data", "Strategy"),
  Goal = c("Competent", "Expert", "Innovator", "Leader")
)

kable(development) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Long-term growth

---

## Network Building

### Stay connected:

- Course participants
- Regional experts
- International community
- Online forums

Exchange:
- Experiences
- Solutions
- Resources
- Opportunities

**Bottom line:** Network is valuable

---

## Final Q&A

### Last chance for questions!

Any topics:
- Technical details
- Implementation issues
- Career development
- Future learning

Ask anything!

**Bottom line:** Clarify everything

---

## Course Evaluation

### Please provide feedback:

1. Most valuable content?
2. Areas for improvement?
3. Missing topics?
4. Delivery methods?
5. Overall satisfaction?

Your feedback improves future courses!

**Bottom line:** Help us improve

---

## Day 4 Key Takeaways

```{r key_takeaways_338, echo=FALSE}
takeaways <- data.frame(
  Number = 1:5,
  Takeaway = c("Complex designs require integration",
              "Responsive methods improve quality",
              "Advanced estimation solves real problems",
              "Software enables implementation",
              "Documentation ensures success")
)

kable(takeaways) %>%
  kable_styling(font_size = 12)
```

**Bottom line:** Complexity mastered

---

## Inspiration

```{r inspiration_339, echo=FALSE}
cat("'The goal is to turn data into information,
and information into insight.'
- Carly Fiorina

'Without data, you're just another person
with an opinion.'
- W. Edwards Deming

'Statistical thinking will one day be as
necessary as the ability to read and write.'
- H.G. Wells

You now have these skills!")
```

**Bottom line:** You're equipped!

---

## Recognition

### Certificate of Completion

You have successfully completed:

**Day 4: Complex Survey Designs**

Competencies demonstrated:
- Complex design integration
- Panel survey methods
- Responsive design principles
- Advanced estimation techniques

**Congratulations!**

**Bottom line:** Achievement recognized

---

## Looking Ahead

### Tomorrow - Day 5:

- Special topics
- Emerging methods
- Country presentations
- Course synthesis
- Certification ceremony

Be ready to share your learnings!

**Bottom line:** Grand finale tomorrow

---

## Thank You!

### Excellent work today!

You've mastered:
- Complex integration
- Advanced methods
- Practical implementation

See you tomorrow at 8:00 AM!

**Bottom line:** Day 4 complete!

---

## Evening Assignment

### Optional homework:

1. Review today's R code
2. Sketch survey design for your context
3. List three applications
4. Prepare one question for Day 5

**Bottom line:** Reinforce learning

---

## Networking Time

### Before you leave:

- Exchange contacts
- Discuss challenges
- Share successes
- Plan collaboration

Build your professional network!

**Bottom line:** Connect with peers

---

## Final Thought

```{r final_thought_345, echo=FALSE}
cat("'The expert in anything was once
a beginner who never gave up.'

Today you've moved from beginner
to practitioner in complex surveys.

Tomorrow you become an expert.

Keep learning, keep growing!")
```

**Bottom line:** Journey continues

---

## See You Tomorrow!

### Rest well!

Day 5 starts at 8:00 AM

Topics:
- Special populations
- Emerging technologies  
- Future of surveys
- Your presentations

**Bottom line:** Ready for Day 5!

---

class: center, middle, inverse

# End of Day 4

## Complex Survey Designs Mastered

### 350 Slides Completed

---

## Break/Social

## 🎉 Day 4 Complete!

### Celebrate your achievement!

Optional evening session:
- Software help desk (5:30-6:30)
- Informal discussions
- Networking

**Bottom line:** Well done!

---

class: center, middle, inverse

# Thank You for Your Dedication

## See You Tomorrow for Day 5

### The Journey Continues...

---